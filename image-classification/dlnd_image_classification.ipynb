{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-60712b9de575>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misdir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mproblem_unittests\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtests\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Dev\\Udacity_Deep-Learning\\Deep-Learning-Project-2\\deep-learning\\image-classification\\problem_unittests.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0munittest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmock\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMagicMock\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 23:\n",
      "Image - Min Value: 4 Max Value: 238\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB4DCFD0AFKboAaCw/wXAtOxTlohGsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16ca7e5ebe0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 23\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return (x / x.max())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "one_hot_encoder = None\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    global one_hot_encoder\n",
    "\n",
    "    if one_hot_encoder == None:\n",
    "        one_hot_encoder = pp.LabelBinarizer()\n",
    "        one_hot_encoder.fit(x)\n",
    "\n",
    "    return one_hot_encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None,\n",
    "                                             image_shape[0],\n",
    "                                             image_shape[1],\n",
    "                                             image_shape[2]), name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.int32, shape=(None, n_classes), name=\"y\")\n",
    "\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    dim = x_tensor.get_shape().as_list()\n",
    "    shape = list(conv_ksize + (dim[-1],) + (conv_num_outputs,))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    weights = tf.Variable(tf.truncated_normal(shape,0,0.1))\n",
    "    \n",
    "    stride = list((1,)+conv_strides+(1,))\n",
    "    \n",
    "    #conv2d(input, filter, stride, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "    conv_layer = tf.nn.conv2d(x_tensor,weights,stride, padding=\"SAME\", use_cudnn_on_gpu=True)\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    #tf.nn.max_pool(value, ksize, strides, padding, data_format='NHWC', name=None)\n",
    "    ksize = list((1,) + pool_ksize + (1,))\n",
    "    strides = list((1,) + pool_strides + (1,))\n",
    "    \n",
    "    #print(conv_layer)\n",
    "    #print(ksize)\n",
    "    #print(strides)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize, strides, padding=\"SAME\")\n",
    "    \n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    #print(x_tensor)\n",
    "    #print(tf.contrib.layers.flatten(x_tensor))\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    #print(x_tensor, num_outputs)\n",
    "    #print(tf.contrib.layers.fully_connected(x_tensor,num_outputs))\n",
    "    return tf.contrib.layers.fully_connected(x_tensor,num_outputs,activation_fn=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    #print(x_tensor)\n",
    "    #print(num_outputs)\n",
    "    #print(tf.contrib.layers.fully_connected(x_tensor, num_outputs))\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def print_stuff(message, net):\n",
    "    print(message)\n",
    "    print(net)\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    #Prints network layer info if True\n",
    "    do_print = False\n",
    "   \n",
    "    #Convolution and Max Pool Layers\n",
    "    #conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    #net = conv2d_maxpool(x, 64, (5,5), (1,1), (5,5), (3,3))\n",
    "    net = conv2d_maxpool(x, 64, (5,5), (1,1), (5,5), (3,3))\n",
    "\n",
    "    net = conv2d_maxpool(net, 64, (5,5), (1,1), (3,3), (3,3))\n",
    "    #net = conv2d_maxpool(x, 18, (16,16), (1,1), (8,8), (1,1))\n",
    "\n",
    "    if do_print:\n",
    "        print_stuff(\"Maxpool:\", net)\n",
    "        \n",
    "    #Flatten Layer\n",
    "    net = flatten(net)\n",
    "    \n",
    "    if do_print:\n",
    "        print_stuff(\"Flatten:\", net)\n",
    "        \n",
    "    #Tryout a dropout :D\n",
    "    net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "    \n",
    "    if do_print:\n",
    "        print_stuff(\"Dropout:\", net)\n",
    "        \n",
    "    #2 Fully Connected Layers\n",
    "    #fully_conn(x_tensor, num_outputs)\n",
    "    net = fully_conn(net, 384)\n",
    "    net = fully_conn(net, 192)\n",
    "   # net = fully_conn(net, 256)\n",
    "\n",
    "\n",
    "    \n",
    "    if do_print:\n",
    "        print_stuff(\"Fully Connected:\", net)\n",
    "    \n",
    "    net = tf.nn.dropout(net, keep_prob)\n",
    "    \n",
    "    #Output Layer!\n",
    "    net = output(net, 10)\n",
    "    \n",
    "    if do_print:\n",
    "        print_stuff(\"Output:\", net)\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    #Run TF session!\n",
    "    session.run(optimizer, feed_dict={x:feature_batch, y:label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    global valid_features, valid_labels\n",
    "    #print(valid_features, valid_labels)\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0})\n",
    "    accuracy = session.run(accuracy, feed_dict={x: valid_features, y:valid_labels, keep_prob:1.0})\n",
    "    \n",
    "    print('\\nLoss = {} \\t Accuracy = {}\\n'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "#epochs = 50\n",
    "#batch_size = 256\n",
    "#keep_probability = 0.25\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 1024\n",
    "keep_probability = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 1:  \n",
      "Loss = 2.290092945098877 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 1:  \n",
      "Loss = 2.2524847984313965 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 1:  \n",
      "Loss = 2.201976776123047 \t Accuracy = 0.12379999458789825\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 1:  \n",
      "Loss = 2.1808207035064697 \t Accuracy = 0.15379998087882996\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 1:  \n",
      "Loss = 2.1702675819396973 \t Accuracy = 0.19219999015331268\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 1:  \n",
      "Loss = 2.0790369510650635 \t Accuracy = 0.25380000472068787\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 1:  \n",
      "Loss = 1.99192476272583 \t Accuracy = 0.30479997396469116\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 1:  \n",
      "Loss = 1.8892314434051514 \t Accuracy = 0.33479997515678406\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 1:  \n",
      "Loss = 1.8167994022369385 \t Accuracy = 0.39139997959136963\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 1:  \n",
      "Loss = 1.6663709878921509 \t Accuracy = 0.40700000524520874\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 1:  \n",
      "Loss = 1.6480740308761597 \t Accuracy = 0.41419994831085205\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 1:  \n",
      "Loss = 1.5903947353363037 \t Accuracy = 0.4197999835014343\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 1:  \n",
      "Loss = 1.565551519393921 \t Accuracy = 0.43199998140335083\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 1:  \n",
      "Loss = 1.4863773584365845 \t Accuracy = 0.454399973154068\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 1:  \n",
      "Loss = 1.4651837348937988 \t Accuracy = 0.46619996428489685\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 1:  \n",
      "Loss = 1.431221604347229 \t Accuracy = 0.4753999412059784\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 1:  \n",
      "Loss = 1.4218171834945679 \t Accuracy = 0.4705999791622162\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 1:  \n",
      "Loss = 1.378117322921753 \t Accuracy = 0.4883999824523926\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 1:  \n",
      "Loss = 1.3444592952728271 \t Accuracy = 0.486799955368042\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 1:  \n",
      "Loss = 1.3374019861221313 \t Accuracy = 0.48079997301101685\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 1:  \n",
      "Loss = 1.3183830976486206 \t Accuracy = 0.49859994649887085\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 1:  \n",
      "Loss = 1.2742877006530762 \t Accuracy = 0.5089999437332153\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 1:  \n",
      "Loss = 1.3074414730072021 \t Accuracy = 0.48479998111724854\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 1:  \n",
      "Loss = 1.2665340900421143 \t Accuracy = 0.49699997901916504\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 1:  \n",
      "Loss = 1.2159295082092285 \t Accuracy = 0.532599925994873\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 1:  \n",
      "Loss = 1.2191100120544434 \t Accuracy = 0.520799994468689\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 1:  \n",
      "Loss = 1.1848134994506836 \t Accuracy = 0.5287999510765076\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 1:  \n",
      "Loss = 1.1672334671020508 \t Accuracy = 0.5331999659538269\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 1:  \n",
      "Loss = 1.1187820434570312 \t Accuracy = 0.5425999760627747\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 1:  \n",
      "Loss = 1.1206127405166626 \t Accuracy = 0.5463999509811401\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 1:  \n",
      "Loss = 1.1012290716171265 \t Accuracy = 0.5479999780654907\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 1:  \n",
      "Loss = 1.0620383024215698 \t Accuracy = 0.555199921131134\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 1:  \n",
      "Loss = 1.049516201019287 \t Accuracy = 0.5613999366760254\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 1:  \n",
      "Loss = 1.0376770496368408 \t Accuracy = 0.5545998811721802\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 1:  \n",
      "Loss = 1.0494012832641602 \t Accuracy = 0.5499999523162842\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 1:  \n",
      "Loss = 1.0140042304992676 \t Accuracy = 0.5625998973846436\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9972331523895264 \t Accuracy = 0.575999915599823\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9856253266334534 \t Accuracy = 0.5725999474525452\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9815484285354614 \t Accuracy = 0.5697999596595764\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9493968486785889 \t Accuracy = 0.5737999081611633\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9282434582710266 \t Accuracy = 0.5781999230384827\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9558067917823792 \t Accuracy = 0.5733999609947205\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 1:  \n",
      "Loss = 0.910616397857666 \t Accuracy = 0.5809999704360962\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8981699347496033 \t Accuracy = 0.5905998945236206\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8641870021820068 \t Accuracy = 0.591999888420105\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8488867878913879 \t Accuracy = 0.5969998836517334\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8417847156524658 \t Accuracy = 0.6011999249458313\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8396891951560974 \t Accuracy = 0.5973999500274658\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8461910486221313 \t Accuracy = 0.5989999175071716\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8110532760620117 \t Accuracy = 0.5981999039649963\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8100953102111816 \t Accuracy = 0.5995998978614807\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7815923690795898 \t Accuracy = 0.5991999506950378\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7557841539382935 \t Accuracy = 0.6075998544692993\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7562233209609985 \t Accuracy = 0.6055999398231506\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7402206659317017 \t Accuracy = 0.6095998883247375\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7464864253997803 \t Accuracy = 0.6057999134063721\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7143686413764954 \t Accuracy = 0.6103999018669128\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6996148228645325 \t Accuracy = 0.6131998896598816\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7038403749465942 \t Accuracy = 0.609799861907959\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6810511350631714 \t Accuracy = 0.6073998808860779\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6619201302528381 \t Accuracy = 0.6171998977661133\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6474202871322632 \t Accuracy = 0.6187998652458191\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6411469578742981 \t Accuracy = 0.6245998740196228\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6274838447570801 \t Accuracy = 0.6305999159812927\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6310270428657532 \t Accuracy = 0.6209998726844788\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6081663966178894 \t Accuracy = 0.6197998523712158\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6513372659683228 \t Accuracy = 0.6125999093055725\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6294978260993958 \t Accuracy = 0.6123998761177063\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6015546917915344 \t Accuracy = 0.6191999316215515\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5880208015441895 \t Accuracy = 0.6255999207496643\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5706637501716614 \t Accuracy = 0.6221999526023865\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5664055943489075 \t Accuracy = 0.6273998618125916\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5834810137748718 \t Accuracy = 0.6181999444961548\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5390875339508057 \t Accuracy = 0.6299998760223389\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5428544878959656 \t Accuracy = 0.6299998760223389\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5284205675125122 \t Accuracy = 0.627799928188324\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5249622464179993 \t Accuracy = 0.6223998665809631\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5405223369598389 \t Accuracy = 0.616399884223938\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5177670121192932 \t Accuracy = 0.6277998685836792\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5145576000213623 \t Accuracy = 0.626599907875061\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5135391354560852 \t Accuracy = 0.6299998760223389\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4974324703216553 \t Accuracy = 0.631199836730957\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, CIFAR-10 Batch 1:  \n",
      "Loss = 0.49349308013916016 \t Accuracy = 0.6275998950004578\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 1:  \n",
      "Loss = 0.48457303643226624 \t Accuracy = 0.630599856376648\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4759840667247772 \t Accuracy = 0.6315999031066895\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4701535403728485 \t Accuracy = 0.6337999105453491\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4729006588459015 \t Accuracy = 0.6335999369621277\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 1:  \n",
      "Loss = 0.45890742540359497 \t Accuracy = 0.6319999098777771\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 1:  \n",
      "Loss = 0.46466562151908875 \t Accuracy = 0.625999927520752\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4432936906814575 \t Accuracy = 0.6283998489379883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 4:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 5:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 4:  \n",
      "Loss = 2.2918765544891357 \t Accuracy = 0.11059999465942383\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 5:  \n",
      "Loss = 2.2838363647460938 \t Accuracy = 0.11500000208616257\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 1:  \n",
      "Loss = 2.2646641731262207 \t Accuracy = 0.11839999258518219\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 2:  \n",
      "Loss = 2.1900718212127686 \t Accuracy = 0.1242000013589859\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 3:  \n",
      "Loss = 2.2217612266540527 \t Accuracy = 0.11760000139474869\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 4:  \n",
      "Loss = 2.19511342048645 \t Accuracy = 0.12300001084804535\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 5:  \n",
      "Loss = 2.131333589553833 \t Accuracy = 0.1509999930858612\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 1:  \n",
      "Loss = 2.0601062774658203 \t Accuracy = 0.20160000026226044\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 2:  \n",
      "Loss = 2.0969676971435547 \t Accuracy = 0.26419997215270996\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 3:  \n",
      "Loss = 1.8467410802841187 \t Accuracy = 0.35579997301101685\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 4:  \n",
      "Loss = 1.8542094230651855 \t Accuracy = 0.3255999684333801\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 5:  \n",
      "Loss = 1.7334191799163818 \t Accuracy = 0.3723999857902527\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 1:  \n",
      "Loss = 1.6474764347076416 \t Accuracy = 0.41519999504089355\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 2:  \n",
      "Loss = 1.58233642578125 \t Accuracy = 0.4139999747276306\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 3:  \n",
      "Loss = 1.4827210903167725 \t Accuracy = 0.4569999873638153\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 4:  \n",
      "Loss = 1.481651782989502 \t Accuracy = 0.465999960899353\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 5:  \n",
      "Loss = 1.4310438632965088 \t Accuracy = 0.4580000042915344\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 1:  \n",
      "Loss = 1.4534884691238403 \t Accuracy = 0.47759997844696045\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 2:  \n",
      "Loss = 1.4159328937530518 \t Accuracy = 0.506399929523468\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 3:  \n",
      "Loss = 1.30161714553833 \t Accuracy = 0.5083999633789062\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 4:  \n",
      "Loss = 1.3064028024673462 \t Accuracy = 0.5239999294281006\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 5:  \n",
      "Loss = 1.2719069719314575 \t Accuracy = 0.5291999578475952\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 1:  \n",
      "Loss = 1.2942289113998413 \t Accuracy = 0.5341999530792236\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 2:  \n",
      "Loss = 1.30490243434906 \t Accuracy = 0.545199990272522\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 3:  \n",
      "Loss = 1.2165104150772095 \t Accuracy = 0.533799946308136\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 4:  \n",
      "Loss = 1.2334105968475342 \t Accuracy = 0.5451999306678772\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 5:  \n",
      "Loss = 1.1596275568008423 \t Accuracy = 0.5649999380111694\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 1:  \n",
      "Loss = 1.2037086486816406 \t Accuracy = 0.5593999624252319\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 2:  \n",
      "Loss = 1.2224934101104736 \t Accuracy = 0.5659999847412109\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, CIFAR-10 Batch 3:  \n",
      "Loss = 1.14084792137146 \t Accuracy = 0.5677999258041382\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 4:  \n",
      "Loss = 1.114132046699524 \t Accuracy = 0.5767999291419983\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 5:  \n",
      "Loss = 1.082906723022461 \t Accuracy = 0.5881999135017395\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 1:  \n",
      "Loss = 1.12198805809021 \t Accuracy = 0.5931999087333679\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 2:  \n",
      "Loss = 1.1733510494232178 \t Accuracy = 0.5877999067306519\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 3:  \n",
      "Loss = 1.0929861068725586 \t Accuracy = 0.5885999798774719\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 4:  \n",
      "Loss = 1.0772141218185425 \t Accuracy = 0.6033998727798462\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 5:  \n",
      "Loss = 1.0524089336395264 \t Accuracy = 0.5865999460220337\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 1:  \n",
      "Loss = 1.093569040298462 \t Accuracy = 0.6043999195098877\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 2:  \n",
      "Loss = 1.1011037826538086 \t Accuracy = 0.6113998889923096\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 3:  \n",
      "Loss = 1.0135482549667358 \t Accuracy = 0.6175999045372009\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 4:  \n",
      "Loss = 1.034607172012329 \t Accuracy = 0.6013998985290527\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 5:  \n",
      "Loss = 0.9984158873558044 \t Accuracy = 0.6075999140739441\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 1:  \n",
      "Loss = 1.0329926013946533 \t Accuracy = 0.6201999187469482\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 2:  \n",
      "Loss = 1.0346530675888062 \t Accuracy = 0.6273999214172363\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 3:  \n",
      "Loss = 0.9946421384811401 \t Accuracy = 0.622999906539917\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 4:  \n",
      "Loss = 0.9844017028808594 \t Accuracy = 0.622999906539917\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 5:  \n",
      "Loss = 0.9321788549423218 \t Accuracy = 0.6291999220848083\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9919136166572571 \t Accuracy = 0.6419998407363892\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 2:  \n",
      "Loss = 1.038059949874878 \t Accuracy = 0.630599856376648\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 3:  \n",
      "Loss = 0.9576947689056396 \t Accuracy = 0.6395999193191528\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 4:  \n",
      "Loss = 0.9522494077682495 \t Accuracy = 0.6345999240875244\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 5:  \n",
      "Loss = 0.8943104147911072 \t Accuracy = 0.636199951171875\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9584265351295471 \t Accuracy = 0.6439998745918274\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 2:  \n",
      "Loss = 0.9800088405609131 \t Accuracy = 0.6477998495101929\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 3:  \n",
      "Loss = 0.9208322763442993 \t Accuracy = 0.6417998671531677\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 4:  \n",
      "Loss = 0.9142913222312927 \t Accuracy = 0.6465998888015747\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 5:  \n",
      "Loss = 0.8545132875442505 \t Accuracy = 0.650399923324585\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9372496604919434 \t Accuracy = 0.6543998718261719\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 2:  \n",
      "Loss = 0.9460872411727905 \t Accuracy = 0.6535998582839966\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 3:  \n",
      "Loss = 0.9088939428329468 \t Accuracy = 0.6549999713897705\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 4:  \n",
      "Loss = 0.8979156613349915 \t Accuracy = 0.6559998989105225\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 5:  \n",
      "Loss = 0.8442990183830261 \t Accuracy = 0.650399923324585\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9132951498031616 \t Accuracy = 0.6631999015808105\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 2:  \n",
      "Loss = 0.9587260484695435 \t Accuracy = 0.6419999599456787\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 3:  \n",
      "Loss = 0.9580973386764526 \t Accuracy = 0.6281999349594116\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 4:  \n",
      "Loss = 0.8949211835861206 \t Accuracy = 0.6447998285293579\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 5:  \n",
      "Loss = 0.8014954328536987 \t Accuracy = 0.6623998880386353\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8940370082855225 \t Accuracy = 0.6659998893737793\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 2:  \n",
      "Loss = 0.9166122078895569 \t Accuracy = 0.672799825668335\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 3:  \n",
      "Loss = 0.8748056292533875 \t Accuracy = 0.6623998284339905\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 4:  \n",
      "Loss = 0.8393039107322693 \t Accuracy = 0.6679998636245728\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 5:  \n",
      "Loss = 0.778281033039093 \t Accuracy = 0.6797999143600464\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8718405961990356 \t Accuracy = 0.6783998608589172\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 2:  \n",
      "Loss = 0.8897175788879395 \t Accuracy = 0.6617999076843262\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 3:  \n",
      "Loss = 0.8923651576042175 \t Accuracy = 0.649199903011322\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 4:  \n",
      "Loss = 0.8391891121864319 \t Accuracy = 0.6645999550819397\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 5:  \n",
      "Loss = 0.7762047052383423 \t Accuracy = 0.6731998920440674\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8613346219062805 \t Accuracy = 0.681999921798706\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 2:  \n",
      "Loss = 0.8660943508148193 \t Accuracy = 0.6759998798370361\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 3:  \n",
      "Loss = 0.8379775285720825 \t Accuracy = 0.6597998738288879\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 4:  \n",
      "Loss = 0.7898654937744141 \t Accuracy = 0.678399920463562\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 5:  \n",
      "Loss = 0.7538022994995117 \t Accuracy = 0.6823998689651489\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8392091393470764 \t Accuracy = 0.6775998473167419\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 2:  \n",
      "Loss = 0.8446429371833801 \t Accuracy = 0.6797998547554016\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 3:  \n",
      "Loss = 0.7826923131942749 \t Accuracy = 0.6873998641967773\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 4:  \n",
      "Loss = 0.7699903249740601 \t Accuracy = 0.6757998466491699\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 5:  \n",
      "Loss = 0.7405447959899902 \t Accuracy = 0.6827998161315918\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8215856552124023 \t Accuracy = 0.6825998425483704\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 2:  \n",
      "Loss = 0.835797905921936 \t Accuracy = 0.6805998682975769\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 3:  \n",
      "Loss = 0.8074173331260681 \t Accuracy = 0.6735998392105103\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 4:  \n",
      "Loss = 0.7546363472938538 \t Accuracy = 0.682999849319458\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 5:  \n",
      "Loss = 0.7126396894454956 \t Accuracy = 0.6911998391151428\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7994093298912048 \t Accuracy = 0.692599892616272\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 2:  \n",
      "Loss = 0.8065587282180786 \t Accuracy = 0.6919999122619629\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 3:  \n",
      "Loss = 0.7750344276428223 \t Accuracy = 0.6823998689651489\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 4:  \n",
      "Loss = 0.7484662532806396 \t Accuracy = 0.6859999299049377\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6855456233024597 \t Accuracy = 0.6915999054908752\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 1:  \n",
      "Loss = 0.787386417388916 \t Accuracy = 0.6945998668670654\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 2:  \n",
      "Loss = 0.829970121383667 \t Accuracy = 0.684199869632721\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 3:  \n",
      "Loss = 0.804364800453186 \t Accuracy = 0.6741999387741089\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 4:  \n",
      "Loss = 0.7429187893867493 \t Accuracy = 0.6897999048233032\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6973387598991394 \t Accuracy = 0.6933999061584473\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7789401412010193 \t Accuracy = 0.6981998682022095\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7926915884017944 \t Accuracy = 0.6941998600959778\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 3:  \n",
      "Loss = 0.7315212488174438 \t Accuracy = 0.6957998275756836\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 4:  \n",
      "Loss = 0.7345497012138367 \t Accuracy = 0.6893998384475708\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6885759830474854 \t Accuracy = 0.6875998973846436\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7613082528114319 \t Accuracy = 0.6891998648643494\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7942829728126526 \t Accuracy = 0.6921998262405396\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 3:  \n",
      "Loss = 0.7563952803611755 \t Accuracy = 0.6803998947143555\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 4:  \n",
      "Loss = 0.7171012163162231 \t Accuracy = 0.6937998533248901\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6739822030067444 \t Accuracy = 0.702599823474884\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 1:  \n",
      "Loss = 0.739060640335083 \t Accuracy = 0.696199893951416\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7608165144920349 \t Accuracy = 0.6969998478889465\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 3:  \n",
      "Loss = 0.7309772372245789 \t Accuracy = 0.6977999210357666\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6895837783813477 \t Accuracy = 0.6927998661994934\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6624671220779419 \t Accuracy = 0.7015998363494873\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 1:  \n",
      "Loss = 0.749941349029541 \t Accuracy = 0.6983999013900757\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7535368204116821 \t Accuracy = 0.7019999027252197\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 3:  \n",
      "Loss = 0.7308796644210815 \t Accuracy = 0.7003998756408691\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6797794103622437 \t Accuracy = 0.705599844455719\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6878502368927002 \t Accuracy = 0.6883998513221741\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, CIFAR-10 Batch 1:  \n",
      "Loss = 0.729364275932312 \t Accuracy = 0.6993998289108276\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7332606911659241 \t Accuracy = 0.7009998559951782\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 3:  \n",
      "Loss = 0.7185033559799194 \t Accuracy = 0.699199914932251\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6838666200637817 \t Accuracy = 0.6965999007225037\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6573669910430908 \t Accuracy = 0.6927998661994934\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7229213118553162 \t Accuracy = 0.7039998769760132\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 2:  \n",
      "Loss = 0.723239541053772 \t Accuracy = 0.7073999047279358\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6776679754257202 \t Accuracy = 0.7107998728752136\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6634485721588135 \t Accuracy = 0.7047998309135437\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6307167410850525 \t Accuracy = 0.7095998525619507\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7082217335700989 \t Accuracy = 0.705599844455719\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7128450870513916 \t Accuracy = 0.7125998139381409\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6597667932510376 \t Accuracy = 0.7131998538970947\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6382977962493896 \t Accuracy = 0.7047998309135437\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6231856942176819 \t Accuracy = 0.7123998403549194\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7032708525657654 \t Accuracy = 0.708599865436554\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 2:  \n",
      "Loss = 0.698011577129364 \t Accuracy = 0.7161998748779297\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6611646413803101 \t Accuracy = 0.7089998722076416\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6320183277130127 \t Accuracy = 0.7117998600006104\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5981185436248779 \t Accuracy = 0.7157998085021973\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6827002763748169 \t Accuracy = 0.710399866104126\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 2:  \n",
      "Loss = 0.694100022315979 \t Accuracy = 0.7133998274803162\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6605679988861084 \t Accuracy = 0.7071998119354248\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 4:  \n",
      "Loss = 0.619574785232544 \t Accuracy = 0.715799868106842\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5915719866752625 \t Accuracy = 0.715199887752533\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6678068041801453 \t Accuracy = 0.7109999060630798\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6999595761299133 \t Accuracy = 0.7107999324798584\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6544132828712463 \t Accuracy = 0.712199866771698\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6341754198074341 \t Accuracy = 0.7125998735427856\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5734332799911499 \t Accuracy = 0.7175997495651245\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6837223172187805 \t Accuracy = 0.7197998762130737\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6852023005485535 \t Accuracy = 0.715199887752533\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6454221606254578 \t Accuracy = 0.707399845123291\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6052860617637634 \t Accuracy = 0.716999888420105\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5710591673851013 \t Accuracy = 0.7207998037338257\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6671053171157837 \t Accuracy = 0.7187998294830322\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6985654830932617 \t Accuracy = 0.7109999060630798\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6598047018051147 \t Accuracy = 0.7065998315811157\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6279918551445007 \t Accuracy = 0.7167998552322388\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5579761862754822 \t Accuracy = 0.7161998152732849\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6614621877670288 \t Accuracy = 0.7183997631072998\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6629696488380432 \t Accuracy = 0.7161998152732849\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6494510173797607 \t Accuracy = 0.7127998471260071\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6161854267120361 \t Accuracy = 0.7151998281478882\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5654051303863525 \t Accuracy = 0.7107999324798584\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6524869203567505 \t Accuracy = 0.7199998497962952\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6438609957695007 \t Accuracy = 0.7221998572349548\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6327430009841919 \t Accuracy = 0.7189998626708984\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 4:  \n",
      "Loss = 0.601449191570282 \t Accuracy = 0.7135998010635376\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5548460483551025 \t Accuracy = 0.7157998085021973\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6416856050491333 \t Accuracy = 0.722399890422821\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6456626653671265 \t Accuracy = 0.7153998613357544\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6253146529197693 \t Accuracy = 0.7175998687744141\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5970786213874817 \t Accuracy = 0.7069998979568481\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5645136833190918 \t Accuracy = 0.7067999243736267\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6718063950538635 \t Accuracy = 0.7149998545646667\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 2:  \n",
      "Loss = 0.650331974029541 \t Accuracy = 0.7145998477935791\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6388469338417053 \t Accuracy = 0.7095997929573059\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5783604383468628 \t Accuracy = 0.7189998030662537\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5415434837341309 \t Accuracy = 0.7199998497962952\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6225280165672302 \t Accuracy = 0.7237998247146606\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6300390362739563 \t Accuracy = 0.7225998044013977\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6013551950454712 \t Accuracy = 0.7181998491287231\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5515520572662354 \t Accuracy = 0.7279998660087585\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5403343439102173 \t Accuracy = 0.7201998829841614\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 1:  \n",
      "Loss = 0.622210681438446 \t Accuracy = 0.7157998085021973\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6119474768638611 \t Accuracy = 0.723599910736084\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5769696831703186 \t Accuracy = 0.7227997779846191\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5723608136177063 \t Accuracy = 0.716799795627594\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5325093269348145 \t Accuracy = 0.7235997915267944\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6152253150939941 \t Accuracy = 0.7309998273849487\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6132906675338745 \t Accuracy = 0.7243998646736145\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5795241594314575 \t Accuracy = 0.7249999046325684\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5395927429199219 \t Accuracy = 0.7271998524665833\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5291205644607544 \t Accuracy = 0.7279999256134033\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6020126342773438 \t Accuracy = 0.7185998558998108\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6256351470947266 \t Accuracy = 0.7165998220443726\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 3:  \n",
      "Loss = 0.571091890335083 \t Accuracy = 0.7227997779846191\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 4:  \n",
      "Loss = 0.55861496925354 \t Accuracy = 0.7211998701095581\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5628076195716858 \t Accuracy = 0.7151998281478882\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6098390817642212 \t Accuracy = 0.7303998470306396\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5940229892730713 \t Accuracy = 0.7235998511314392\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5714285969734192 \t Accuracy = 0.7171998023986816\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5405520796775818 \t Accuracy = 0.7267998456954956\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5199562311172485 \t Accuracy = 0.7211998701095581\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5880162715911865 \t Accuracy = 0.7331998348236084\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6093406677246094 \t Accuracy = 0.7157998085021973\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 3:  \n",
      "Loss = 0.598228394985199 \t Accuracy = 0.7087998390197754\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5333617329597473 \t Accuracy = 0.725199818611145\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 5:  \n",
      "Loss = 0.518455445766449 \t Accuracy = 0.7225998640060425\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 1:  \n",
      "Loss = 0.60776686668396 \t Accuracy = 0.7271998524665833\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5896241068840027 \t Accuracy = 0.7273998260498047\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5416356325149536 \t Accuracy = 0.7255998253822327\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5332964658737183 \t Accuracy = 0.7263997793197632\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5039981603622437 \t Accuracy = 0.7315998077392578\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5923371911048889 \t Accuracy = 0.7275997400283813\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5772153735160828 \t Accuracy = 0.7309998273849487\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5367915034294128 \t Accuracy = 0.7289998531341553\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5206696391105652 \t Accuracy = 0.731999933719635\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 5:  \n",
      "Loss = 0.49092090129852295 \t Accuracy = 0.729999840259552\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5771976709365845 \t Accuracy = 0.7367998361587524\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5760303735733032 \t Accuracy = 0.7305998802185059\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5583371520042419 \t Accuracy = 0.7171998620033264\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5166763663291931 \t Accuracy = 0.7287997603416443\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4932492673397064 \t Accuracy = 0.7257997989654541\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5742590427398682 \t Accuracy = 0.7305998802185059\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5891731977462769 \t Accuracy = 0.7287999391555786\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5515635013580322 \t Accuracy = 0.7209998965263367\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5210787057876587 \t Accuracy = 0.7295997738838196\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4905267059803009 \t Accuracy = 0.7241998910903931\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5666860342025757 \t Accuracy = 0.732999861240387\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5684221386909485 \t Accuracy = 0.7337998747825623\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5478032827377319 \t Accuracy = 0.7237998247146606\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5059595704078674 \t Accuracy = 0.7275998592376709\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 5:  \n",
      "Loss = 0.482499897480011 \t Accuracy = 0.7221998572349548\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5530723333358765 \t Accuracy = 0.7311998009681702\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5685784220695496 \t Accuracy = 0.7257998585700989\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 3:  \n",
      "Loss = 0.548444390296936 \t Accuracy = 0.7259998321533203\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5020596981048584 \t Accuracy = 0.7301998734474182\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4753633141517639 \t Accuracy = 0.7271997928619385\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5476795434951782 \t Accuracy = 0.7319998741149902\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5710324048995972 \t Accuracy = 0.7229999303817749\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5580920577049255 \t Accuracy = 0.7197998762130737\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4951081871986389 \t Accuracy = 0.7327998280525208\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4783034920692444 \t Accuracy = 0.726999819278717\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5515249371528625 \t Accuracy = 0.733599841594696\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 2:  \n",
      "Loss = 0.542458176612854 \t Accuracy = 0.7337998151779175\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5304943919181824 \t Accuracy = 0.7293998599052429\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 4:  \n",
      "Loss = 0.48487451672554016 \t Accuracy = 0.733599841594696\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4750499129295349 \t Accuracy = 0.7225998640060425\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 1:  \n",
      "Loss = 0.555073082447052 \t Accuracy = 0.7275998592376709\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5527465343475342 \t Accuracy = 0.7325998544692993\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5269937515258789 \t Accuracy = 0.7277998328208923\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 4:  \n",
      "Loss = 0.47751790285110474 \t Accuracy = 0.739399790763855\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 5:  \n",
      "Loss = 0.46392330527305603 \t Accuracy = 0.7277998328208923\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5354366302490234 \t Accuracy = 0.7343997955322266\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5414618253707886 \t Accuracy = 0.7307998538017273\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5184639096260071 \t Accuracy = 0.7277998328208923\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4767196774482727 \t Accuracy = 0.7363998293876648\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 5:  \n",
      "Loss = 0.45291808247566223 \t Accuracy = 0.7291998863220215\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5302081108093262 \t Accuracy = 0.7319998741149902\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5562486052513123 \t Accuracy = 0.731799840927124\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5123789310455322 \t Accuracy = 0.7299998998641968\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 4:  \n",
      "Loss = 0.46863245964050293 \t Accuracy = 0.7309998869895935\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 5:  \n",
      "Loss = 0.46095651388168335 \t Accuracy = 0.7289998531341553\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5386061668395996 \t Accuracy = 0.7311998009681702\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5300955176353455 \t Accuracy = 0.736599862575531\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5186945199966431 \t Accuracy = 0.7227998375892639\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 4:  \n",
      "Loss = 0.46776559948921204 \t Accuracy = 0.7295998930931091\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 5:  \n",
      "Loss = 0.44911476969718933 \t Accuracy = 0.7293998003005981\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5329281091690063 \t Accuracy = 0.7275998592376709\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 2:  \n",
      "Loss = 0.530555009841919 \t Accuracy = 0.7351998686790466\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5193886756896973 \t Accuracy = 0.7309998869895935\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4723235070705414 \t Accuracy = 0.7389998435974121\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 5:  \n",
      "Loss = 0.44661492109298706 \t Accuracy = 0.7295998334884644\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5264092683792114 \t Accuracy = 0.7301998734474182\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5228069424629211 \t Accuracy = 0.7357999086380005\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 3:  \n",
      "Loss = 0.49656936526298523 \t Accuracy = 0.7303999066352844\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4650494456291199 \t Accuracy = 0.7331998348236084\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4524574875831604 \t Accuracy = 0.7321999073028564\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5335770845413208 \t Accuracy = 0.7297998070716858\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5134803056716919 \t Accuracy = 0.7427998185157776\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4824419915676117 \t Accuracy = 0.7353998422622681\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4555104672908783 \t Accuracy = 0.7335999011993408\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4202369153499603 \t Accuracy = 0.7405998706817627\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5245087146759033 \t Accuracy = 0.7419998645782471\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5266474485397339 \t Accuracy = 0.7331998348236084\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4666904807090759 \t Accuracy = 0.7339997887611389\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 4:  \n",
      "Loss = 0.44822683930397034 \t Accuracy = 0.7417998313903809\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 5:  \n",
      "Loss = 0.420911580324173 \t Accuracy = 0.7417998313903809\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 1:  \n",
      "Loss = 0.504792332649231 \t Accuracy = 0.7399998307228088\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5083545446395874 \t Accuracy = 0.7355998158454895\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4716178774833679 \t Accuracy = 0.7369998097419739\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4403343200683594 \t Accuracy = 0.7407998442649841\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 5:  \n",
      "Loss = 0.42659440636634827 \t Accuracy = 0.7327998280525208\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5070460438728333 \t Accuracy = 0.7379997968673706\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 2:  \n",
      "Loss = 0.527144193649292 \t Accuracy = 0.7409998774528503\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4635615348815918 \t Accuracy = 0.7323998212814331\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4358441233634949 \t Accuracy = 0.738399863243103\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4380794167518616 \t Accuracy = 0.7307999134063721\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5084325075149536 \t Accuracy = 0.7361998558044434\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5305904150009155 \t Accuracy = 0.7337998151779175\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 3:  \n",
      "Loss = 0.46747198700904846 \t Accuracy = 0.7353998422622681\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4332927167415619 \t Accuracy = 0.7433997392654419\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 5:  \n",
      "Loss = 0.418226033449173 \t Accuracy = 0.726999819278717\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 1:  \n",
      "Loss = 0.49438679218292236 \t Accuracy = 0.7389998435974121\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, CIFAR-10 Batch 2:  \n",
      "Loss = 0.49581024050712585 \t Accuracy = 0.7291998267173767\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 3:  \n",
      "Loss = 0.45439085364341736 \t Accuracy = 0.7363998889923096\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 4:  \n",
      "Loss = 0.43169718980789185 \t Accuracy = 0.7399998307228088\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 5:  \n",
      "Loss = 0.39562082290649414 \t Accuracy = 0.7343998551368713\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5041167736053467 \t Accuracy = 0.7323998808860779\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4887263774871826 \t Accuracy = 0.7401998043060303\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 3:  \n",
      "Loss = 0.47342929244041443 \t Accuracy = 0.7245998382568359\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 4:  \n",
      "Loss = 0.44810229539871216 \t Accuracy = 0.7385998964309692\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4078371226787567 \t Accuracy = 0.7349998354911804\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 1:  \n",
      "Loss = 0.48626816272735596 \t Accuracy = 0.7387998104095459\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 2:  \n",
      "Loss = 0.47640928626060486 \t Accuracy = 0.7401998043060303\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 3:  \n",
      "Loss = 0.45503392815589905 \t Accuracy = 0.7355998754501343\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4243406057357788 \t Accuracy = 0.7411998510360718\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 5:  \n",
      "Loss = 0.39073416590690613 \t Accuracy = 0.7343998551368713\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 1:  \n",
      "Loss = 0.47701865434646606 \t Accuracy = 0.7401998043060303\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 2:  \n",
      "Loss = 0.47701382637023926 \t Accuracy = 0.7421998381614685\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 3:  \n",
      "Loss = 0.45240020751953125 \t Accuracy = 0.7289998531341553\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4338531792163849 \t Accuracy = 0.7409998178482056\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 5:  \n",
      "Loss = 0.40958482027053833 \t Accuracy = 0.7265998125076294\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4913997948169708 \t Accuracy = 0.7349998950958252\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4997515380382538 \t Accuracy = 0.7403998970985413\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 3:  \n",
      "Loss = 0.46994537115097046 \t Accuracy = 0.7345998883247375\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 4:  \n",
      "Loss = 0.43060824275016785 \t Accuracy = 0.7427998185157776\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 5:  \n",
      "Loss = 0.39642223715782166 \t Accuracy = 0.72819983959198\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4698488712310791 \t Accuracy = 0.7377997636795044\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 2:  \n",
      "Loss = 0.47558626532554626 \t Accuracy = 0.7351997494697571\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4529021680355072 \t Accuracy = 0.7387999296188354\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 4:  \n",
      "Loss = 0.42774254083633423 \t Accuracy = 0.739599883556366\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3912346363067627 \t Accuracy = 0.7351998686790466\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4776497781276703 \t Accuracy = 0.726399838924408\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 2:  \n",
      "Loss = 0.47221970558166504 \t Accuracy = 0.7331998348236084\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 3:  \n",
      "Loss = 0.44413289427757263 \t Accuracy = 0.7317997813224792\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4041230380535126 \t Accuracy = 0.7381998896598816\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 5:  \n",
      "Loss = 0.39879515767097473 \t Accuracy = 0.7279998660087585\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5050896406173706 \t Accuracy = 0.7315999269485474\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4724820852279663 \t Accuracy = 0.732999861240387\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4478074312210083 \t Accuracy = 0.7409997582435608\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 4:  \n",
      "Loss = 0.40739530324935913 \t Accuracy = 0.7421998977661133\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3868539035320282 \t Accuracy = 0.739599883556366\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 1:  \n",
      "Loss = 0.46814650297164917 \t Accuracy = 0.7399998307228088\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4796183109283447 \t Accuracy = 0.7407997846603394\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4545089602470398 \t Accuracy = 0.7285998463630676\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 4:  \n",
      "Loss = 0.39263975620269775 \t Accuracy = 0.7441998720169067\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4042898416519165 \t Accuracy = 0.7285997867584229\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4791388511657715 \t Accuracy = 0.734799861907959\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4536053538322449 \t Accuracy = 0.7411998510360718\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 3:  \n",
      "Loss = 0.43372249603271484 \t Accuracy = 0.7343997955322266\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 4:  \n",
      "Loss = 0.40400174260139465 \t Accuracy = 0.7421998977661133\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3852529525756836 \t Accuracy = 0.7351998686790466\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 1:  \n",
      "Loss = 0.47222068905830383 \t Accuracy = 0.7387998104095459\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4516725242137909 \t Accuracy = 0.7381998300552368\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4252529740333557 \t Accuracy = 0.7385998368263245\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3992554545402527 \t Accuracy = 0.7435998320579529\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 5:  \n",
      "Loss = 0.37699806690216064 \t Accuracy = 0.738399863243103\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4526522159576416 \t Accuracy = 0.7445998787879944\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4468734860420227 \t Accuracy = 0.7425998449325562\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4147014021873474 \t Accuracy = 0.742999792098999\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3982526659965515 \t Accuracy = 0.7441998720169067\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3827934265136719 \t Accuracy = 0.7345998287200928\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4514448046684265 \t Accuracy = 0.7363998889923096\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 2:  \n",
      "Loss = 0.45899298787117004 \t Accuracy = 0.7443998456001282\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4244595766067505 \t Accuracy = 0.7333998680114746\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3945411443710327 \t Accuracy = 0.7419997453689575\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3694906532764435 \t Accuracy = 0.7423999309539795\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 1:  \n",
      "Loss = 0.45404624938964844 \t Accuracy = 0.744999885559082\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 2:  \n",
      "Loss = 0.44982773065567017 \t Accuracy = 0.7369997501373291\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4069107174873352 \t Accuracy = 0.7429999113082886\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 4:  \n",
      "Loss = 0.39336809515953064 \t Accuracy = 0.7447998523712158\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 5:  \n",
      "Loss = 0.36925166845321655 \t Accuracy = 0.734799861907959\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4526998996734619 \t Accuracy = 0.7403998374938965\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 2:  \n",
      "Loss = 0.43589651584625244 \t Accuracy = 0.7449998259544373\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4140230119228363 \t Accuracy = 0.7427998781204224\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 4:  \n",
      "Loss = 0.38927263021469116 \t Accuracy = 0.7459998726844788\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3577485680580139 \t Accuracy = 0.7325998544692993\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4557972550392151 \t Accuracy = 0.7359998822212219\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 2:  \n",
      "Loss = 0.43817049264907837 \t Accuracy = 0.7459998726844788\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4068751931190491 \t Accuracy = 0.7415997982025146\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3891596794128418 \t Accuracy = 0.7447998523712158\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 5:  \n",
      "Loss = 0.36262351274490356 \t Accuracy = 0.7411999106407166\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4426003694534302 \t Accuracy = 0.7397997975349426\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 2:  \n",
      "Loss = 0.437405526638031 \t Accuracy = 0.7455998063087463\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 3:  \n",
      "Loss = 0.40919336676597595 \t Accuracy = 0.7409998774528503\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3838465213775635 \t Accuracy = 0.7423998117446899\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3438011109828949 \t Accuracy = 0.7371998429298401\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4352746307849884 \t Accuracy = 0.7445998191833496\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4291405975818634 \t Accuracy = 0.7397998571395874\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 3:  \n",
      "Loss = 0.38096320629119873 \t Accuracy = 0.7447998523712158\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 4:  \n",
      "Loss = 0.37710991501808167 \t Accuracy = 0.7435998320579529\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3647434711456299 \t Accuracy = 0.7371999025344849\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 1:  \n",
      "Loss = 0.437160462141037 \t Accuracy = 0.7371999025344849\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 2:  \n",
      "Loss = 0.43569961190223694 \t Accuracy = 0.7435998320579529\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4108251631259918 \t Accuracy = 0.7415997982025146\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3899444043636322 \t Accuracy = 0.7431998252868652\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3446784019470215 \t Accuracy = 0.7403998374938965\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4230126142501831 \t Accuracy = 0.7485998868942261\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4163728952407837 \t Accuracy = 0.7469998598098755\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4101310968399048 \t Accuracy = 0.7385998964309692\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 4:  \n",
      "Loss = 0.40070778131484985 \t Accuracy = 0.7339998483657837\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3498961329460144 \t Accuracy = 0.7381998896598816\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 1:  \n",
      "Loss = 0.443790078163147 \t Accuracy = 0.7385998368263245\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4399035573005676 \t Accuracy = 0.7431998252868652\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 3:  \n",
      "Loss = 0.40855830907821655 \t Accuracy = 0.7409998774528503\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 4:  \n",
      "Loss = 0.39239877462387085 \t Accuracy = 0.7397998571395874\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3488992750644684 \t Accuracy = 0.7411998510360718\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 1:  \n",
      "Loss = 0.42011749744415283 \t Accuracy = 0.744199812412262\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4189199209213257 \t Accuracy = 0.7423997521400452\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 3:  \n",
      "Loss = 0.3995506763458252 \t Accuracy = 0.7459998726844788\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 4:  \n",
      "Loss = 0.38132670521736145 \t Accuracy = 0.7425998449325562\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 5:  \n",
      "Loss = 0.33400437235832214 \t Accuracy = 0.7399998903274536\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 1:  \n",
      "Loss = 0.42185261845588684 \t Accuracy = 0.7463998794555664\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4092310070991516 \t Accuracy = 0.7457997798919678\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 3:  \n",
      "Loss = 0.39537936449050903 \t Accuracy = 0.7425998449325562\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3637992739677429 \t Accuracy = 0.7459998726844788\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 5:  \n",
      "Loss = 0.34284788370132446 \t Accuracy = 0.7395998239517212\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4303934872150421 \t Accuracy = 0.7363998293876648\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4084164500236511 \t Accuracy = 0.7439997792243958\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 3:  \n",
      "Loss = 0.379765123128891 \t Accuracy = 0.7487998008728027\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3766498565673828 \t Accuracy = 0.7391998171806335\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3369455337524414 \t Accuracy = 0.7387998700141907\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 1:  \n",
      "Loss = 0.42366334795951843 \t Accuracy = 0.7449998259544373\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4249492287635803 \t Accuracy = 0.7475998401641846\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 3:  \n",
      "Loss = 0.3877939283847809 \t Accuracy = 0.7425998449325562\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 4:  \n",
      "Loss = 0.36525750160217285 \t Accuracy = 0.7435998916625977\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3226047158241272 \t Accuracy = 0.7403997778892517\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4062681496143341 \t Accuracy = 0.7493999004364014\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 2:  \n",
      "Loss = 0.41603147983551025 \t Accuracy = 0.7447998523712158\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 3:  \n",
      "Loss = 0.36573824286460876 \t Accuracy = 0.7439998984336853\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3681245446205139 \t Accuracy = 0.749599814414978\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3353426158428192 \t Accuracy = 0.7397997975349426\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7405512630939484\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9P8BT87Uqm/7/IcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2584ae252b0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
