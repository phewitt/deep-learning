{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-60712b9de575>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misdir\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mproblem_unittests\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtests\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Dev\\Udacity_Deep-Learning\\Deep-Learning-Project-2\\deep-learning\\image-classification\\problem_unittests.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0munittest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmock\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMagicMock\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 23:\n",
      "Image - Min Value: 4 Max Value: 238\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 6 Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB4DCFD0AFKboAaCw/wXAtOxTlohGsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16ca7e5ebe0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 23\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return (x / x.max())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 1 ..., 0 0 0]]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "one_hot_encoder = None\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    global one_hot_encoder\n",
    "\n",
    "    if one_hot_encoder == None:\n",
    "        one_hot_encoder = pp.LabelBinarizer()\n",
    "        one_hot_encoder.fit(x)\n",
    "\n",
    "    return one_hot_encoder.transform(x)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None,\n",
    "                                             image_shape[0],\n",
    "                                             image_shape[1],\n",
    "                                             image_shape[2]), name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.int32, shape=(None, n_classes), name=\"y\")\n",
    "\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    dim = x_tensor.get_shape().as_list()\n",
    "    shape = list(conv_ksize + (dim[-1],) + (conv_num_outputs,))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    weights = tf.Variable(tf.truncated_normal(shape,0,0.1))\n",
    "    \n",
    "    stride = list((1,)+conv_strides+(1,))\n",
    "    \n",
    "    #conv2d(input, filter, stride, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "    conv_layer = tf.nn.conv2d(x_tensor,weights,stride, padding=\"SAME\", use_cudnn_on_gpu=True)\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    #tf.nn.max_pool(value, ksize, strides, padding, data_format='NHWC', name=None)\n",
    "    ksize = list((1,) + pool_ksize + (1,))\n",
    "    strides = list((1,) + pool_strides + (1,))\n",
    "    \n",
    "    #print(conv_layer)\n",
    "    #print(ksize)\n",
    "    #print(strides)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize, strides, padding=\"SAME\")\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    #print(x_tensor)\n",
    "    #print(tf.contrib.layers.flatten(x_tensor))\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    #print(x_tensor, num_outputs)\n",
    "    #print(tf.contrib.layers.fully_connected(x_tensor,num_outputs))\n",
    "    return tf.contrib.layers.fully_connected(x_tensor,num_outputs,activation_fn=tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    #print(x_tensor)\n",
    "    #print(num_outputs)\n",
    "    #print(tf.contrib.layers.fully_connected(x_tensor, num_outputs))\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def print_stuff(message, net):\n",
    "    print(message)\n",
    "    print(net)\n",
    "\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    #Prints network layer info if True\n",
    "    do_print = False\n",
    "   \n",
    "    #Convolution and Max Pool Layers\n",
    "    #conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    #net = conv2d_maxpool(x, 64, (5,5), (1,1), (5,5), (3,3))\n",
    "    net = conv2d_maxpool(x, 64, (5,5), (1,1), (5,5), (3,3))\n",
    "\n",
    "    net = conv2d_maxpool(net, 64, (5,5), (1,1), (3,3), (3,3))\n",
    "    #net = conv2d_maxpool(x, 18, (16,16), (1,1), (8,8), (1,1))\n",
    "\n",
    "    if do_print:\n",
    "        print_stuff(\"Maxpool:\", net)\n",
    "        \n",
    "    #Flatten Layer\n",
    "    net = flatten(net)\n",
    "    \n",
    "    if do_print:\n",
    "        print_stuff(\"Flatten:\", net)\n",
    "        \n",
    "    #Tryout a dropout :D\n",
    "    net = tf.nn.dropout(net, keep_prob)\n",
    "\n",
    "    \n",
    "    if do_print:\n",
    "        print_stuff(\"Dropout:\", net)\n",
    "        \n",
    "    #2 Fully Connected Layers\n",
    "    #fully_conn(x_tensor, num_outputs)\n",
    "    net = fully_conn(net, 384)\n",
    "    net = fully_conn(net, 192)\n",
    "   # net = fully_conn(net, 256)\n",
    "\n",
    "\n",
    "    \n",
    "    if do_print:\n",
    "        print_stuff(\"Fully Connected:\", net)\n",
    "    \n",
    "    net = tf.nn.dropout(net, keep_prob)\n",
    "    \n",
    "    #Output Layer!\n",
    "    net = output(net, 10)\n",
    "    \n",
    "    if do_print:\n",
    "        print_stuff(\"Output:\", net)\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    #Run TF session!\n",
    "    session.run(optimizer, feed_dict={x:feature_batch, y:label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    global valid_features, valid_labels\n",
    "    #print(valid_features, valid_labels)\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0})\n",
    "    accuracy = session.run(accuracy, feed_dict={x: valid_features, y:valid_labels, keep_prob:1.0})\n",
    "    \n",
    "    print('\\nLoss = {} \\t Accuracy = {}\\n'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "#epochs = 50\n",
    "#batch_size = 256\n",
    "#keep_probability = 0.25\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 1024\n",
    "keep_probability = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3022396564483643 \t Accuracy = 0.10259999334812164\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 1:  \n",
      "Loss = 2.2659289836883545 \t Accuracy = 0.1111999899148941\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 1:  \n",
      "Loss = 2.21113657951355 \t Accuracy = 0.12139999121427536\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 1:  \n",
      "Loss = 2.186004400253296 \t Accuracy = 0.13439999520778656\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 1:  \n",
      "Loss = 2.167438268661499 \t Accuracy = 0.2093999981880188\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 1:  \n",
      "Loss = 2.1510512828826904 \t Accuracy = 0.22419999539852142\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 1:  \n",
      "Loss = 2.1231532096862793 \t Accuracy = 0.22380000352859497\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 1:  \n",
      "Loss = 2.1061978340148926 \t Accuracy = 0.24079999327659607\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 1:  \n",
      "Loss = 2.0337746143341064 \t Accuracy = 0.2961999773979187\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 1:  \n",
      "Loss = 1.9162343740463257 \t Accuracy = 0.33399999141693115\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 1:  \n",
      "Loss = 1.8006525039672852 \t Accuracy = 0.35599997639656067\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 1:  \n",
      "Loss = 1.716140627861023 \t Accuracy = 0.3797999918460846\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 1:  \n",
      "Loss = 1.6343317031860352 \t Accuracy = 0.41019997000694275\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 1:  \n",
      "Loss = 1.5637626647949219 \t Accuracy = 0.42059996724128723\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 1:  \n",
      "Loss = 1.5001405477523804 \t Accuracy = 0.4466000199317932\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 1:  \n",
      "Loss = 1.4536967277526855 \t Accuracy = 0.4505999684333801\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 1:  \n",
      "Loss = 1.4391640424728394 \t Accuracy = 0.4517999291419983\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 1:  \n",
      "Loss = 1.390807867050171 \t Accuracy = 0.46439996361732483\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 1:  \n",
      "Loss = 1.3579660654067993 \t Accuracy = 0.48100000619888306\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 1:  \n",
      "Loss = 1.3424104452133179 \t Accuracy = 0.47999992966651917\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 1:  \n",
      "Loss = 1.2967150211334229 \t Accuracy = 0.5027999877929688\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 1:  \n",
      "Loss = 1.2791448831558228 \t Accuracy = 0.49799996614456177\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 1:  \n",
      "Loss = 1.2408181428909302 \t Accuracy = 0.506399929523468\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 1:  \n",
      "Loss = 1.2066930532455444 \t Accuracy = 0.5245999097824097\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 1:  \n",
      "Loss = 1.1582870483398438 \t Accuracy = 0.5413999557495117\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 1:  \n",
      "Loss = 1.1674044132232666 \t Accuracy = 0.5335999727249146\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 1:  \n",
      "Loss = 1.1151854991912842 \t Accuracy = 0.5447999238967896\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 1:  \n",
      "Loss = 1.0825436115264893 \t Accuracy = 0.5565999150276184\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 1:  \n",
      "Loss = 1.0747785568237305 \t Accuracy = 0.5561999678611755\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 1:  \n",
      "Loss = 1.035900354385376 \t Accuracy = 0.559999942779541\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 1:  \n",
      "Loss = 1.0240612030029297 \t Accuracy = 0.5705999135971069\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 1:  \n",
      "Loss = 1.0041872262954712 \t Accuracy = 0.5707998871803284\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9841433167457581 \t Accuracy = 0.5677999258041382\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9729578495025635 \t Accuracy = 0.5773999691009521\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9399228096008301 \t Accuracy = 0.5737999081611633\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8924297094345093 \t Accuracy = 0.591999888420105\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9024778008460999 \t Accuracy = 0.5963999032974243\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8603522777557373 \t Accuracy = 0.598599910736084\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8468782901763916 \t Accuracy = 0.5977998971939087\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 1:  \n",
      "Loss = 0.830460786819458 \t Accuracy = 0.5993999242782593\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8370547294616699 \t Accuracy = 0.5995998978614807\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8219438791275024 \t Accuracy = 0.601599931716919\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8120992183685303 \t Accuracy = 0.6085999011993408\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7999441623687744 \t Accuracy = 0.6003998517990112\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7528637647628784 \t Accuracy = 0.6153998970985413\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7701525688171387 \t Accuracy = 0.6029999256134033\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7446122765541077 \t Accuracy = 0.6059999465942383\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7109588384628296 \t Accuracy = 0.6135998964309692\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6870666742324829 \t Accuracy = 0.624799907207489\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7113229036331177 \t Accuracy = 0.605199933052063\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6611666679382324 \t Accuracy = 0.6345998644828796\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6459037065505981 \t Accuracy = 0.6303999423980713\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6449329853057861 \t Accuracy = 0.6245998740196228\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5993943214416504 \t Accuracy = 0.637199878692627\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5889542102813721 \t Accuracy = 0.6419999003410339\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5920206904411316 \t Accuracy = 0.6345998644828796\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 1:  \n",
      "Loss = 0.57353276014328 \t Accuracy = 0.6379998922348022\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5892436504364014 \t Accuracy = 0.6411999464035034\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5416975021362305 \t Accuracy = 0.6417999863624573\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5464061498641968 \t Accuracy = 0.6391999125480652\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 1:  \n",
      "Loss = 0.543238639831543 \t Accuracy = 0.6365998983383179\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5315191149711609 \t Accuracy = 0.6347998976707458\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5074950456619263 \t Accuracy = 0.643799901008606\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 1:  \n",
      "Loss = 0.47625187039375305 \t Accuracy = 0.6463999152183533\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 1:  \n",
      "Loss = 0.47594866156578064 \t Accuracy = 0.645599901676178\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 1:  \n",
      "Loss = 0.457577645778656 \t Accuracy = 0.6523998975753784\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4418085813522339 \t Accuracy = 0.6513999104499817\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 1:  \n",
      "Loss = 0.43349185585975647 \t Accuracy = 0.6523998975753784\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 1:  \n",
      "Loss = 0.43455058336257935 \t Accuracy = 0.6455998420715332\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 1:  \n",
      "Loss = 0.40846890211105347 \t Accuracy = 0.6513998508453369\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 1:  \n",
      "Loss = 0.40661805868148804 \t Accuracy = 0.6459999084472656\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 1:  \n",
      "Loss = 0.40029576420783997 \t Accuracy = 0.6533998847007751\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4056273400783539 \t Accuracy = 0.657599925994873\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 1:  \n",
      "Loss = 0.379214882850647 \t Accuracy = 0.6567999124526978\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 1:  \n",
      "Loss = 0.3782317638397217 \t Accuracy = 0.6587998867034912\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 1:  \n",
      "Loss = 0.376279354095459 \t Accuracy = 0.65559983253479\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 1:  \n",
      "Loss = 0.3558739423751831 \t Accuracy = 0.6561998724937439\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 1:  \n",
      "Loss = 0.35152003169059753 \t Accuracy = 0.6583998203277588\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 1:  \n",
      "Loss = 0.32584068179130554 \t Accuracy = 0.6603999137878418\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 1:  \n",
      "Loss = 0.3154192566871643 \t Accuracy = 0.6653998494148254\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 1:  \n",
      "Loss = 0.31568580865859985 \t Accuracy = 0.6571999192237854\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 1:  \n",
      "Loss = 0.32155776023864746 \t Accuracy = 0.6585999131202698\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 1:  \n",
      "Loss = 0.31737446784973145 \t Accuracy = 0.6573998928070068\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2841051518917084 \t Accuracy = 0.6681998372077942\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2961489260196686 \t Accuracy = 0.6563999056816101\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 1:  \n",
      "Loss = 0.265175998210907 \t Accuracy = 0.6645998358726501\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2980708181858063 \t Accuracy = 0.6617998480796814\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2734180688858032 \t Accuracy = 0.6647998690605164\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2520563006401062 \t Accuracy = 0.6697998642921448\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 1:  \n",
      "Loss = 0.23385737836360931 \t Accuracy = 0.6697998046875\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 1:  \n",
      "Loss = 0.23784668743610382 \t Accuracy = 0.6633999347686768\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 1:  \n",
      "Loss = 0.22708472609519958 \t Accuracy = 0.6617999076843262\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 1:  \n",
      "Loss = 0.24784019589424133 \t Accuracy = 0.6625999212265015\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 1:  \n",
      "Loss = 0.21064186096191406 \t Accuracy = 0.6649998426437378\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 1:  \n",
      "Loss = 0.20124007761478424 \t Accuracy = 0.6699998378753662\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2062685787677765 \t Accuracy = 0.6745998859405518\n",
      "\n",
      "Epoch 101, CIFAR-10 Batch 1:  \n",
      "Loss = 0.17869693040847778 \t Accuracy = 0.6771999001502991\n",
      "\n",
      "Epoch 102, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2150479257106781 \t Accuracy = 0.658599853515625\n",
      "\n",
      "Epoch 103, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1736830323934555 \t Accuracy = 0.6747998595237732\n",
      "\n",
      "Epoch 104, CIFAR-10 Batch 1:  \n",
      "Loss = 0.18385283648967743 \t Accuracy = 0.6701998710632324\n",
      "\n",
      "Epoch 105, CIFAR-10 Batch 1:  \n",
      "Loss = 0.16977538168430328 \t Accuracy = 0.6691998839378357\n",
      "\n",
      "Epoch 106, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1612495332956314 \t Accuracy = 0.6815998554229736\n",
      "\n",
      "Epoch 107, CIFAR-10 Batch 1:  \n",
      "Loss = 0.16616708040237427 \t Accuracy = 0.6741998195648193\n",
      "\n",
      "Epoch 108, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1464204341173172 \t Accuracy = 0.674799919128418\n",
      "\n",
      "Epoch 109, CIFAR-10 Batch 1:  \n",
      "Loss = 0.14581400156021118 \t Accuracy = 0.6719998121261597\n",
      "\n",
      "Epoch 110, CIFAR-10 Batch 1:  \n",
      "Loss = 0.14387746155261993 \t Accuracy = 0.6669999361038208\n",
      "\n",
      "Epoch 111, CIFAR-10 Batch 1:  \n",
      "Loss = 0.13589465618133545 \t Accuracy = 0.6795998811721802\n",
      "\n",
      "Epoch 112, CIFAR-10 Batch 1:  \n",
      "Loss = 0.13527321815490723 \t Accuracy = 0.6769999265670776\n",
      "\n",
      "Epoch 113, CIFAR-10 Batch 1:  \n",
      "Loss = 0.13374477624893188 \t Accuracy = 0.6713998913764954\n",
      "\n",
      "Epoch 114, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1310025453567505 \t Accuracy = 0.6615999341011047\n",
      "\n",
      "Epoch 115, CIFAR-10 Batch 1:  \n",
      "Loss = 0.13619548082351685 \t Accuracy = 0.6661998629570007\n",
      "\n",
      "Epoch 116, CIFAR-10 Batch 1:  \n",
      "Loss = 0.12714286148548126 \t Accuracy = 0.6715998649597168\n",
      "\n",
      "Epoch 117, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1231255829334259 \t Accuracy = 0.6729998588562012\n",
      "\n",
      "Epoch 118, CIFAR-10 Batch 1:  \n",
      "Loss = 0.11791747063398361 \t Accuracy = 0.6797999143600464\n",
      "\n",
      "Epoch 119, CIFAR-10 Batch 1:  \n",
      "Loss = 0.10660775005817413 \t Accuracy = 0.6695999503135681\n",
      "\n",
      "Epoch 120, CIFAR-10 Batch 1:  \n",
      "Loss = 0.10384654253721237 \t Accuracy = 0.6755999326705933\n",
      "\n",
      "Epoch 121, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0960640087723732 \t Accuracy = 0.6747998595237732\n",
      "\n",
      "Epoch 122, CIFAR-10 Batch 1:  \n",
      "Loss = 0.10294783115386963 \t Accuracy = 0.6695998907089233\n",
      "\n",
      "Epoch 123, CIFAR-10 Batch 1:  \n",
      "Loss = 0.10618536174297333 \t Accuracy = 0.6697999238967896\n",
      "\n",
      "Epoch 124, CIFAR-10 Batch 1:  \n",
      "Loss = 0.11081915348768234 \t Accuracy = 0.6647999286651611\n",
      "\n",
      "Epoch 125, CIFAR-10 Batch 1:  \n",
      "Loss = 0.10371924191713333 \t Accuracy = 0.6655998229980469\n",
      "\n",
      "Epoch 126, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1031787246465683 \t Accuracy = 0.6575998663902283\n",
      "\n",
      "Epoch 127, CIFAR-10 Batch 1:  \n",
      "Loss = 0.09273481369018555 \t Accuracy = 0.6639999151229858\n",
      "\n",
      "Epoch 128, CIFAR-10 Batch 1:  \n",
      "Loss = 0.08169233798980713 \t Accuracy = 0.666999876499176\n",
      "\n",
      "Epoch 129, CIFAR-10 Batch 1:  \n",
      "Loss = 0.07412745803594589 \t Accuracy = 0.6661998629570007\n",
      "\n",
      "Epoch 130, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06817451119422913 \t Accuracy = 0.6731998920440674\n",
      "\n",
      "Epoch 131, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06531580537557602 \t Accuracy = 0.6761998534202576\n",
      "\n",
      "Epoch 132, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06268240511417389 \t Accuracy = 0.6777998805046082\n",
      "\n",
      "Epoch 133, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06681421399116516 \t Accuracy = 0.6739999055862427\n",
      "\n",
      "Epoch 134, CIFAR-10 Batch 1:  \n",
      "Loss = 0.056071970611810684 \t Accuracy = 0.6789999008178711\n",
      "\n",
      "Epoch 135, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06159310042858124 \t Accuracy = 0.6691998839378357\n",
      "\n",
      "Epoch 136, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06105712056159973 \t Accuracy = 0.6743998527526855\n",
      "\n",
      "Epoch 137, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0648321583867073 \t Accuracy = 0.6673998832702637\n",
      "\n",
      "Epoch 138, CIFAR-10 Batch 1:  \n",
      "Loss = 0.056491654366254807 \t Accuracy = 0.6737998723983765\n",
      "\n",
      "Epoch 139, CIFAR-10 Batch 1:  \n",
      "Loss = 0.05419990420341492 \t Accuracy = 0.6803998351097107\n",
      "\n",
      "Epoch 140, CIFAR-10 Batch 1:  \n",
      "Loss = 0.05944998189806938 \t Accuracy = 0.6683998703956604\n",
      "\n",
      "Epoch 141, CIFAR-10 Batch 1:  \n",
      "Loss = 0.057297077029943466 \t Accuracy = 0.6811999082565308\n",
      "\n",
      "Epoch 142, CIFAR-10 Batch 1:  \n",
      "Loss = 0.05388186126947403 \t Accuracy = 0.6733998656272888\n",
      "\n",
      "Epoch 143, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04291348159313202 \t Accuracy = 0.6799998879432678\n",
      "\n",
      "Epoch 144, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04113762453198433 \t Accuracy = 0.6809998154640198\n",
      "\n",
      "Epoch 145, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04263182356953621 \t Accuracy = 0.6785998940467834\n",
      "\n",
      "Epoch 146, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03866299241781235 \t Accuracy = 0.676399827003479\n",
      "\n",
      "Epoch 147, CIFAR-10 Batch 1:  \n",
      "Loss = 0.037410005927085876 \t Accuracy = 0.6741998791694641\n",
      "\n",
      "Epoch 148, CIFAR-10 Batch 1:  \n",
      "Loss = 0.033452581614255905 \t Accuracy = 0.6761998534202576\n",
      "\n",
      "Epoch 149, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03803551569581032 \t Accuracy = 0.6759998202323914\n",
      "\n",
      "Epoch 150, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0389673113822937 \t Accuracy = 0.6697998046875\n",
      "\n",
      "Epoch 151, CIFAR-10 Batch 1:  \n",
      "Loss = 0.038990944623947144 \t Accuracy = 0.6817998886108398\n",
      "\n",
      "Epoch 152, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03135521337389946 \t Accuracy = 0.672799825668335\n",
      "\n",
      "Epoch 153, CIFAR-10 Batch 1:  \n",
      "Loss = 0.030872825533151627 \t Accuracy = 0.6773998141288757\n",
      "\n",
      "Epoch 154, CIFAR-10 Batch 1:  \n",
      "Loss = 0.031735610216856 \t Accuracy = 0.6725998520851135\n",
      "\n",
      "Epoch 155, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03123357892036438 \t Accuracy = 0.6751998662948608\n",
      "\n",
      "Epoch 156, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03106437809765339 \t Accuracy = 0.6785998940467834\n",
      "\n",
      "Epoch 157, CIFAR-10 Batch 1:  \n",
      "Loss = 0.031743310391902924 \t Accuracy = 0.6773998737335205\n",
      "\n",
      "Epoch 158, CIFAR-10 Batch 1:  \n",
      "Loss = 0.027417542412877083 \t Accuracy = 0.6743998527526855\n",
      "\n",
      "Epoch 159, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03107861988246441 \t Accuracy = 0.6713998913764954\n",
      "\n",
      "Epoch 160, CIFAR-10 Batch 1:  \n",
      "Loss = 0.023216955363750458 \t Accuracy = 0.6757998466491699\n",
      "\n",
      "Epoch 161, CIFAR-10 Batch 1:  \n",
      "Loss = 0.023152748122811317 \t Accuracy = 0.6789999008178711\n",
      "\n",
      "Epoch 162, CIFAR-10 Batch 1:  \n",
      "Loss = 0.021438321098685265 \t Accuracy = 0.6775999069213867\n",
      "\n",
      "Epoch 163, CIFAR-10 Batch 1:  \n",
      "Loss = 0.022730499505996704 \t Accuracy = 0.6819998025894165\n",
      "\n",
      "Epoch 164, CIFAR-10 Batch 1:  \n",
      "Loss = 0.020947346463799477 \t Accuracy = 0.673599898815155\n",
      "\n",
      "Epoch 165, CIFAR-10 Batch 1:  \n",
      "Loss = 0.020143665373325348 \t Accuracy = 0.6783998012542725\n",
      "\n",
      "Epoch 166, CIFAR-10 Batch 1:  \n",
      "Loss = 0.018420230597257614 \t Accuracy = 0.6821998357772827\n",
      "\n",
      "Epoch 167, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01857060007750988 \t Accuracy = 0.6799998879432678\n",
      "\n",
      "Epoch 168, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014353133738040924 \t Accuracy = 0.6785998940467834\n",
      "\n",
      "Epoch 169, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01535292062908411 \t Accuracy = 0.6807999014854431\n",
      "\n",
      "Epoch 170, CIFAR-10 Batch 1:  \n",
      "Loss = 0.015058677643537521 \t Accuracy = 0.6759998202323914\n",
      "\n",
      "Epoch 171, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01590869575738907 \t Accuracy = 0.6793999075889587\n",
      "\n",
      "Epoch 172, CIFAR-10 Batch 1:  \n",
      "Loss = 0.019560348242521286 \t Accuracy = 0.6675999164581299\n",
      "\n",
      "Epoch 173, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014314220286905766 \t Accuracy = 0.6779998540878296\n",
      "\n",
      "Epoch 174, CIFAR-10 Batch 1:  \n",
      "Loss = 0.013561563566327095 \t Accuracy = 0.678399920463562\n",
      "\n",
      "Epoch 175, CIFAR-10 Batch 1:  \n",
      "Loss = 0.013870880007743835 \t Accuracy = 0.6791998744010925\n",
      "\n",
      "Epoch 176, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014212493784725666 \t Accuracy = 0.6831998825073242\n",
      "\n",
      "Epoch 177, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014853711239993572 \t Accuracy = 0.6783998608589172\n",
      "\n",
      "Epoch 178, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01336304284632206 \t Accuracy = 0.6801998615264893\n",
      "\n",
      "Epoch 179, CIFAR-10 Batch 1:  \n",
      "Loss = 0.015151698142290115 \t Accuracy = 0.6807998418807983\n",
      "\n",
      "Epoch 180, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01193990744650364 \t Accuracy = 0.6785998940467834\n",
      "\n",
      "Epoch 181, CIFAR-10 Batch 1:  \n",
      "Loss = 0.012717228382825851 \t Accuracy = 0.6805998682975769\n",
      "\n",
      "Epoch 182, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014494689181447029 \t Accuracy = 0.6773999333381653\n",
      "\n",
      "Epoch 183, CIFAR-10 Batch 1:  \n",
      "Loss = 0.011160027235746384 \t Accuracy = 0.6751999258995056\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016098618507385254 \t Accuracy = 0.6769998669624329\n",
      "\n",
      "Epoch 185, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01108364574611187 \t Accuracy = 0.6733998656272888\n",
      "\n",
      "Epoch 186, CIFAR-10 Batch 1:  \n",
      "Loss = 0.017185915261507034 \t Accuracy = 0.6563998460769653\n",
      "\n",
      "Epoch 187, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01916857808828354 \t Accuracy = 0.6665999889373779\n",
      "\n",
      "Epoch 188, CIFAR-10 Batch 1:  \n",
      "Loss = 0.015796706080436707 \t Accuracy = 0.6609998345375061\n",
      "\n",
      "Epoch 189, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014124900102615356 \t Accuracy = 0.669999897480011\n",
      "\n",
      "Epoch 190, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01945437490940094 \t Accuracy = 0.6593998670578003\n",
      "\n",
      "Epoch 191, CIFAR-10 Batch 1:  \n",
      "Loss = 0.019256705418229103 \t Accuracy = 0.6587998867034912\n",
      "\n",
      "Epoch 192, CIFAR-10 Batch 1:  \n",
      "Loss = 0.015334812924265862 \t Accuracy = 0.6649998426437378\n",
      "\n",
      "Epoch 193, CIFAR-10 Batch 1:  \n",
      "Loss = 0.019729772582650185 \t Accuracy = 0.6581999063491821\n",
      "\n",
      "Epoch 194, CIFAR-10 Batch 1:  \n",
      "Loss = 0.025110680609941483 \t Accuracy = 0.660399854183197\n",
      "\n",
      "Epoch 195, CIFAR-10 Batch 1:  \n",
      "Loss = 0.02050752565264702 \t Accuracy = 0.6625998616218567\n",
      "\n",
      "Epoch 196, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014492399990558624 \t Accuracy = 0.666199803352356\n",
      "\n",
      "Epoch 197, CIFAR-10 Batch 1:  \n",
      "Loss = 0.011021672748029232 \t Accuracy = 0.6693999171257019\n",
      "\n",
      "Epoch 198, CIFAR-10 Batch 1:  \n",
      "Loss = 0.00939261820167303 \t Accuracy = 0.6763998866081238\n",
      "\n",
      "Epoch 199, CIFAR-10 Batch 1:  \n",
      "Loss = 0.010310965590178967 \t Accuracy = 0.6807998418807983\n",
      "\n",
      "Epoch 200, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01482093520462513 \t Accuracy = 0.6657999157905579\n",
      "\n",
      "Epoch 201, CIFAR-10 Batch 1:  \n",
      "Loss = 0.015699446201324463 \t Accuracy = 0.6719998717308044\n",
      "\n",
      "Epoch 202, CIFAR-10 Batch 1:  \n",
      "Loss = 0.011425108648836613 \t Accuracy = 0.6691998243331909\n",
      "\n",
      "Epoch 203, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01039423793554306 \t Accuracy = 0.6737999320030212\n",
      "\n",
      "Epoch 204, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0077452948316931725 \t Accuracy = 0.6741999387741089\n",
      "\n",
      "Epoch 205, CIFAR-10 Batch 1:  \n",
      "Loss = 0.008545678108930588 \t Accuracy = 0.6771998405456543\n",
      "\n",
      "Epoch 206, CIFAR-10 Batch 1:  \n",
      "Loss = 0.007855203002691269 \t Accuracy = 0.6709998250007629\n",
      "\n",
      "Epoch 207, CIFAR-10 Batch 1:  \n",
      "Loss = 0.007860840298235416 \t Accuracy = 0.6713998317718506\n",
      "\n",
      "Epoch 208, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0067184800282120705 \t Accuracy = 0.6723998785018921\n",
      "\n",
      "Epoch 209, CIFAR-10 Batch 1:  \n",
      "Loss = 0.00645335391163826 \t Accuracy = 0.6753998398780823\n",
      "\n",
      "Epoch 210, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0076131029054522514 \t Accuracy = 0.6761999130249023\n",
      "\n",
      "Epoch 211, CIFAR-10 Batch 1:  \n",
      "Loss = 0.007825152948498726 \t Accuracy = 0.6753998398780823\n",
      "\n",
      "Epoch 212, CIFAR-10 Batch 1:  \n",
      "Loss = 0.008136248216032982 \t Accuracy = 0.6659998893737793\n",
      "\n",
      "Epoch 213, CIFAR-10 Batch 1:  \n",
      "Loss = 0.008209814317524433 \t Accuracy = 0.6723998785018921\n",
      "\n",
      "Epoch 214, CIFAR-10 Batch 1:  \n",
      "Loss = 0.006223754025995731 \t Accuracy = 0.6713998913764954\n",
      "\n",
      "Epoch 215, CIFAR-10 Batch 1:  \n",
      "Loss = 0.005676358938217163 \t Accuracy = 0.6749998927116394\n",
      "\n",
      "Epoch 216, CIFAR-10 Batch 1:  \n",
      "Loss = 0.006408317945897579 \t Accuracy = 0.6705998778343201\n",
      "\n",
      "Epoch 217, CIFAR-10 Batch 1:  \n",
      "Loss = 0.005559270735830069 \t Accuracy = 0.6749998927116394\n",
      "\n",
      "Epoch 218, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0048687029629945755 \t Accuracy = 0.6679998636245728\n",
      "\n",
      "Epoch 219, CIFAR-10 Batch 1:  \n",
      "Loss = 0.005907486192882061 \t Accuracy = 0.6685999035835266\n",
      "\n",
      "Epoch 220, CIFAR-10 Batch 1:  \n",
      "Loss = 0.004462052136659622 \t Accuracy = 0.66839998960495\n",
      "\n",
      "Epoch 221, CIFAR-10 Batch 1:  \n",
      "Loss = 0.005249097011983395 \t Accuracy = 0.6667999029159546\n",
      "\n",
      "Epoch 222, CIFAR-10 Batch 1:  \n",
      "Loss = 0.005742100533097982 \t Accuracy = 0.6603999137878418\n",
      "\n",
      "Epoch 223, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0072461506351828575 \t Accuracy = 0.6637998819351196\n",
      "\n",
      "Epoch 224, CIFAR-10 Batch 1:  \n",
      "Loss = 0.006401302758604288 \t Accuracy = 0.66159987449646\n",
      "\n",
      "Epoch 225, CIFAR-10 Batch 1:  \n",
      "Loss = 0.006699700374156237 \t Accuracy = 0.6665999293327332\n",
      "\n",
      "Epoch 226, CIFAR-10 Batch 1:  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-0ebd1bbc35ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-161-ebadf2c2aa97>\u001b[0m in \u001b[0;36mprint_stats\u001b[1;34m(session, feature_batch, label_batch, cost, accuracy)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#print(valid_features, valid_labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalid_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mvalid_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nLoss = {} \\t Accuracy = {}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Parker\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Parker\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Parker\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\Parker\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Parker\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 2:  \n",
      "Loss = 2.3025851249694824 \t Accuracy = 0.09780000150203705\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 3:  \n",
      "Loss = 2.3016457557678223 \t Accuracy = 0.11079999059438705\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 4:  \n",
      "Loss = 2.262849807739258 \t Accuracy = 0.11459998786449432\n",
      "\n",
      "Epoch  1, CIFAR-10 Batch 5:  \n",
      "Loss = 2.2408368587493896 \t Accuracy = 0.11899998784065247\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 1:  \n",
      "Loss = 2.2200701236724854 \t Accuracy = 0.16339999437332153\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 2:  \n",
      "Loss = 2.146300792694092 \t Accuracy = 0.1802000105381012\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 3:  \n",
      "Loss = 2.1310806274414062 \t Accuracy = 0.2621999979019165\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 4:  \n",
      "Loss = 2.0378243923187256 \t Accuracy = 0.27239999175071716\n",
      "\n",
      "Epoch  2, CIFAR-10 Batch 5:  \n",
      "Loss = 1.9712275266647339 \t Accuracy = 0.33719998598098755\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 1:  \n",
      "Loss = 1.9266173839569092 \t Accuracy = 0.3487999737262726\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 2:  \n",
      "Loss = 1.8268605470657349 \t Accuracy = 0.3901999592781067\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 3:  \n",
      "Loss = 1.7012580633163452 \t Accuracy = 0.4099999666213989\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 4:  \n",
      "Loss = 1.624494194984436 \t Accuracy = 0.421999990940094\n",
      "\n",
      "Epoch  3, CIFAR-10 Batch 5:  \n",
      "Loss = 1.5838769674301147 \t Accuracy = 0.4351999759674072\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 1:  \n",
      "Loss = 1.5453754663467407 \t Accuracy = 0.4505999684333801\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 2:  \n",
      "Loss = 1.5571653842926025 \t Accuracy = 0.444599986076355\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 3:  \n",
      "Loss = 1.4286760091781616 \t Accuracy = 0.4809999465942383\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 4:  \n",
      "Loss = 1.403273582458496 \t Accuracy = 0.4891999363899231\n",
      "\n",
      "Epoch  4, CIFAR-10 Batch 5:  \n",
      "Loss = 1.4013911485671997 \t Accuracy = 0.5007998943328857\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 1:  \n",
      "Loss = 1.3976017236709595 \t Accuracy = 0.4999999403953552\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 2:  \n",
      "Loss = 1.3896976709365845 \t Accuracy = 0.5143999457359314\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 3:  \n",
      "Loss = 1.3066222667694092 \t Accuracy = 0.5095999240875244\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 4:  \n",
      "Loss = 1.276619553565979 \t Accuracy = 0.5277999639511108\n",
      "\n",
      "Epoch  5, CIFAR-10 Batch 5:  \n",
      "Loss = 1.2924352884292603 \t Accuracy = 0.5299999713897705\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 1:  \n",
      "Loss = 1.30129873752594 \t Accuracy = 0.5359998941421509\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 2:  \n",
      "Loss = 1.281429409980774 \t Accuracy = 0.5433999300003052\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 3:  \n",
      "Loss = 1.189569115638733 \t Accuracy = 0.5553998947143555\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 4:  \n",
      "Loss = 1.201087236404419 \t Accuracy = 0.5453999042510986\n",
      "\n",
      "Epoch  6, CIFAR-10 Batch 5:  \n",
      "Loss = 1.1817115545272827 \t Accuracy = 0.5665999054908752\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 1:  \n",
      "Loss = 1.2286227941513062 \t Accuracy = 0.5559999346733093\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 2:  \n",
      "Loss = 1.201903223991394 \t Accuracy = 0.5775998830795288\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 3:  \n",
      "Loss = 1.1053390502929688 \t Accuracy = 0.5725998878479004\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 4:  \n",
      "Loss = 1.1256932020187378 \t Accuracy = 0.5699998736381531\n",
      "\n",
      "Epoch  7, CIFAR-10 Batch 5:  \n",
      "Loss = 1.1049296855926514 \t Accuracy = 0.5763999819755554\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 1:  \n",
      "Loss = 1.1667180061340332 \t Accuracy = 0.5841999650001526\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 2:  \n",
      "Loss = 1.1563926935195923 \t Accuracy = 0.5781999230384827\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 3:  \n",
      "Loss = 1.097705602645874 \t Accuracy = 0.5795999765396118\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 4:  \n",
      "Loss = 1.0586233139038086 \t Accuracy = 0.5925999283790588\n",
      "\n",
      "Epoch  8, CIFAR-10 Batch 5:  \n",
      "Loss = 1.0331312417984009 \t Accuracy = 0.6001999378204346\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 1:  \n",
      "Loss = 1.121614694595337 \t Accuracy = 0.5973999500274658\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 2:  \n",
      "Loss = 1.0553703308105469 \t Accuracy = 0.610599935054779\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 3:  \n",
      "Loss = 1.02216637134552 \t Accuracy = 0.6017999649047852\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 4:  \n",
      "Loss = 1.0093861818313599 \t Accuracy = 0.6141998767852783\n",
      "\n",
      "Epoch  9, CIFAR-10 Batch 5:  \n",
      "Loss = 0.9755560159683228 \t Accuracy = 0.6153998970985413\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 1:  \n",
      "Loss = 1.0485740900039673 \t Accuracy = 0.6161998510360718\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 2:  \n",
      "Loss = 1.034116506576538 \t Accuracy = 0.6203999519348145\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 3:  \n",
      "Loss = 0.9668872952461243 \t Accuracy = 0.6271999478340149\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 4:  \n",
      "Loss = 0.9461408257484436 \t Accuracy = 0.63319993019104\n",
      "\n",
      "Epoch 10, CIFAR-10 Batch 5:  \n",
      "Loss = 0.935187041759491 \t Accuracy = 0.6321998834609985\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 1:  \n",
      "Loss = 1.0423896312713623 \t Accuracy = 0.621799886226654\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 2:  \n",
      "Loss = 0.9908619523048401 \t Accuracy = 0.6311998963356018\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 3:  \n",
      "Loss = 0.9510461091995239 \t Accuracy = 0.6323999166488647\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 4:  \n",
      "Loss = 0.9170221090316772 \t Accuracy = 0.6411998867988586\n",
      "\n",
      "Epoch 11, CIFAR-10 Batch 5:  \n",
      "Loss = 0.8945530652999878 \t Accuracy = 0.6375998854637146\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9599606990814209 \t Accuracy = 0.6525999307632446\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 2:  \n",
      "Loss = 0.9570242762565613 \t Accuracy = 0.6497999429702759\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 3:  \n",
      "Loss = 0.8991039991378784 \t Accuracy = 0.6417999267578125\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 4:  \n",
      "Loss = 0.893220067024231 \t Accuracy = 0.6477999091148376\n",
      "\n",
      "Epoch 12, CIFAR-10 Batch 5:  \n",
      "Loss = 0.860352098941803 \t Accuracy = 0.6465998888015747\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 1:  \n",
      "Loss = 0.9310729503631592 \t Accuracy = 0.6591998934745789\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 2:  \n",
      "Loss = 0.9308755993843079 \t Accuracy = 0.6573998928070068\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 3:  \n",
      "Loss = 0.8391824960708618 \t Accuracy = 0.665199875831604\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 4:  \n",
      "Loss = 0.844780445098877 \t Accuracy = 0.6635998487472534\n",
      "\n",
      "Epoch 13, CIFAR-10 Batch 5:  \n",
      "Loss = 0.8220146298408508 \t Accuracy = 0.6621999144554138\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8857235908508301 \t Accuracy = 0.6649999022483826\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 2:  \n",
      "Loss = 0.9006284475326538 \t Accuracy = 0.6629999279975891\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 3:  \n",
      "Loss = 0.8117313385009766 \t Accuracy = 0.6727998852729797\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 4:  \n",
      "Loss = 0.8320801854133606 \t Accuracy = 0.6713998317718506\n",
      "\n",
      "Epoch 14, CIFAR-10 Batch 5:  \n",
      "Loss = 0.7854424715042114 \t Accuracy = 0.679999828338623\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8962808847427368 \t Accuracy = 0.6697998642921448\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 2:  \n",
      "Loss = 0.8629586696624756 \t Accuracy = 0.6713998913764954\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 3:  \n",
      "Loss = 0.8566208481788635 \t Accuracy = 0.6587998867034912\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 4:  \n",
      "Loss = 0.7997336387634277 \t Accuracy = 0.6817998886108398\n",
      "\n",
      "Epoch 15, CIFAR-10 Batch 5:  \n",
      "Loss = 0.7601257562637329 \t Accuracy = 0.6837998628616333\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 1:  \n",
      "Loss = 0.8152515888214111 \t Accuracy = 0.69159996509552\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 2:  \n",
      "Loss = 0.8223937153816223 \t Accuracy = 0.6903998255729675\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 3:  \n",
      "Loss = 0.7465609312057495 \t Accuracy = 0.6875998377799988\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 4:  \n",
      "Loss = 0.7655020356178284 \t Accuracy = 0.674599826335907\n",
      "\n",
      "Epoch 16, CIFAR-10 Batch 5:  \n",
      "Loss = 0.7242520451545715 \t Accuracy = 0.6877998113632202\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7994620203971863 \t Accuracy = 0.6927998661994934\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7987877130508423 \t Accuracy = 0.7013998627662659\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 3:  \n",
      "Loss = 0.7514482140541077 \t Accuracy = 0.6891999244689941\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 4:  \n",
      "Loss = 0.7373383641242981 \t Accuracy = 0.6953998804092407\n",
      "\n",
      "Epoch 17, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6901440620422363 \t Accuracy = 0.7005999088287354\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7495836615562439 \t Accuracy = 0.7029998302459717\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7869501113891602 \t Accuracy = 0.6911998987197876\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 3:  \n",
      "Loss = 0.7180936932563782 \t Accuracy = 0.7003998160362244\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 4:  \n",
      "Loss = 0.7213913798332214 \t Accuracy = 0.7031998634338379\n",
      "\n",
      "Epoch 18, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6725167036056519 \t Accuracy = 0.7069998979568481\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7427859306335449 \t Accuracy = 0.7029998302459717\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7661432027816772 \t Accuracy = 0.6967999935150146\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 3:  \n",
      "Loss = 0.7246593236923218 \t Accuracy = 0.6945998668670654\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6908572912216187 \t Accuracy = 0.7067998647689819\n",
      "\n",
      "Epoch 19, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6588056087493896 \t Accuracy = 0.7083998918533325\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 1:  \n",
      "Loss = 0.7269093990325928 \t Accuracy = 0.7165998220443726\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7607898712158203 \t Accuracy = 0.6919998526573181\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6949728727340698 \t Accuracy = 0.7031998634338379\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6777400970458984 \t Accuracy = 0.7143998742103577\n",
      "\n",
      "Epoch 20, CIFAR-10 Batch 5:  \n",
      "Loss = 0.651023268699646 \t Accuracy = 0.7129998803138733\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6910544633865356 \t Accuracy = 0.7137999534606934\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7053613662719727 \t Accuracy = 0.7165998220443726\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6778244972229004 \t Accuracy = 0.707399845123291\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6432267427444458 \t Accuracy = 0.7213998436927795\n",
      "\n",
      "Epoch 21, CIFAR-10 Batch 5:  \n",
      "Loss = 0.6130452752113342 \t Accuracy = 0.7165998816490173\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6762622594833374 \t Accuracy = 0.7159998416900635\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 2:  \n",
      "Loss = 0.7024633884429932 \t Accuracy = 0.7219998836517334\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6654057502746582 \t Accuracy = 0.7073997855186462\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6263196468353271 \t Accuracy = 0.7173998355865479\n",
      "\n",
      "Epoch 22, CIFAR-10 Batch 5:  \n",
      "Loss = 0.61011803150177 \t Accuracy = 0.7195998430252075\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6604164242744446 \t Accuracy = 0.721599817276001\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6783059239387512 \t Accuracy = 0.7255998253822327\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 3:  \n",
      "Loss = 0.6177593469619751 \t Accuracy = 0.7271998524665833\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 4:  \n",
      "Loss = 0.599956750869751 \t Accuracy = 0.7287998199462891\n",
      "\n",
      "Epoch 23, CIFAR-10 Batch 5:  \n",
      "Loss = 0.578813374042511 \t Accuracy = 0.7295998334884644\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6241028308868408 \t Accuracy = 0.7325998544692993\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6441665887832642 \t Accuracy = 0.7369998693466187\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 3:  \n",
      "Loss = 0.582789421081543 \t Accuracy = 0.7289998531341553\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 4:  \n",
      "Loss = 0.6028468608856201 \t Accuracy = 0.7311998605728149\n",
      "\n",
      "Epoch 24, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5631738901138306 \t Accuracy = 0.7319998741149902\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 1:  \n",
      "Loss = 0.6246588230133057 \t Accuracy = 0.7327998280525208\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6229431629180908 \t Accuracy = 0.7367998361587524\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5702261924743652 \t Accuracy = 0.7375998497009277\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5610564351081848 \t Accuracy = 0.732999861240387\n",
      "\n",
      "Epoch 25, CIFAR-10 Batch 5:  \n",
      "Loss = 0.543074369430542 \t Accuracy = 0.7359998226165771\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5926415324211121 \t Accuracy = 0.7365998029708862\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 2:  \n",
      "Loss = 0.6347382068634033 \t Accuracy = 0.7271998524665833\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5506992340087891 \t Accuracy = 0.7363998889923096\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 4:  \n",
      "Loss = 0.576600968837738 \t Accuracy = 0.7383998036384583\n",
      "\n",
      "Epoch 26, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5385412573814392 \t Accuracy = 0.7419998645782471\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5710379481315613 \t Accuracy = 0.7361997961997986\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5984612703323364 \t Accuracy = 0.7417997717857361\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5484160780906677 \t Accuracy = 0.7381998896598816\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5647433996200562 \t Accuracy = 0.7351998090744019\n",
      "\n",
      "Epoch 27, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5203186273574829 \t Accuracy = 0.7447998523712158\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5868538618087769 \t Accuracy = 0.7387998104095459\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5811502933502197 \t Accuracy = 0.7355998754501343\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 3:  \n",
      "Loss = 0.550822913646698 \t Accuracy = 0.733599841594696\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5257362723350525 \t Accuracy = 0.7433998584747314\n",
      "\n",
      "Epoch 28, CIFAR-10 Batch 5:  \n",
      "Loss = 0.49838733673095703 \t Accuracy = 0.7493997812271118\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 1:  \n",
      "Loss = 0.551458477973938 \t Accuracy = 0.7461998462677002\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5692465901374817 \t Accuracy = 0.7427998781204224\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5220016837120056 \t Accuracy = 0.738399863243103\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5199140906333923 \t Accuracy = 0.7479997873306274\n",
      "\n",
      "Epoch 29, CIFAR-10 Batch 5:  \n",
      "Loss = 0.5020965933799744 \t Accuracy = 0.7399998903274536\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5506749153137207 \t Accuracy = 0.7385997772216797\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5671995878219604 \t Accuracy = 0.7387998104095459\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5009685754776001 \t Accuracy = 0.749599814414978\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 4:  \n",
      "Loss = 0.5036783218383789 \t Accuracy = 0.7507997751235962\n",
      "\n",
      "Epoch 30, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4808503985404968 \t Accuracy = 0.7547998428344727\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 1:  \n",
      "Loss = 0.5146865844726562 \t Accuracy = 0.7491998076438904\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5408121347427368 \t Accuracy = 0.7469998598098755\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 3:  \n",
      "Loss = 0.5030055046081543 \t Accuracy = 0.7487998008728027\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 4:  \n",
      "Loss = 0.48362040519714355 \t Accuracy = 0.7503998279571533\n",
      "\n",
      "Epoch 31, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4893971085548401 \t Accuracy = 0.7443997859954834\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 1:  \n",
      "Loss = 0.511838972568512 \t Accuracy = 0.7443998456001282\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 2:  \n",
      "Loss = 0.539280116558075 \t Accuracy = 0.7485998272895813\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4893651306629181 \t Accuracy = 0.7465997934341431\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 4:  \n",
      "Loss = 0.47474104166030884 \t Accuracy = 0.742999792098999\n",
      "\n",
      "Epoch 32, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4722227156162262 \t Accuracy = 0.7457998991012573\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4982922673225403 \t Accuracy = 0.7483998537063599\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5424520969390869 \t Accuracy = 0.7457998394966125\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 3:  \n",
      "Loss = 0.45220497250556946 \t Accuracy = 0.747999906539917\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 4:  \n",
      "Loss = 0.45540282130241394 \t Accuracy = 0.7581998109817505\n",
      "\n",
      "Epoch 33, CIFAR-10 Batch 5:  \n",
      "Loss = 0.4351532459259033 \t Accuracy = 0.7577998638153076\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4871927499771118 \t Accuracy = 0.7519998550415039\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5068872570991516 \t Accuracy = 0.7583998441696167\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4493069648742676 \t Accuracy = 0.7567998766899109\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 4:  \n",
      "Loss = 0.43186619877815247 \t Accuracy = 0.7557998299598694\n",
      "\n",
      "Epoch 34, CIFAR-10 Batch 5:  \n",
      "Loss = 0.427936851978302 \t Accuracy = 0.7551998496055603\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 1:  \n",
      "Loss = 0.47090840339660645 \t Accuracy = 0.7593998312950134\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 2:  \n",
      "Loss = 0.5145725011825562 \t Accuracy = 0.7619998455047607\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4617648124694824 \t Accuracy = 0.7473998069763184\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4385267496109009 \t Accuracy = 0.7569998502731323\n",
      "\n",
      "Epoch 35, CIFAR-10 Batch 5:  \n",
      "Loss = 0.42990538477897644 \t Accuracy = 0.7593998312950134\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4679095447063446 \t Accuracy = 0.7557997703552246\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 2:  \n",
      "Loss = 0.504067599773407 \t Accuracy = 0.7553998231887817\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 3:  \n",
      "Loss = 0.4366951286792755 \t Accuracy = 0.7583998441696167\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 4:  \n",
      "Loss = 0.4161010980606079 \t Accuracy = 0.7519997954368591\n",
      "\n",
      "Epoch 36, CIFAR-10 Batch 5:  \n",
      "Loss = 0.41070789098739624 \t Accuracy = 0.7541998624801636\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4502989947795868 \t Accuracy = 0.757199764251709\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4757477343082428 \t Accuracy = 0.7619999051094055\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 3:  \n",
      "Loss = 0.42423856258392334 \t Accuracy = 0.7569998502731323\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3986801505088806 \t Accuracy = 0.7537997364997864\n",
      "\n",
      "Epoch 37, CIFAR-10 Batch 5:  \n",
      "Loss = 0.40716326236724854 \t Accuracy = 0.762599766254425\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 1:  \n",
      "Loss = 0.44204628467559814 \t Accuracy = 0.7641997933387756\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, CIFAR-10 Batch 2:  \n",
      "Loss = 0.47060588002204895 \t Accuracy = 0.7629997730255127\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 3:  \n",
      "Loss = 0.42427074909210205 \t Accuracy = 0.759199857711792\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 4:  \n",
      "Loss = 0.39613863825798035 \t Accuracy = 0.7653998136520386\n",
      "\n",
      "Epoch 38, CIFAR-10 Batch 5:  \n",
      "Loss = 0.39479896426200867 \t Accuracy = 0.7575997114181519\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 1:  \n",
      "Loss = 0.41475027799606323 \t Accuracy = 0.768599808216095\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4481392502784729 \t Accuracy = 0.7677997946739197\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 3:  \n",
      "Loss = 0.411155104637146 \t Accuracy = 0.7633998394012451\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 4:  \n",
      "Loss = 0.39437586069107056 \t Accuracy = 0.7671998143196106\n",
      "\n",
      "Epoch 39, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3965737223625183 \t Accuracy = 0.7547998428344727\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4156632423400879 \t Accuracy = 0.7617998719215393\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 2:  \n",
      "Loss = 0.437502920627594 \t Accuracy = 0.7579997777938843\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 3:  \n",
      "Loss = 0.3889011740684509 \t Accuracy = 0.7603998184204102\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 4:  \n",
      "Loss = 0.38758447766304016 \t Accuracy = 0.7655998468399048\n",
      "\n",
      "Epoch 40, CIFAR-10 Batch 5:  \n",
      "Loss = 0.36331456899642944 \t Accuracy = 0.763799786567688\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 1:  \n",
      "Loss = 0.39413130283355713 \t Accuracy = 0.7623998522758484\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4231713116168976 \t Accuracy = 0.7653998136520386\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 3:  \n",
      "Loss = 0.38197222352027893 \t Accuracy = 0.7657998204231262\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 4:  \n",
      "Loss = 0.36722326278686523 \t Accuracy = 0.7625998258590698\n",
      "\n",
      "Epoch 41, CIFAR-10 Batch 5:  \n",
      "Loss = 0.35956838726997375 \t Accuracy = 0.770399808883667\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 1:  \n",
      "Loss = 0.4061012268066406 \t Accuracy = 0.7643998265266418\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 2:  \n",
      "Loss = 0.42794060707092285 \t Accuracy = 0.7661998271942139\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 3:  \n",
      "Loss = 0.39186713099479675 \t Accuracy = 0.7629998922348022\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3556059002876282 \t Accuracy = 0.770399808883667\n",
      "\n",
      "Epoch 42, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3557347357273102 \t Accuracy = 0.7653998732566833\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 1:  \n",
      "Loss = 0.39000123739242554 \t Accuracy = 0.766799807548523\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 2:  \n",
      "Loss = 0.4099392592906952 \t Accuracy = 0.765799880027771\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 3:  \n",
      "Loss = 0.36175140738487244 \t Accuracy = 0.7705997824668884\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 4:  \n",
      "Loss = 0.34700384736061096 \t Accuracy = 0.7689998149871826\n",
      "\n",
      "Epoch 43, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3673925995826721 \t Accuracy = 0.7609997987747192\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 1:  \n",
      "Loss = 0.39326316118240356 \t Accuracy = 0.7597998380661011\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 2:  \n",
      "Loss = 0.40435925126075745 \t Accuracy = 0.7677998542785645\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 3:  \n",
      "Loss = 0.373345285654068 \t Accuracy = 0.7617998719215393\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 4:  \n",
      "Loss = 0.33805951476097107 \t Accuracy = 0.766799807548523\n",
      "\n",
      "Epoch 44, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3408629894256592 \t Accuracy = 0.7645998001098633\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 1:  \n",
      "Loss = 0.38244885206222534 \t Accuracy = 0.7661998271942139\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 2:  \n",
      "Loss = 0.40385928750038147 \t Accuracy = 0.7639998197555542\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 3:  \n",
      "Loss = 0.34286975860595703 \t Accuracy = 0.767399787902832\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 4:  \n",
      "Loss = 0.3235252797603607 \t Accuracy = 0.7697998285293579\n",
      "\n",
      "Epoch 45, CIFAR-10 Batch 5:  \n",
      "Loss = 0.32564306259155273 \t Accuracy = 0.7695998549461365\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 1:  \n",
      "Loss = 0.3831542432308197 \t Accuracy = 0.7519998550415039\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 2:  \n",
      "Loss = 0.388795405626297 \t Accuracy = 0.7689998149871826\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 3:  \n",
      "Loss = 0.35465776920318604 \t Accuracy = 0.7589998245239258\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 4:  \n",
      "Loss = 0.320392370223999 \t Accuracy = 0.7707998156547546\n",
      "\n",
      "Epoch 46, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3258102238178253 \t Accuracy = 0.7661998271942139\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 1:  \n",
      "Loss = 0.37283554673194885 \t Accuracy = 0.7635998129844666\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 2:  \n",
      "Loss = 0.36754223704338074 \t Accuracy = 0.7705998420715332\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 3:  \n",
      "Loss = 0.3502042293548584 \t Accuracy = 0.7645998001098633\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 4:  \n",
      "Loss = 0.32489192485809326 \t Accuracy = 0.7677998542785645\n",
      "\n",
      "Epoch 47, CIFAR-10 Batch 5:  \n",
      "Loss = 0.31806641817092896 \t Accuracy = 0.7655998468399048\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 1:  \n",
      "Loss = 0.3804507553577423 \t Accuracy = 0.7681998014450073\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 2:  \n",
      "Loss = 0.36475151777267456 \t Accuracy = 0.7705998420715332\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 3:  \n",
      "Loss = 0.3528062105178833 \t Accuracy = 0.7641998529434204\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 4:  \n",
      "Loss = 0.30129241943359375 \t Accuracy = 0.7715998291969299\n",
      "\n",
      "Epoch 48, CIFAR-10 Batch 5:  \n",
      "Loss = 0.32077327370643616 \t Accuracy = 0.7563998699188232\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 1:  \n",
      "Loss = 0.3441088795661926 \t Accuracy = 0.762199878692627\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 2:  \n",
      "Loss = 0.36327144503593445 \t Accuracy = 0.7649997472763062\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 3:  \n",
      "Loss = 0.32181620597839355 \t Accuracy = 0.7693998217582703\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 4:  \n",
      "Loss = 0.28735241293907166 \t Accuracy = 0.7699998021125793\n",
      "\n",
      "Epoch 49, CIFAR-10 Batch 5:  \n",
      "Loss = 0.30589744448661804 \t Accuracy = 0.7659997940063477\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 1:  \n",
      "Loss = 0.33568447828292847 \t Accuracy = 0.7709998488426208\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 2:  \n",
      "Loss = 0.35149627923965454 \t Accuracy = 0.7709997892379761\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 3:  \n",
      "Loss = 0.3130151629447937 \t Accuracy = 0.7693998217582703\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 4:  \n",
      "Loss = 0.27638673782348633 \t Accuracy = 0.7687998414039612\n",
      "\n",
      "Epoch 50, CIFAR-10 Batch 5:  \n",
      "Loss = 0.3017937242984772 \t Accuracy = 0.7699998617172241\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 1:  \n",
      "Loss = 0.32941970229148865 \t Accuracy = 0.767399787902832\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 2:  \n",
      "Loss = 0.346480131149292 \t Accuracy = 0.7697998285293579\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 3:  \n",
      "Loss = 0.3261622190475464 \t Accuracy = 0.7647998332977295\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 4:  \n",
      "Loss = 0.2930949032306671 \t Accuracy = 0.7663998603820801\n",
      "\n",
      "Epoch 51, CIFAR-10 Batch 5:  \n",
      "Loss = 0.2911869287490845 \t Accuracy = 0.767399787902832\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 1:  \n",
      "Loss = 0.3315746784210205 \t Accuracy = 0.767399787902832\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 2:  \n",
      "Loss = 0.34843507409095764 \t Accuracy = 0.7619998455047607\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 3:  \n",
      "Loss = 0.32294973731040955 \t Accuracy = 0.7675998210906982\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 4:  \n",
      "Loss = 0.2824845314025879 \t Accuracy = 0.7613998651504517\n",
      "\n",
      "Epoch 52, CIFAR-10 Batch 5:  \n",
      "Loss = 0.294145792722702 \t Accuracy = 0.7653998136520386\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 1:  \n",
      "Loss = 0.3052879571914673 \t Accuracy = 0.7761998176574707\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 2:  \n",
      "Loss = 0.3261597156524658 \t Accuracy = 0.7695997953414917\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 3:  \n",
      "Loss = 0.3035925626754761 \t Accuracy = 0.7705998420715332\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 4:  \n",
      "Loss = 0.2755574584007263 \t Accuracy = 0.7771998047828674\n",
      "\n",
      "Epoch 53, CIFAR-10 Batch 5:  \n",
      "Loss = 0.2731456756591797 \t Accuracy = 0.7747998237609863\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2999207079410553 \t Accuracy = 0.7719998955726624\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 2:  \n",
      "Loss = 0.3014502227306366 \t Accuracy = 0.7777998447418213\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 3:  \n",
      "Loss = 0.2902038097381592 \t Accuracy = 0.7715998888015747\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 4:  \n",
      "Loss = 0.26299917697906494 \t Accuracy = 0.7735998034477234\n",
      "\n",
      "Epoch 54, CIFAR-10 Batch 5:  \n",
      "Loss = 0.28396081924438477 \t Accuracy = 0.7733997702598572\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 1:  \n",
      "Loss = 0.31260204315185547 \t Accuracy = 0.7747998833656311\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 2:  \n",
      "Loss = 0.326887309551239 \t Accuracy = 0.7645998597145081\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 3:  \n",
      "Loss = 0.29900655150413513 \t Accuracy = 0.7639998197555542\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 4:  \n",
      "Loss = 0.26590749621391296 \t Accuracy = 0.7759997844696045\n",
      "\n",
      "Epoch 55, CIFAR-10 Batch 5:  \n",
      "Loss = 0.26441654562950134 \t Accuracy = 0.7777998447418213\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 1:  \n",
      "Loss = 0.28324052691459656 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 2:  \n",
      "Loss = 0.3047778904438019 \t Accuracy = 0.771399736404419\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 3:  \n",
      "Loss = 0.27054813504219055 \t Accuracy = 0.7761998176574707\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, CIFAR-10 Batch 4:  \n",
      "Loss = 0.24543970823287964 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 56, CIFAR-10 Batch 5:  \n",
      "Loss = 0.26394498348236084 \t Accuracy = 0.7691998481750488\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2772996127605438 \t Accuracy = 0.7833998203277588\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 2:  \n",
      "Loss = 0.29080930352211 \t Accuracy = 0.7727997303009033\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 3:  \n",
      "Loss = 0.270976722240448 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 4:  \n",
      "Loss = 0.25268223881721497 \t Accuracy = 0.7685998678207397\n",
      "\n",
      "Epoch 57, CIFAR-10 Batch 5:  \n",
      "Loss = 0.26386189460754395 \t Accuracy = 0.7757997512817383\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2647540271282196 \t Accuracy = 0.7849997878074646\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 2:  \n",
      "Loss = 0.29288697242736816 \t Accuracy = 0.7713997960090637\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 3:  \n",
      "Loss = 0.2907634377479553 \t Accuracy = 0.7641998529434204\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 4:  \n",
      "Loss = 0.25267863273620605 \t Accuracy = 0.762599766254425\n",
      "\n",
      "Epoch 58, CIFAR-10 Batch 5:  \n",
      "Loss = 0.27150020003318787 \t Accuracy = 0.7689998149871826\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 1:  \n",
      "Loss = 0.28155410289764404 \t Accuracy = 0.7755998373031616\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 2:  \n",
      "Loss = 0.28130966424942017 \t Accuracy = 0.7795999050140381\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 3:  \n",
      "Loss = 0.26154372096061707 \t Accuracy = 0.7763997912406921\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 4:  \n",
      "Loss = 0.24479159712791443 \t Accuracy = 0.7707998156547546\n",
      "\n",
      "Epoch 59, CIFAR-10 Batch 5:  \n",
      "Loss = 0.2463175505399704 \t Accuracy = 0.7791998386383057\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 1:  \n",
      "Loss = 0.26318028569221497 \t Accuracy = 0.7825997471809387\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 2:  \n",
      "Loss = 0.27767738699913025 \t Accuracy = 0.7765998840332031\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 3:  \n",
      "Loss = 0.25519341230392456 \t Accuracy = 0.7755997776985168\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 4:  \n",
      "Loss = 0.2484235018491745 \t Accuracy = 0.7677998542785645\n",
      "\n",
      "Epoch 60, CIFAR-10 Batch 5:  \n",
      "Loss = 0.2519157826900482 \t Accuracy = 0.7697998881340027\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 1:  \n",
      "Loss = 0.287878155708313 \t Accuracy = 0.7723998427391052\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 2:  \n",
      "Loss = 0.28496041893959045 \t Accuracy = 0.7683998346328735\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 3:  \n",
      "Loss = 0.2801843583583832 \t Accuracy = 0.7629998922348022\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 4:  \n",
      "Loss = 0.24018511176109314 \t Accuracy = 0.7693998217582703\n",
      "\n",
      "Epoch 61, CIFAR-10 Batch 5:  \n",
      "Loss = 0.23742803931236267 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 1:  \n",
      "Loss = 0.26182693243026733 \t Accuracy = 0.7739998698234558\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 2:  \n",
      "Loss = 0.2722344696521759 \t Accuracy = 0.7747997641563416\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 3:  \n",
      "Loss = 0.2461363971233368 \t Accuracy = 0.7753998637199402\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 4:  \n",
      "Loss = 0.23020845651626587 \t Accuracy = 0.7675998210906982\n",
      "\n",
      "Epoch 62, CIFAR-10 Batch 5:  \n",
      "Loss = 0.23452910780906677 \t Accuracy = 0.7679998278617859\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 1:  \n",
      "Loss = 0.25460919737815857 \t Accuracy = 0.7727998495101929\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 2:  \n",
      "Loss = 0.26305556297302246 \t Accuracy = 0.7725998163223267\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 3:  \n",
      "Loss = 0.2468755692243576 \t Accuracy = 0.7681998610496521\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 4:  \n",
      "Loss = 0.23721447587013245 \t Accuracy = 0.7737997770309448\n",
      "\n",
      "Epoch 63, CIFAR-10 Batch 5:  \n",
      "Loss = 0.22499828040599823 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2516057789325714 \t Accuracy = 0.7695998549461365\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 2:  \n",
      "Loss = 0.25702551007270813 \t Accuracy = 0.7823997735977173\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 3:  \n",
      "Loss = 0.2344973087310791 \t Accuracy = 0.7723997831344604\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 4:  \n",
      "Loss = 0.22201292216777802 \t Accuracy = 0.7783998250961304\n",
      "\n",
      "Epoch 64, CIFAR-10 Batch 5:  \n",
      "Loss = 0.22317412495613098 \t Accuracy = 0.7765998244285583\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2369256466627121 \t Accuracy = 0.7809998989105225\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 2:  \n",
      "Loss = 0.24573072791099548 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 3:  \n",
      "Loss = 0.2344600111246109 \t Accuracy = 0.7669997811317444\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 4:  \n",
      "Loss = 0.21136754751205444 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 65, CIFAR-10 Batch 5:  \n",
      "Loss = 0.2246093600988388 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 1:  \n",
      "Loss = 0.23552578687667847 \t Accuracy = 0.7817997932434082\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 2:  \n",
      "Loss = 0.23583249747753143 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 3:  \n",
      "Loss = 0.21987055242061615 \t Accuracy = 0.7761998176574707\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 4:  \n",
      "Loss = 0.2188938558101654 \t Accuracy = 0.7709997892379761\n",
      "\n",
      "Epoch 66, CIFAR-10 Batch 5:  \n",
      "Loss = 0.2079700380563736 \t Accuracy = 0.779999852180481\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 1:  \n",
      "Loss = 0.22253844141960144 \t Accuracy = 0.781599760055542\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 2:  \n",
      "Loss = 0.2399268001317978 \t Accuracy = 0.7839998602867126\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 3:  \n",
      "Loss = 0.21132823824882507 \t Accuracy = 0.7803998589515686\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 4:  \n",
      "Loss = 0.1964484453201294 \t Accuracy = 0.7829998731613159\n",
      "\n",
      "Epoch 67, CIFAR-10 Batch 5:  \n",
      "Loss = 0.20109319686889648 \t Accuracy = 0.7823998928070068\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 1:  \n",
      "Loss = 0.21984024345874786 \t Accuracy = 0.7827997803688049\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 2:  \n",
      "Loss = 0.23406849801540375 \t Accuracy = 0.7793998122215271\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 3:  \n",
      "Loss = 0.20069558918476105 \t Accuracy = 0.7773997783660889\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 4:  \n",
      "Loss = 0.19208523631095886 \t Accuracy = 0.7831998467445374\n",
      "\n",
      "Epoch 68, CIFAR-10 Batch 5:  \n",
      "Loss = 0.2063295543193817 \t Accuracy = 0.7829998135566711\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 1:  \n",
      "Loss = 0.20058663189411163 \t Accuracy = 0.7913998365402222\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 2:  \n",
      "Loss = 0.22356495261192322 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 3:  \n",
      "Loss = 0.2051510512828827 \t Accuracy = 0.7719998359680176\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 4:  \n",
      "Loss = 0.18825525045394897 \t Accuracy = 0.7869997620582581\n",
      "\n",
      "Epoch 69, CIFAR-10 Batch 5:  \n",
      "Loss = 0.20223240554332733 \t Accuracy = 0.7711998224258423\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 1:  \n",
      "Loss = 0.2286035120487213 \t Accuracy = 0.7793998122215271\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 2:  \n",
      "Loss = 0.21367990970611572 \t Accuracy = 0.781799852848053\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 3:  \n",
      "Loss = 0.19723941385746002 \t Accuracy = 0.7807998657226562\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 4:  \n",
      "Loss = 0.19268491864204407 \t Accuracy = 0.7799997329711914\n",
      "\n",
      "Epoch 70, CIFAR-10 Batch 5:  \n",
      "Loss = 0.19005225598812103 \t Accuracy = 0.7809998393058777\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 1:  \n",
      "Loss = 0.20871706306934357 \t Accuracy = 0.7861998081207275\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 2:  \n",
      "Loss = 0.21422599256038666 \t Accuracy = 0.7875998616218567\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 3:  \n",
      "Loss = 0.18670432269573212 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 4:  \n",
      "Loss = 0.18011659383773804 \t Accuracy = 0.7839998006820679\n",
      "\n",
      "Epoch 71, CIFAR-10 Batch 5:  \n",
      "Loss = 0.17931078374385834 \t Accuracy = 0.7811998128890991\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 1:  \n",
      "Loss = 0.19943107664585114 \t Accuracy = 0.7819998264312744\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 2:  \n",
      "Loss = 0.1998007893562317 \t Accuracy = 0.7823997735977173\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 3:  \n",
      "Loss = 0.19100835919380188 \t Accuracy = 0.7801998257637024\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 4:  \n",
      "Loss = 0.17338255047798157 \t Accuracy = 0.7877998948097229\n",
      "\n",
      "Epoch 72, CIFAR-10 Batch 5:  \n",
      "Loss = 0.18000738322734833 \t Accuracy = 0.7841997742652893\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 1:  \n",
      "Loss = 0.19298022985458374 \t Accuracy = 0.7875998020172119\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 2:  \n",
      "Loss = 0.19457226991653442 \t Accuracy = 0.7869998216629028\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 3:  \n",
      "Loss = 0.18702873587608337 \t Accuracy = 0.7807998657226562\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 4:  \n",
      "Loss = 0.17806817591190338 \t Accuracy = 0.7831998467445374\n",
      "\n",
      "Epoch 73, CIFAR-10 Batch 5:  \n",
      "Loss = 0.176517054438591 \t Accuracy = 0.7817999124526978\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 1:  \n",
      "Loss = 0.18848468363285065 \t Accuracy = 0.7877998352050781\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 2:  \n",
      "Loss = 0.19334040582180023 \t Accuracy = 0.7869998216629028\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 3:  \n",
      "Loss = 0.17149503529071808 \t Accuracy = 0.7865998148918152\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 4:  \n",
      "Loss = 0.1631791591644287 \t Accuracy = 0.7889997959136963\n",
      "\n",
      "Epoch 74, CIFAR-10 Batch 5:  \n",
      "Loss = 0.1724749654531479 \t Accuracy = 0.773399829864502\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1875169724225998 \t Accuracy = 0.7869997620582581\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 2:  \n",
      "Loss = 0.1864551603794098 \t Accuracy = 0.7843997478485107\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 3:  \n",
      "Loss = 0.19967952370643616 \t Accuracy = 0.773399829864502\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 4:  \n",
      "Loss = 0.1704573929309845 \t Accuracy = 0.7845998406410217\n",
      "\n",
      "Epoch 75, CIFAR-10 Batch 5:  \n",
      "Loss = 0.15945667028427124 \t Accuracy = 0.7823998332023621\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 1:  \n",
      "Loss = 0.17029953002929688 \t Accuracy = 0.785399854183197\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 2:  \n",
      "Loss = 0.18004316091537476 \t Accuracy = 0.7843998074531555\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 3:  \n",
      "Loss = 0.16103844344615936 \t Accuracy = 0.7849998474121094\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 4:  \n",
      "Loss = 0.15675586462020874 \t Accuracy = 0.7867999076843262\n",
      "\n",
      "Epoch 76, CIFAR-10 Batch 5:  \n",
      "Loss = 0.15422260761260986 \t Accuracy = 0.776999831199646\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1683884561061859 \t Accuracy = 0.7831998467445374\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 2:  \n",
      "Loss = 0.17481525242328644 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 3:  \n",
      "Loss = 0.16344040632247925 \t Accuracy = 0.7841997742652893\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 4:  \n",
      "Loss = 0.15384742617607117 \t Accuracy = 0.7861998081207275\n",
      "\n",
      "Epoch 77, CIFAR-10 Batch 5:  \n",
      "Loss = 0.15616372227668762 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 1:  \n",
      "Loss = 0.16779093444347382 \t Accuracy = 0.784599781036377\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 2:  \n",
      "Loss = 0.17569388449192047 \t Accuracy = 0.7861997485160828\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 3:  \n",
      "Loss = 0.16267262399196625 \t Accuracy = 0.7873997688293457\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 4:  \n",
      "Loss = 0.15015056729316711 \t Accuracy = 0.7837998270988464\n",
      "\n",
      "Epoch 78, CIFAR-10 Batch 5:  \n",
      "Loss = 0.15846911072731018 \t Accuracy = 0.7757997512817383\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 1:  \n",
      "Loss = 0.15988010168075562 \t Accuracy = 0.7859998941421509\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 2:  \n",
      "Loss = 0.16224010288715363 \t Accuracy = 0.7853997945785522\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 3:  \n",
      "Loss = 0.1571103185415268 \t Accuracy = 0.7845998406410217\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 4:  \n",
      "Loss = 0.14681489765644073 \t Accuracy = 0.7871997952461243\n",
      "\n",
      "Epoch 79, CIFAR-10 Batch 5:  \n",
      "Loss = 0.15643031895160675 \t Accuracy = 0.7765998244285583\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 1:  \n",
      "Loss = 0.16525568068027496 \t Accuracy = 0.7869997620582581\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 2:  \n",
      "Loss = 0.163740873336792 \t Accuracy = 0.7849998474121094\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 3:  \n",
      "Loss = 0.16074596345424652 \t Accuracy = 0.7839998006820679\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 4:  \n",
      "Loss = 0.14043982326984406 \t Accuracy = 0.7817997932434082\n",
      "\n",
      "Epoch 80, CIFAR-10 Batch 5:  \n",
      "Loss = 0.15012726187705994 \t Accuracy = 0.7739997506141663\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 1:  \n",
      "Loss = 0.15598970651626587 \t Accuracy = 0.7891998291015625\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 2:  \n",
      "Loss = 0.15263113379478455 \t Accuracy = 0.7885998487472534\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 3:  \n",
      "Loss = 0.15998458862304688 \t Accuracy = 0.7825998067855835\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 4:  \n",
      "Loss = 0.1513144075870514 \t Accuracy = 0.7915998697280884\n",
      "\n",
      "Epoch 81, CIFAR-10 Batch 5:  \n",
      "Loss = 0.14940710365772247 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1607399433851242 \t Accuracy = 0.7791998386383057\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 2:  \n",
      "Loss = 0.17691338062286377 \t Accuracy = 0.7789997458457947\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 3:  \n",
      "Loss = 0.1656830757856369 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 4:  \n",
      "Loss = 0.1512405425310135 \t Accuracy = 0.7845999002456665\n",
      "\n",
      "Epoch 82, CIFAR-10 Batch 5:  \n",
      "Loss = 0.14854902029037476 \t Accuracy = 0.7827998399734497\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1560179740190506 \t Accuracy = 0.7907998561859131\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 2:  \n",
      "Loss = 0.14990173280239105 \t Accuracy = 0.7903997898101807\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 3:  \n",
      "Loss = 0.1543368548154831 \t Accuracy = 0.7883998155593872\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 4:  \n",
      "Loss = 0.15253981947898865 \t Accuracy = 0.7837998270988464\n",
      "\n",
      "Epoch 83, CIFAR-10 Batch 5:  \n",
      "Loss = 0.14697423577308655 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 1:  \n",
      "Loss = 0.15730899572372437 \t Accuracy = 0.7837997674942017\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 2:  \n",
      "Loss = 0.1750882863998413 \t Accuracy = 0.7781997919082642\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 3:  \n",
      "Loss = 0.1651856005191803 \t Accuracy = 0.7777998447418213\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 4:  \n",
      "Loss = 0.14416548609733582 \t Accuracy = 0.7851998209953308\n",
      "\n",
      "Epoch 84, CIFAR-10 Batch 5:  \n",
      "Loss = 0.1482471376657486 \t Accuracy = 0.7841998934745789\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1760522723197937 \t Accuracy = 0.7667997479438782\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 2:  \n",
      "Loss = 0.16945454478263855 \t Accuracy = 0.7695997953414917\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 3:  \n",
      "Loss = 0.16569939255714417 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 4:  \n",
      "Loss = 0.14680294692516327 \t Accuracy = 0.7865998148918152\n",
      "\n",
      "Epoch 85, CIFAR-10 Batch 5:  \n",
      "Loss = 0.14270681142807007 \t Accuracy = 0.7857998013496399\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 1:  \n",
      "Loss = 0.16090603172779083 \t Accuracy = 0.7787997722625732\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 2:  \n",
      "Loss = 0.15922632813453674 \t Accuracy = 0.7731997966766357\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 3:  \n",
      "Loss = 0.16243675351142883 \t Accuracy = 0.7753998637199402\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 4:  \n",
      "Loss = 0.14397220313549042 \t Accuracy = 0.7803998589515686\n",
      "\n",
      "Epoch 86, CIFAR-10 Batch 5:  \n",
      "Loss = 0.14960427582263947 \t Accuracy = 0.7841997742652893\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 1:  \n",
      "Loss = 0.15715496242046356 \t Accuracy = 0.7777997851371765\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 2:  \n",
      "Loss = 0.15622378885746002 \t Accuracy = 0.7861998081207275\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 3:  \n",
      "Loss = 0.14177601039409637 \t Accuracy = 0.7871997952461243\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 4:  \n",
      "Loss = 0.14985394477844238 \t Accuracy = 0.7793997526168823\n",
      "\n",
      "Epoch 87, CIFAR-10 Batch 5:  \n",
      "Loss = 0.15563923120498657 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 1:  \n",
      "Loss = 0.15759462118148804 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 2:  \n",
      "Loss = 0.15455974638462067 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 3:  \n",
      "Loss = 0.1618504822254181 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 4:  \n",
      "Loss = 0.14989133179187775 \t Accuracy = 0.7751998901367188\n",
      "\n",
      "Epoch 88, CIFAR-10 Batch 5:  \n",
      "Loss = 0.1428244262933731 \t Accuracy = 0.7725998163223267\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 1:  \n",
      "Loss = 0.15479272603988647 \t Accuracy = 0.7753997445106506\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 2:  \n",
      "Loss = 0.15641915798187256 \t Accuracy = 0.7811998724937439\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 3:  \n",
      "Loss = 0.1519003063440323 \t Accuracy = 0.7839998602867126\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 4:  \n",
      "Loss = 0.14007428288459778 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 89, CIFAR-10 Batch 5:  \n",
      "Loss = 0.1474776417016983 \t Accuracy = 0.7733997702598572\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 1:  \n",
      "Loss = 0.14489434659481049 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 2:  \n",
      "Loss = 0.16486197710037231 \t Accuracy = 0.7749998569488525\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 3:  \n",
      "Loss = 0.1392817497253418 \t Accuracy = 0.7829997539520264\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 4:  \n",
      "Loss = 0.14311635494232178 \t Accuracy = 0.7755998373031616\n",
      "\n",
      "Epoch 90, CIFAR-10 Batch 5:  \n",
      "Loss = 0.1393614560365677 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 1:  \n",
      "Loss = 0.13933686912059784 \t Accuracy = 0.7761998176574707\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 2:  \n",
      "Loss = 0.15257707238197327 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 3:  \n",
      "Loss = 0.15201660990715027 \t Accuracy = 0.7811998128890991\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 4:  \n",
      "Loss = 0.14357249438762665 \t Accuracy = 0.7681998610496521\n",
      "\n",
      "Epoch 91, CIFAR-10 Batch 5:  \n",
      "Loss = 0.14850585162639618 \t Accuracy = 0.7715998291969299\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 1:  \n",
      "Loss = 0.15127874910831451 \t Accuracy = 0.770399808883667\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 2:  \n",
      "Loss = 0.15874682366847992 \t Accuracy = 0.7709998488426208\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 3:  \n",
      "Loss = 0.1431138664484024 \t Accuracy = 0.7757998704910278\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 4:  \n",
      "Loss = 0.1391008496284485 \t Accuracy = 0.7749997973442078\n",
      "\n",
      "Epoch 92, CIFAR-10 Batch 5:  \n",
      "Loss = 0.15345458686351776 \t Accuracy = 0.7699998617172241\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 1:  \n",
      "Loss = 0.13166071474552155 \t Accuracy = 0.77979975938797\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 2:  \n",
      "Loss = 0.17294253408908844 \t Accuracy = 0.768599808216095\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, CIFAR-10 Batch 3:  \n",
      "Loss = 0.14411921799182892 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 4:  \n",
      "Loss = 0.12235510349273682 \t Accuracy = 0.7793998122215271\n",
      "\n",
      "Epoch 93, CIFAR-10 Batch 5:  \n",
      "Loss = 0.1326783001422882 \t Accuracy = 0.7757998108863831\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 1:  \n",
      "Loss = 0.13524769246578217 \t Accuracy = 0.7823998332023621\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 2:  \n",
      "Loss = 0.14427483081817627 \t Accuracy = 0.7735998034477234\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 3:  \n",
      "Loss = 0.13122421503067017 \t Accuracy = 0.7787997722625732\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 4:  \n",
      "Loss = 0.12678742408752441 \t Accuracy = 0.7741998434066772\n",
      "\n",
      "Epoch 94, CIFAR-10 Batch 5:  \n",
      "Loss = 0.1259889006614685 \t Accuracy = 0.7791998386383057\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 1:  \n",
      "Loss = 0.1198040097951889 \t Accuracy = 0.7959997653961182\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 2:  \n",
      "Loss = 0.13634014129638672 \t Accuracy = 0.7837998867034912\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 3:  \n",
      "Loss = 0.10982505977153778 \t Accuracy = 0.7883998155593872\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 4:  \n",
      "Loss = 0.1176837682723999 \t Accuracy = 0.7881998419761658\n",
      "\n",
      "Epoch 95, CIFAR-10 Batch 5:  \n",
      "Loss = 0.12275532633066177 \t Accuracy = 0.7747998237609863\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 1:  \n",
      "Loss = 0.12142843008041382 \t Accuracy = 0.7907997965812683\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 2:  \n",
      "Loss = 0.12206780910491943 \t Accuracy = 0.7895998358726501\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 3:  \n",
      "Loss = 0.11425255239009857 \t Accuracy = 0.7879998683929443\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 4:  \n",
      "Loss = 0.11368675529956818 \t Accuracy = 0.7735998034477234\n",
      "\n",
      "Epoch 96, CIFAR-10 Batch 5:  \n",
      "Loss = 0.13524805009365082 \t Accuracy = 0.7745999097824097\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 1:  \n",
      "Loss = 0.13369785249233246 \t Accuracy = 0.777999758720398\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 2:  \n",
      "Loss = 0.13592036068439484 \t Accuracy = 0.7731997966766357\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 3:  \n",
      "Loss = 0.1186629980802536 \t Accuracy = 0.7867997884750366\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 4:  \n",
      "Loss = 0.10549943149089813 \t Accuracy = 0.783599853515625\n",
      "\n",
      "Epoch 97, CIFAR-10 Batch 5:  \n",
      "Loss = 0.11437837034463882 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 1:  \n",
      "Loss = 0.11618126928806305 \t Accuracy = 0.7875997424125671\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 2:  \n",
      "Loss = 0.12445859611034393 \t Accuracy = 0.7775998115539551\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 3:  \n",
      "Loss = 0.11445540934801102 \t Accuracy = 0.7801997661590576\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 4:  \n",
      "Loss = 0.10495971888303757 \t Accuracy = 0.7821998596191406\n",
      "\n",
      "Epoch 98, CIFAR-10 Batch 5:  \n",
      "Loss = 0.11066706478595734 \t Accuracy = 0.7775998115539551\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 1:  \n",
      "Loss = 0.11616375297307968 \t Accuracy = 0.7807997465133667\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 2:  \n",
      "Loss = 0.12083125114440918 \t Accuracy = 0.7773997783660889\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 3:  \n",
      "Loss = 0.11683681607246399 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 4:  \n",
      "Loss = 0.10124503821134567 \t Accuracy = 0.776999831199646\n",
      "\n",
      "Epoch 99, CIFAR-10 Batch 5:  \n",
      "Loss = 0.09808829426765442 \t Accuracy = 0.7819998264312744\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 1:  \n",
      "Loss = 0.10305974632501602 \t Accuracy = 0.7859998345375061\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 2:  \n",
      "Loss = 0.10996013879776001 \t Accuracy = 0.7841997742652893\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 3:  \n",
      "Loss = 0.11264002323150635 \t Accuracy = 0.7839998602867126\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 4:  \n",
      "Loss = 0.10724704712629318 \t Accuracy = 0.7795998454093933\n",
      "\n",
      "Epoch 100, CIFAR-10 Batch 5:  \n",
      "Loss = 0.12727810442447662 \t Accuracy = 0.7611998319625854\n",
      "\n",
      "Epoch 101, CIFAR-10 Batch 1:  \n",
      "Loss = 0.13802474737167358 \t Accuracy = 0.7807998657226562\n",
      "\n",
      "Epoch 101, CIFAR-10 Batch 2:  \n",
      "Loss = 0.1292884647846222 \t Accuracy = 0.7725998163223267\n",
      "\n",
      "Epoch 101, CIFAR-10 Batch 3:  \n",
      "Loss = 0.11291863024234772 \t Accuracy = 0.7839998006820679\n",
      "\n",
      "Epoch 101, CIFAR-10 Batch 4:  \n",
      "Loss = 0.11043621599674225 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 101, CIFAR-10 Batch 5:  \n",
      "Loss = 0.11002816259860992 \t Accuracy = 0.7747998237609863\n",
      "\n",
      "Epoch 102, CIFAR-10 Batch 1:  \n",
      "Loss = 0.11822964251041412 \t Accuracy = 0.775199830532074\n",
      "\n",
      "Epoch 102, CIFAR-10 Batch 2:  \n",
      "Loss = 0.12261933088302612 \t Accuracy = 0.7755997776985168\n",
      "\n",
      "Epoch 102, CIFAR-10 Batch 3:  \n",
      "Loss = 0.10539711266756058 \t Accuracy = 0.7889997959136963\n",
      "\n",
      "Epoch 102, CIFAR-10 Batch 4:  \n",
      "Loss = 0.09977278113365173 \t Accuracy = 0.772199809551239\n",
      "\n",
      "Epoch 102, CIFAR-10 Batch 5:  \n",
      "Loss = 0.10377094149589539 \t Accuracy = 0.7795998454093933\n",
      "\n",
      "Epoch 103, CIFAR-10 Batch 1:  \n",
      "Loss = 0.10277918726205826 \t Accuracy = 0.7905998229980469\n",
      "\n",
      "Epoch 103, CIFAR-10 Batch 2:  \n",
      "Loss = 0.10324148088693619 \t Accuracy = 0.7855997681617737\n",
      "\n",
      "Epoch 103, CIFAR-10 Batch 3:  \n",
      "Loss = 0.09920720756053925 \t Accuracy = 0.7869998216629028\n",
      "\n",
      "Epoch 103, CIFAR-10 Batch 4:  \n",
      "Loss = 0.08751396834850311 \t Accuracy = 0.7849997878074646\n",
      "\n",
      "Epoch 103, CIFAR-10 Batch 5:  \n",
      "Loss = 0.09574441611766815 \t Accuracy = 0.7873998284339905\n",
      "\n",
      "Epoch 104, CIFAR-10 Batch 1:  \n",
      "Loss = 0.11059355735778809 \t Accuracy = 0.7883998155593872\n",
      "\n",
      "Epoch 104, CIFAR-10 Batch 2:  \n",
      "Loss = 0.10184930264949799 \t Accuracy = 0.7877998352050781\n",
      "\n",
      "Epoch 104, CIFAR-10 Batch 3:  \n",
      "Loss = 0.09127257764339447 \t Accuracy = 0.7861998081207275\n",
      "\n",
      "Epoch 104, CIFAR-10 Batch 4:  \n",
      "Loss = 0.08522088825702667 \t Accuracy = 0.7859998345375061\n",
      "\n",
      "Epoch 104, CIFAR-10 Batch 5:  \n",
      "Loss = 0.09058718383312225 \t Accuracy = 0.7777997851371765\n",
      "\n",
      "Epoch 105, CIFAR-10 Batch 1:  \n",
      "Loss = 0.10443846881389618 \t Accuracy = 0.7853997945785522\n",
      "\n",
      "Epoch 105, CIFAR-10 Batch 2:  \n",
      "Loss = 0.10339708626270294 \t Accuracy = 0.7897998094558716\n",
      "\n",
      "Epoch 105, CIFAR-10 Batch 3:  \n",
      "Loss = 0.09502272307872772 \t Accuracy = 0.7817997932434082\n",
      "\n",
      "Epoch 105, CIFAR-10 Batch 4:  \n",
      "Loss = 0.08405791968107224 \t Accuracy = 0.783599853515625\n",
      "\n",
      "Epoch 105, CIFAR-10 Batch 5:  \n",
      "Loss = 0.09072956442832947 \t Accuracy = 0.7731997966766357\n",
      "\n",
      "Epoch 106, CIFAR-10 Batch 1:  \n",
      "Loss = 0.09679138660430908 \t Accuracy = 0.7909998297691345\n",
      "\n",
      "Epoch 106, CIFAR-10 Batch 2:  \n",
      "Loss = 0.09780542552471161 \t Accuracy = 0.7889998555183411\n",
      "\n",
      "Epoch 106, CIFAR-10 Batch 3:  \n",
      "Loss = 0.09165035933256149 \t Accuracy = 0.7865998148918152\n",
      "\n",
      "Epoch 106, CIFAR-10 Batch 4:  \n",
      "Loss = 0.08690954744815826 \t Accuracy = 0.7859998941421509\n",
      "\n",
      "Epoch 106, CIFAR-10 Batch 5:  \n",
      "Loss = 0.08998461067676544 \t Accuracy = 0.783599853515625\n",
      "\n",
      "Epoch 107, CIFAR-10 Batch 1:  \n",
      "Loss = 0.09596141427755356 \t Accuracy = 0.7875998616218567\n",
      "\n",
      "Epoch 107, CIFAR-10 Batch 2:  \n",
      "Loss = 0.093669593334198 \t Accuracy = 0.7873997688293457\n",
      "\n",
      "Epoch 107, CIFAR-10 Batch 3:  \n",
      "Loss = 0.09318523108959198 \t Accuracy = 0.7897998094558716\n",
      "\n",
      "Epoch 107, CIFAR-10 Batch 4:  \n",
      "Loss = 0.08670730888843536 \t Accuracy = 0.7833998203277588\n",
      "\n",
      "Epoch 107, CIFAR-10 Batch 5:  \n",
      "Loss = 0.09155649691820145 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 108, CIFAR-10 Batch 1:  \n",
      "Loss = 0.09533198922872543 \t Accuracy = 0.7867998480796814\n",
      "\n",
      "Epoch 108, CIFAR-10 Batch 2:  \n",
      "Loss = 0.10245662182569504 \t Accuracy = 0.7819998860359192\n",
      "\n",
      "Epoch 108, CIFAR-10 Batch 3:  \n",
      "Loss = 0.09318582713603973 \t Accuracy = 0.7859997749328613\n",
      "\n",
      "Epoch 108, CIFAR-10 Batch 4:  \n",
      "Loss = 0.07999725639820099 \t Accuracy = 0.787199854850769\n",
      "\n",
      "Epoch 108, CIFAR-10 Batch 5:  \n",
      "Loss = 0.08701588958501816 \t Accuracy = 0.7855998277664185\n",
      "\n",
      "Epoch 109, CIFAR-10 Batch 1:  \n",
      "Loss = 0.09129931777715683 \t Accuracy = 0.7863998413085938\n",
      "\n",
      "Epoch 109, CIFAR-10 Batch 2:  \n",
      "Loss = 0.08767537772655487 \t Accuracy = 0.7877998352050781\n",
      "\n",
      "Epoch 109, CIFAR-10 Batch 3:  \n",
      "Loss = 0.08604559302330017 \t Accuracy = 0.7837998270988464\n",
      "\n",
      "Epoch 109, CIFAR-10 Batch 4:  \n",
      "Loss = 0.08322634547948837 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 109, CIFAR-10 Batch 5:  \n",
      "Loss = 0.08498680591583252 \t Accuracy = 0.7813997864723206\n",
      "\n",
      "Epoch 110, CIFAR-10 Batch 1:  \n",
      "Loss = 0.08776938915252686 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 110, CIFAR-10 Batch 2:  \n",
      "Loss = 0.09965638071298599 \t Accuracy = 0.7847998738288879\n",
      "\n",
      "Epoch 110, CIFAR-10 Batch 3:  \n",
      "Loss = 0.09130352735519409 \t Accuracy = 0.7839998006820679\n",
      "\n",
      "Epoch 110, CIFAR-10 Batch 4:  \n",
      "Loss = 0.08733142912387848 \t Accuracy = 0.7811998724937439\n",
      "\n",
      "Epoch 110, CIFAR-10 Batch 5:  \n",
      "Loss = 0.08922725170850754 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 111, CIFAR-10 Batch 1:  \n",
      "Loss = 0.08953611552715302 \t Accuracy = 0.7853997349739075\n",
      "\n",
      "Epoch 111, CIFAR-10 Batch 2:  \n",
      "Loss = 0.1019098162651062 \t Accuracy = 0.7815998196601868\n",
      "\n",
      "Epoch 111, CIFAR-10 Batch 3:  \n",
      "Loss = 0.09803418815135956 \t Accuracy = 0.7857998609542847\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111, CIFAR-10 Batch 4:  \n",
      "Loss = 0.0846809446811676 \t Accuracy = 0.7849997878074646\n",
      "\n",
      "Epoch 111, CIFAR-10 Batch 5:  \n",
      "Loss = 0.07853800058364868 \t Accuracy = 0.7825998663902283\n",
      "\n",
      "Epoch 112, CIFAR-10 Batch 1:  \n",
      "Loss = 0.09150700271129608 \t Accuracy = 0.7827998399734497\n",
      "\n",
      "Epoch 112, CIFAR-10 Batch 2:  \n",
      "Loss = 0.08202718198299408 \t Accuracy = 0.7843997478485107\n",
      "\n",
      "Epoch 112, CIFAR-10 Batch 3:  \n",
      "Loss = 0.08410704135894775 \t Accuracy = 0.7865997552871704\n",
      "\n",
      "Epoch 112, CIFAR-10 Batch 4:  \n",
      "Loss = 0.08678974211215973 \t Accuracy = 0.7851998209953308\n",
      "\n",
      "Epoch 112, CIFAR-10 Batch 5:  \n",
      "Loss = 0.07416149973869324 \t Accuracy = 0.7821998000144958\n",
      "\n",
      "Epoch 113, CIFAR-10 Batch 1:  \n",
      "Loss = 0.08299347758293152 \t Accuracy = 0.788399875164032\n",
      "\n",
      "Epoch 113, CIFAR-10 Batch 2:  \n",
      "Loss = 0.09049651026725769 \t Accuracy = 0.7897998690605164\n",
      "\n",
      "Epoch 113, CIFAR-10 Batch 3:  \n",
      "Loss = 0.08327941596508026 \t Accuracy = 0.786399781703949\n",
      "\n",
      "Epoch 113, CIFAR-10 Batch 4:  \n",
      "Loss = 0.07105669379234314 \t Accuracy = 0.7877998352050781\n",
      "\n",
      "Epoch 113, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06866589188575745 \t Accuracy = 0.7843998670578003\n",
      "\n",
      "Epoch 114, CIFAR-10 Batch 1:  \n",
      "Loss = 0.08302384614944458 \t Accuracy = 0.7857998609542847\n",
      "\n",
      "Epoch 114, CIFAR-10 Batch 2:  \n",
      "Loss = 0.09082764387130737 \t Accuracy = 0.784599781036377\n",
      "\n",
      "Epoch 114, CIFAR-10 Batch 3:  \n",
      "Loss = 0.08124341070652008 \t Accuracy = 0.7867998480796814\n",
      "\n",
      "Epoch 114, CIFAR-10 Batch 4:  \n",
      "Loss = 0.0768526941537857 \t Accuracy = 0.7847998142242432\n",
      "\n",
      "Epoch 114, CIFAR-10 Batch 5:  \n",
      "Loss = 0.07625329494476318 \t Accuracy = 0.7823997735977173\n",
      "\n",
      "Epoch 115, CIFAR-10 Batch 1:  \n",
      "Loss = 0.08364275097846985 \t Accuracy = 0.7821998596191406\n",
      "\n",
      "Epoch 115, CIFAR-10 Batch 2:  \n",
      "Loss = 0.08809001743793488 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 115, CIFAR-10 Batch 3:  \n",
      "Loss = 0.07812544703483582 \t Accuracy = 0.790199875831604\n",
      "\n",
      "Epoch 115, CIFAR-10 Batch 4:  \n",
      "Loss = 0.07256006449460983 \t Accuracy = 0.7869997620582581\n",
      "\n",
      "Epoch 115, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06696133315563202 \t Accuracy = 0.7867998480796814\n",
      "\n",
      "Epoch 116, CIFAR-10 Batch 1:  \n",
      "Loss = 0.07813212275505066 \t Accuracy = 0.7833998203277588\n",
      "\n",
      "Epoch 116, CIFAR-10 Batch 2:  \n",
      "Loss = 0.10036744177341461 \t Accuracy = 0.7759997844696045\n",
      "\n",
      "Epoch 116, CIFAR-10 Batch 3:  \n",
      "Loss = 0.07712675631046295 \t Accuracy = 0.7837998867034912\n",
      "\n",
      "Epoch 116, CIFAR-10 Batch 4:  \n",
      "Loss = 0.08165958523750305 \t Accuracy = 0.7797998785972595\n",
      "\n",
      "Epoch 116, CIFAR-10 Batch 5:  \n",
      "Loss = 0.08044657856225967 \t Accuracy = 0.7719998359680176\n",
      "\n",
      "Epoch 117, CIFAR-10 Batch 1:  \n",
      "Loss = 0.08342882245779037 \t Accuracy = 0.7891997694969177\n",
      "\n",
      "Epoch 117, CIFAR-10 Batch 2:  \n",
      "Loss = 0.08586981892585754 \t Accuracy = 0.7845998406410217\n",
      "\n",
      "Epoch 117, CIFAR-10 Batch 3:  \n",
      "Loss = 0.07324927300214767 \t Accuracy = 0.787199854850769\n",
      "\n",
      "Epoch 117, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06413957476615906 \t Accuracy = 0.7925997972488403\n",
      "\n",
      "Epoch 117, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06534034013748169 \t Accuracy = 0.7873998284339905\n",
      "\n",
      "Epoch 118, CIFAR-10 Batch 1:  \n",
      "Loss = 0.08419246226549149 \t Accuracy = 0.7807998061180115\n",
      "\n",
      "Epoch 118, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0833355039358139 \t Accuracy = 0.783599853515625\n",
      "\n",
      "Epoch 118, CIFAR-10 Batch 3:  \n",
      "Loss = 0.07928287237882614 \t Accuracy = 0.7837998270988464\n",
      "\n",
      "Epoch 118, CIFAR-10 Batch 4:  \n",
      "Loss = 0.0796373263001442 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 118, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06821011006832123 \t Accuracy = 0.7821998596191406\n",
      "\n",
      "Epoch 119, CIFAR-10 Batch 1:  \n",
      "Loss = 0.09016130864620209 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 119, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0845312848687172 \t Accuracy = 0.7831997871398926\n",
      "\n",
      "Epoch 119, CIFAR-10 Batch 3:  \n",
      "Loss = 0.07722336053848267 \t Accuracy = 0.779999852180481\n",
      "\n",
      "Epoch 119, CIFAR-10 Batch 4:  \n",
      "Loss = 0.07054164260625839 \t Accuracy = 0.7867997884750366\n",
      "\n",
      "Epoch 119, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06400183588266373 \t Accuracy = 0.7821998000144958\n",
      "\n",
      "Epoch 120, CIFAR-10 Batch 1:  \n",
      "Loss = 0.08335478603839874 \t Accuracy = 0.7801997661590576\n",
      "\n",
      "Epoch 120, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07572543621063232 \t Accuracy = 0.7793998718261719\n",
      "\n",
      "Epoch 120, CIFAR-10 Batch 3:  \n",
      "Loss = 0.06618455052375793 \t Accuracy = 0.7857997417449951\n",
      "\n",
      "Epoch 120, CIFAR-10 Batch 4:  \n",
      "Loss = 0.061545729637145996 \t Accuracy = 0.783399760723114\n",
      "\n",
      "Epoch 120, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06801235675811768 \t Accuracy = 0.773999810218811\n",
      "\n",
      "Epoch 121, CIFAR-10 Batch 1:  \n",
      "Loss = 0.07246711105108261 \t Accuracy = 0.7833998799324036\n",
      "\n",
      "Epoch 121, CIFAR-10 Batch 2:  \n",
      "Loss = 0.08167911320924759 \t Accuracy = 0.7787997722625732\n",
      "\n",
      "Epoch 121, CIFAR-10 Batch 3:  \n",
      "Loss = 0.0684676468372345 \t Accuracy = 0.7861998081207275\n",
      "\n",
      "Epoch 121, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06202586740255356 \t Accuracy = 0.7823997735977173\n",
      "\n",
      "Epoch 121, CIFAR-10 Batch 5:  \n",
      "Loss = 0.07021040469408035 \t Accuracy = 0.7665998339653015\n",
      "\n",
      "Epoch 122, CIFAR-10 Batch 1:  \n",
      "Loss = 0.08513141423463821 \t Accuracy = 0.7773998379707336\n",
      "\n",
      "Epoch 122, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07466347515583038 \t Accuracy = 0.7867998480796814\n",
      "\n",
      "Epoch 122, CIFAR-10 Batch 3:  \n",
      "Loss = 0.07694154232740402 \t Accuracy = 0.7845998406410217\n",
      "\n",
      "Epoch 122, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06215260922908783 \t Accuracy = 0.7861998081207275\n",
      "\n",
      "Epoch 122, CIFAR-10 Batch 5:  \n",
      "Loss = 0.07023419439792633 \t Accuracy = 0.7755997776985168\n",
      "\n",
      "Epoch 123, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06877974420785904 \t Accuracy = 0.7815998196601868\n",
      "\n",
      "Epoch 123, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07035791873931885 \t Accuracy = 0.7855998277664185\n",
      "\n",
      "Epoch 123, CIFAR-10 Batch 3:  \n",
      "Loss = 0.0723564475774765 \t Accuracy = 0.7811998724937439\n",
      "\n",
      "Epoch 123, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06283565610647202 \t Accuracy = 0.7825998067855835\n",
      "\n",
      "Epoch 123, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06066856160759926 \t Accuracy = 0.7807998061180115\n",
      "\n",
      "Epoch 124, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06663785874843597 \t Accuracy = 0.7857998013496399\n",
      "\n",
      "Epoch 124, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07469883561134338 \t Accuracy = 0.7799997925758362\n",
      "\n",
      "Epoch 124, CIFAR-10 Batch 3:  \n",
      "Loss = 0.07518459111452103 \t Accuracy = 0.7795998454093933\n",
      "\n",
      "Epoch 124, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06891242414712906 \t Accuracy = 0.7753998041152954\n",
      "\n",
      "Epoch 124, CIFAR-10 Batch 5:  \n",
      "Loss = 0.07099153846502304 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 125, CIFAR-10 Batch 1:  \n",
      "Loss = 0.07994270324707031 \t Accuracy = 0.7807997465133667\n",
      "\n",
      "Epoch 125, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07127686589956284 \t Accuracy = 0.7885997295379639\n",
      "\n",
      "Epoch 125, CIFAR-10 Batch 3:  \n",
      "Loss = 0.06501268595457077 \t Accuracy = 0.785199761390686\n",
      "\n",
      "Epoch 125, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06173098459839821 \t Accuracy = 0.7823997735977173\n",
      "\n",
      "Epoch 125, CIFAR-10 Batch 5:  \n",
      "Loss = 0.058055330067873 \t Accuracy = 0.7827997803688049\n",
      "\n",
      "Epoch 126, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06718987971544266 \t Accuracy = 0.7889998555183411\n",
      "\n",
      "Epoch 126, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07345174998044968 \t Accuracy = 0.7809997797012329\n",
      "\n",
      "Epoch 126, CIFAR-10 Batch 3:  \n",
      "Loss = 0.06919606029987335 \t Accuracy = 0.7851998805999756\n",
      "\n",
      "Epoch 126, CIFAR-10 Batch 4:  \n",
      "Loss = 0.05737064406275749 \t Accuracy = 0.7825998663902283\n",
      "\n",
      "Epoch 126, CIFAR-10 Batch 5:  \n",
      "Loss = 0.052189901471138 \t Accuracy = 0.7919998168945312\n",
      "\n",
      "Epoch 127, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06547492742538452 \t Accuracy = 0.7843997478485107\n",
      "\n",
      "Epoch 127, CIFAR-10 Batch 2:  \n",
      "Loss = 0.06932154297828674 \t Accuracy = 0.7793997526168823\n",
      "\n",
      "Epoch 127, CIFAR-10 Batch 3:  \n",
      "Loss = 0.0672580748796463 \t Accuracy = 0.7859998345375061\n",
      "\n",
      "Epoch 127, CIFAR-10 Batch 4:  \n",
      "Loss = 0.054864611476659775 \t Accuracy = 0.7829997539520264\n",
      "\n",
      "Epoch 127, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06055950000882149 \t Accuracy = 0.7843998670578003\n",
      "\n",
      "Epoch 128, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06560374796390533 \t Accuracy = 0.7843998074531555\n",
      "\n",
      "Epoch 128, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07114069908857346 \t Accuracy = 0.7767997980117798\n",
      "\n",
      "Epoch 128, CIFAR-10 Batch 3:  \n",
      "Loss = 0.06910202652215958 \t Accuracy = 0.789999783039093\n",
      "\n",
      "Epoch 128, CIFAR-10 Batch 4:  \n",
      "Loss = 0.05944782868027687 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 128, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06814965605735779 \t Accuracy = 0.7731998562812805\n",
      "\n",
      "Epoch 129, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06617988646030426 \t Accuracy = 0.7873998284339905\n",
      "\n",
      "Epoch 129, CIFAR-10 Batch 2:  \n",
      "Loss = 0.06533263623714447 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 129, CIFAR-10 Batch 3:  \n",
      "Loss = 0.07038459181785583 \t Accuracy = 0.7853997945785522\n",
      "\n",
      "Epoch 129, CIFAR-10 Batch 4:  \n",
      "Loss = 0.05387445539236069 \t Accuracy = 0.7859998941421509\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129, CIFAR-10 Batch 5:  \n",
      "Loss = 0.056267015635967255 \t Accuracy = 0.7909998297691345\n",
      "\n",
      "Epoch 130, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06419513374567032 \t Accuracy = 0.787199854850769\n",
      "\n",
      "Epoch 130, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07275554537773132 \t Accuracy = 0.7715998291969299\n",
      "\n",
      "Epoch 130, CIFAR-10 Batch 3:  \n",
      "Loss = 0.07711351662874222 \t Accuracy = 0.7777998447418213\n",
      "\n",
      "Epoch 130, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06716085970401764 \t Accuracy = 0.7743998765945435\n",
      "\n",
      "Epoch 130, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06327817589044571 \t Accuracy = 0.7775998115539551\n",
      "\n",
      "Epoch 131, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0746881291270256 \t Accuracy = 0.7799997925758362\n",
      "\n",
      "Epoch 131, CIFAR-10 Batch 2:  \n",
      "Loss = 0.08409228920936584 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 131, CIFAR-10 Batch 3:  \n",
      "Loss = 0.07054028660058975 \t Accuracy = 0.7781998515129089\n",
      "\n",
      "Epoch 131, CIFAR-10 Batch 4:  \n",
      "Loss = 0.0686025321483612 \t Accuracy = 0.7817997336387634\n",
      "\n",
      "Epoch 131, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06761500239372253 \t Accuracy = 0.7809998393058777\n",
      "\n",
      "Epoch 132, CIFAR-10 Batch 1:  \n",
      "Loss = 0.07602813094854355 \t Accuracy = 0.7731998562812805\n",
      "\n",
      "Epoch 132, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0816311463713646 \t Accuracy = 0.7697998285293579\n",
      "\n",
      "Epoch 132, CIFAR-10 Batch 3:  \n",
      "Loss = 0.06818346679210663 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 132, CIFAR-10 Batch 4:  \n",
      "Loss = 0.07509506493806839 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 132, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06497032195329666 \t Accuracy = 0.7819998264312744\n",
      "\n",
      "Epoch 133, CIFAR-10 Batch 1:  \n",
      "Loss = 0.08543667197227478 \t Accuracy = 0.7659998536109924\n",
      "\n",
      "Epoch 133, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07587722688913345 \t Accuracy = 0.7719998359680176\n",
      "\n",
      "Epoch 133, CIFAR-10 Batch 3:  \n",
      "Loss = 0.07483602315187454 \t Accuracy = 0.7729998230934143\n",
      "\n",
      "Epoch 133, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06781937927007675 \t Accuracy = 0.7717998027801514\n",
      "\n",
      "Epoch 133, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06574979424476624 \t Accuracy = 0.7795999050140381\n",
      "\n",
      "Epoch 134, CIFAR-10 Batch 1:  \n",
      "Loss = 0.077022485435009 \t Accuracy = 0.7759997844696045\n",
      "\n",
      "Epoch 134, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0839773565530777 \t Accuracy = 0.7715997695922852\n",
      "\n",
      "Epoch 134, CIFAR-10 Batch 3:  \n",
      "Loss = 0.06596820056438446 \t Accuracy = 0.7827998399734497\n",
      "\n",
      "Epoch 134, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06878648698329926 \t Accuracy = 0.7747998237609863\n",
      "\n",
      "Epoch 134, CIFAR-10 Batch 5:  \n",
      "Loss = 0.06577928364276886 \t Accuracy = 0.778999924659729\n",
      "\n",
      "Epoch 135, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06673560291528702 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 135, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07869072258472443 \t Accuracy = 0.7771998643875122\n",
      "\n",
      "Epoch 135, CIFAR-10 Batch 3:  \n",
      "Loss = 0.0678081065416336 \t Accuracy = 0.7789998650550842\n",
      "\n",
      "Epoch 135, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06227806210517883 \t Accuracy = 0.7771998643875122\n",
      "\n",
      "Epoch 135, CIFAR-10 Batch 5:  \n",
      "Loss = 0.055207401514053345 \t Accuracy = 0.7821998000144958\n",
      "\n",
      "Epoch 136, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06432472914457321 \t Accuracy = 0.7827997803688049\n",
      "\n",
      "Epoch 136, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07219312340021133 \t Accuracy = 0.7753998041152954\n",
      "\n",
      "Epoch 136, CIFAR-10 Batch 3:  \n",
      "Loss = 0.0676148384809494 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 136, CIFAR-10 Batch 4:  \n",
      "Loss = 0.05193035304546356 \t Accuracy = 0.7913997769355774\n",
      "\n",
      "Epoch 136, CIFAR-10 Batch 5:  \n",
      "Loss = 0.05348050221800804 \t Accuracy = 0.7891998291015625\n",
      "\n",
      "Epoch 137, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0636257454752922 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 137, CIFAR-10 Batch 2:  \n",
      "Loss = 0.07358016073703766 \t Accuracy = 0.7767997980117798\n",
      "\n",
      "Epoch 137, CIFAR-10 Batch 3:  \n",
      "Loss = 0.06577742099761963 \t Accuracy = 0.7815998196601868\n",
      "\n",
      "Epoch 137, CIFAR-10 Batch 4:  \n",
      "Loss = 0.05651659518480301 \t Accuracy = 0.785399854183197\n",
      "\n",
      "Epoch 137, CIFAR-10 Batch 5:  \n",
      "Loss = 0.049729324877262115 \t Accuracy = 0.7903998494148254\n",
      "\n",
      "Epoch 138, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0708782821893692 \t Accuracy = 0.7815998196601868\n",
      "\n",
      "Epoch 138, CIFAR-10 Batch 2:  \n",
      "Loss = 0.06646056473255157 \t Accuracy = 0.7783997654914856\n",
      "\n",
      "Epoch 138, CIFAR-10 Batch 3:  \n",
      "Loss = 0.06491027772426605 \t Accuracy = 0.7869998216629028\n",
      "\n",
      "Epoch 138, CIFAR-10 Batch 4:  \n",
      "Loss = 0.05537775903940201 \t Accuracy = 0.7821998596191406\n",
      "\n",
      "Epoch 138, CIFAR-10 Batch 5:  \n",
      "Loss = 0.05083473399281502 \t Accuracy = 0.7813997864723206\n",
      "\n",
      "Epoch 139, CIFAR-10 Batch 1:  \n",
      "Loss = 0.05537073686718941 \t Accuracy = 0.7867998480796814\n",
      "\n",
      "Epoch 139, CIFAR-10 Batch 2:  \n",
      "Loss = 0.061353713274002075 \t Accuracy = 0.7901998162269592\n",
      "\n",
      "Epoch 139, CIFAR-10 Batch 3:  \n",
      "Loss = 0.0636192113161087 \t Accuracy = 0.7831997871398926\n",
      "\n",
      "Epoch 139, CIFAR-10 Batch 4:  \n",
      "Loss = 0.056138988584280014 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 139, CIFAR-10 Batch 5:  \n",
      "Loss = 0.04793646186590195 \t Accuracy = 0.7855998277664185\n",
      "\n",
      "Epoch 140, CIFAR-10 Batch 1:  \n",
      "Loss = 0.05380823463201523 \t Accuracy = 0.7857997417449951\n",
      "\n",
      "Epoch 140, CIFAR-10 Batch 2:  \n",
      "Loss = 0.059687308967113495 \t Accuracy = 0.7839998006820679\n",
      "\n",
      "Epoch 140, CIFAR-10 Batch 3:  \n",
      "Loss = 0.06443481147289276 \t Accuracy = 0.7771998047828674\n",
      "\n",
      "Epoch 140, CIFAR-10 Batch 4:  \n",
      "Loss = 0.056800492107868195 \t Accuracy = 0.778799831867218\n",
      "\n",
      "Epoch 140, CIFAR-10 Batch 5:  \n",
      "Loss = 0.053339362144470215 \t Accuracy = 0.7819997072219849\n",
      "\n",
      "Epoch 141, CIFAR-10 Batch 1:  \n",
      "Loss = 0.05459671840071678 \t Accuracy = 0.7845998406410217\n",
      "\n",
      "Epoch 141, CIFAR-10 Batch 2:  \n",
      "Loss = 0.06191828101873398 \t Accuracy = 0.7881998419761658\n",
      "\n",
      "Epoch 141, CIFAR-10 Batch 3:  \n",
      "Loss = 0.06311079859733582 \t Accuracy = 0.7727998495101929\n",
      "\n",
      "Epoch 141, CIFAR-10 Batch 4:  \n",
      "Loss = 0.05906154215335846 \t Accuracy = 0.7749998569488525\n",
      "\n",
      "Epoch 141, CIFAR-10 Batch 5:  \n",
      "Loss = 0.047593459486961365 \t Accuracy = 0.7855998873710632\n",
      "\n",
      "Epoch 142, CIFAR-10 Batch 1:  \n",
      "Loss = 0.05868220701813698 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 142, CIFAR-10 Batch 2:  \n",
      "Loss = 0.06932883709669113 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 142, CIFAR-10 Batch 3:  \n",
      "Loss = 0.05775082856416702 \t Accuracy = 0.7799997925758362\n",
      "\n",
      "Epoch 142, CIFAR-10 Batch 4:  \n",
      "Loss = 0.05240996927022934 \t Accuracy = 0.7821998000144958\n",
      "\n",
      "Epoch 142, CIFAR-10 Batch 5:  \n",
      "Loss = 0.04918762296438217 \t Accuracy = 0.7843998074531555\n",
      "\n",
      "Epoch 143, CIFAR-10 Batch 1:  \n",
      "Loss = 0.05725671723484993 \t Accuracy = 0.7793997526168823\n",
      "\n",
      "Epoch 143, CIFAR-10 Batch 2:  \n",
      "Loss = 0.061129938811063766 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 143, CIFAR-10 Batch 3:  \n",
      "Loss = 0.051926445215940475 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 143, CIFAR-10 Batch 4:  \n",
      "Loss = 0.05319099500775337 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 143, CIFAR-10 Batch 5:  \n",
      "Loss = 0.046332187950611115 \t Accuracy = 0.7837998867034912\n",
      "\n",
      "Epoch 144, CIFAR-10 Batch 1:  \n",
      "Loss = 0.06008264049887657 \t Accuracy = 0.7807998657226562\n",
      "\n",
      "Epoch 144, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0527382455766201 \t Accuracy = 0.7827998399734497\n",
      "\n",
      "Epoch 144, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04973612725734711 \t Accuracy = 0.7817997932434082\n",
      "\n",
      "Epoch 144, CIFAR-10 Batch 4:  \n",
      "Loss = 0.051650095731019974 \t Accuracy = 0.7855998277664185\n",
      "\n",
      "Epoch 144, CIFAR-10 Batch 5:  \n",
      "Loss = 0.042198725044727325 \t Accuracy = 0.7821998000144958\n",
      "\n",
      "Epoch 145, CIFAR-10 Batch 1:  \n",
      "Loss = 0.047865744680166245 \t Accuracy = 0.781599760055542\n",
      "\n",
      "Epoch 145, CIFAR-10 Batch 2:  \n",
      "Loss = 0.05235063284635544 \t Accuracy = 0.7795999050140381\n",
      "\n",
      "Epoch 145, CIFAR-10 Batch 3:  \n",
      "Loss = 0.05251637473702431 \t Accuracy = 0.7783997654914856\n",
      "\n",
      "Epoch 145, CIFAR-10 Batch 4:  \n",
      "Loss = 0.05522805079817772 \t Accuracy = 0.7767998576164246\n",
      "\n",
      "Epoch 145, CIFAR-10 Batch 5:  \n",
      "Loss = 0.05230315402150154 \t Accuracy = 0.7745997905731201\n",
      "\n",
      "Epoch 146, CIFAR-10 Batch 1:  \n",
      "Loss = 0.05121348053216934 \t Accuracy = 0.7787997722625732\n",
      "\n",
      "Epoch 146, CIFAR-10 Batch 2:  \n",
      "Loss = 0.056181471794843674 \t Accuracy = 0.7743998169898987\n",
      "\n",
      "Epoch 146, CIFAR-10 Batch 3:  \n",
      "Loss = 0.061023637652397156 \t Accuracy = 0.7683998346328735\n",
      "\n",
      "Epoch 146, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04640715569257736 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 146, CIFAR-10 Batch 5:  \n",
      "Loss = 0.04449295252561569 \t Accuracy = 0.7827997803688049\n",
      "\n",
      "Epoch 147, CIFAR-10 Batch 1:  \n",
      "Loss = 0.056581176817417145 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 147, CIFAR-10 Batch 2:  \n",
      "Loss = 0.053511328995227814 \t Accuracy = 0.7807998061180115\n",
      "\n",
      "Epoch 147, CIFAR-10 Batch 3:  \n",
      "Loss = 0.05473118647933006 \t Accuracy = 0.775199830532074\n",
      "\n",
      "Epoch 147, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06582648307085037 \t Accuracy = 0.7691998481750488\n",
      "\n",
      "Epoch 147, CIFAR-10 Batch 5:  \n",
      "Loss = 0.04359491169452667 \t Accuracy = 0.7795997858047485\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148, CIFAR-10 Batch 1:  \n",
      "Loss = 0.046838607639074326 \t Accuracy = 0.7869998216629028\n",
      "\n",
      "Epoch 148, CIFAR-10 Batch 2:  \n",
      "Loss = 0.05455847084522247 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 148, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04810415953397751 \t Accuracy = 0.7829998135566711\n",
      "\n",
      "Epoch 148, CIFAR-10 Batch 4:  \n",
      "Loss = 0.06186901777982712 \t Accuracy = 0.768799901008606\n",
      "\n",
      "Epoch 148, CIFAR-10 Batch 5:  \n",
      "Loss = 0.04586347937583923 \t Accuracy = 0.7761998176574707\n",
      "\n",
      "Epoch 149, CIFAR-10 Batch 1:  \n",
      "Loss = 0.05428187921643257 \t Accuracy = 0.7839998006820679\n",
      "\n",
      "Epoch 149, CIFAR-10 Batch 2:  \n",
      "Loss = 0.05086028575897217 \t Accuracy = 0.7807998061180115\n",
      "\n",
      "Epoch 149, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04967076703906059 \t Accuracy = 0.7839997410774231\n",
      "\n",
      "Epoch 149, CIFAR-10 Batch 4:  \n",
      "Loss = 0.0488826259970665 \t Accuracy = 0.7783998250961304\n",
      "\n",
      "Epoch 149, CIFAR-10 Batch 5:  \n",
      "Loss = 0.036337416619062424 \t Accuracy = 0.7895997762680054\n",
      "\n",
      "Epoch 150, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0492367260158062 \t Accuracy = 0.788399875164032\n",
      "\n",
      "Epoch 150, CIFAR-10 Batch 2:  \n",
      "Loss = 0.05340844765305519 \t Accuracy = 0.7787997722625732\n",
      "\n",
      "Epoch 150, CIFAR-10 Batch 3:  \n",
      "Loss = 0.0522061251103878 \t Accuracy = 0.7791998386383057\n",
      "\n",
      "Epoch 150, CIFAR-10 Batch 4:  \n",
      "Loss = 0.054764166474342346 \t Accuracy = 0.7735998630523682\n",
      "\n",
      "Epoch 150, CIFAR-10 Batch 5:  \n",
      "Loss = 0.0480424165725708 \t Accuracy = 0.7803998589515686\n",
      "\n",
      "Epoch 151, CIFAR-10 Batch 1:  \n",
      "Loss = 0.050811998546123505 \t Accuracy = 0.785399854183197\n",
      "\n",
      "Epoch 151, CIFAR-10 Batch 2:  \n",
      "Loss = 0.05839977413415909 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 151, CIFAR-10 Batch 3:  \n",
      "Loss = 0.05010135844349861 \t Accuracy = 0.7797998785972595\n",
      "\n",
      "Epoch 151, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04782916605472565 \t Accuracy = 0.7835997939109802\n",
      "\n",
      "Epoch 151, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03742894157767296 \t Accuracy = 0.7847998142242432\n",
      "\n",
      "Epoch 152, CIFAR-10 Batch 1:  \n",
      "Loss = 0.049687229096889496 \t Accuracy = 0.7841998934745789\n",
      "\n",
      "Epoch 152, CIFAR-10 Batch 2:  \n",
      "Loss = 0.05755239352583885 \t Accuracy = 0.7713998556137085\n",
      "\n",
      "Epoch 152, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04568380489945412 \t Accuracy = 0.7811998128890991\n",
      "\n",
      "Epoch 152, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04702691733837128 \t Accuracy = 0.7833998203277588\n",
      "\n",
      "Epoch 152, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03921288624405861 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 153, CIFAR-10 Batch 1:  \n",
      "Loss = 0.044359639286994934 \t Accuracy = 0.7867997884750366\n",
      "\n",
      "Epoch 153, CIFAR-10 Batch 2:  \n",
      "Loss = 0.048285678029060364 \t Accuracy = 0.7755997776985168\n",
      "\n",
      "Epoch 153, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04807674139738083 \t Accuracy = 0.7827997803688049\n",
      "\n",
      "Epoch 153, CIFAR-10 Batch 4:  \n",
      "Loss = 0.05019763857126236 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 153, CIFAR-10 Batch 5:  \n",
      "Loss = 0.034262772649526596 \t Accuracy = 0.7825998067855835\n",
      "\n",
      "Epoch 154, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04568800330162048 \t Accuracy = 0.7835997939109802\n",
      "\n",
      "Epoch 154, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0504322424530983 \t Accuracy = 0.7747998237609863\n",
      "\n",
      "Epoch 154, CIFAR-10 Batch 3:  \n",
      "Loss = 0.047147005796432495 \t Accuracy = 0.7779998779296875\n",
      "\n",
      "Epoch 154, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04753825068473816 \t Accuracy = 0.7749997973442078\n",
      "\n",
      "Epoch 154, CIFAR-10 Batch 5:  \n",
      "Loss = 0.04310840368270874 \t Accuracy = 0.7801998257637024\n",
      "\n",
      "Epoch 155, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04172467067837715 \t Accuracy = 0.7833998203277588\n",
      "\n",
      "Epoch 155, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0474669374525547 \t Accuracy = 0.7853997945785522\n",
      "\n",
      "Epoch 155, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04597894102334976 \t Accuracy = 0.7803998589515686\n",
      "\n",
      "Epoch 155, CIFAR-10 Batch 4:  \n",
      "Loss = 0.041203659027814865 \t Accuracy = 0.7827998399734497\n",
      "\n",
      "Epoch 155, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03937258571386337 \t Accuracy = 0.7851998209953308\n",
      "\n",
      "Epoch 156, CIFAR-10 Batch 1:  \n",
      "Loss = 0.047248151153326035 \t Accuracy = 0.7765998244285583\n",
      "\n",
      "Epoch 156, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04719612002372742 \t Accuracy = 0.7777998447418213\n",
      "\n",
      "Epoch 156, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04624096676707268 \t Accuracy = 0.7807998061180115\n",
      "\n",
      "Epoch 156, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04666030406951904 \t Accuracy = 0.7713998556137085\n",
      "\n",
      "Epoch 156, CIFAR-10 Batch 5:  \n",
      "Loss = 0.0472276508808136 \t Accuracy = 0.7739999294281006\n",
      "\n",
      "Epoch 157, CIFAR-10 Batch 1:  \n",
      "Loss = 0.045080047100782394 \t Accuracy = 0.7703998684883118\n",
      "\n",
      "Epoch 157, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04798365756869316 \t Accuracy = 0.7783997654914856\n",
      "\n",
      "Epoch 157, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04671824350953102 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 157, CIFAR-10 Batch 4:  \n",
      "Loss = 0.041447609663009644 \t Accuracy = 0.7745998501777649\n",
      "\n",
      "Epoch 157, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03660571947693825 \t Accuracy = 0.7821998000144958\n",
      "\n",
      "Epoch 158, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04346401244401932 \t Accuracy = 0.7791998386383057\n",
      "\n",
      "Epoch 158, CIFAR-10 Batch 2:  \n",
      "Loss = 0.043879151344299316 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 158, CIFAR-10 Batch 3:  \n",
      "Loss = 0.045273084193468094 \t Accuracy = 0.7833998203277588\n",
      "\n",
      "Epoch 158, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04481067880988121 \t Accuracy = 0.7721998691558838\n",
      "\n",
      "Epoch 158, CIFAR-10 Batch 5:  \n",
      "Loss = 0.036165911704301834 \t Accuracy = 0.7873998284339905\n",
      "\n",
      "Epoch 159, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04327649623155594 \t Accuracy = 0.7809998393058777\n",
      "\n",
      "Epoch 159, CIFAR-10 Batch 2:  \n",
      "Loss = 0.05207622051239014 \t Accuracy = 0.7779998779296875\n",
      "\n",
      "Epoch 159, CIFAR-10 Batch 3:  \n",
      "Loss = 0.042835816740989685 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 159, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04761733114719391 \t Accuracy = 0.7711998224258423\n",
      "\n",
      "Epoch 159, CIFAR-10 Batch 5:  \n",
      "Loss = 0.04061556234955788 \t Accuracy = 0.781599760055542\n",
      "\n",
      "Epoch 160, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04651521146297455 \t Accuracy = 0.7811998128890991\n",
      "\n",
      "Epoch 160, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04655908793210983 \t Accuracy = 0.7761998772621155\n",
      "\n",
      "Epoch 160, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04488642141222954 \t Accuracy = 0.7821998596191406\n",
      "\n",
      "Epoch 160, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04062552750110626 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 160, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03393127769231796 \t Accuracy = 0.7845998406410217\n",
      "\n",
      "Epoch 161, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04481327533721924 \t Accuracy = 0.7795998454093933\n",
      "\n",
      "Epoch 161, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0431366041302681 \t Accuracy = 0.7801998853683472\n",
      "\n",
      "Epoch 161, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04129204526543617 \t Accuracy = 0.783599853515625\n",
      "\n",
      "Epoch 161, CIFAR-10 Batch 4:  \n",
      "Loss = 0.040603090077638626 \t Accuracy = 0.7817997932434082\n",
      "\n",
      "Epoch 161, CIFAR-10 Batch 5:  \n",
      "Loss = 0.0330781489610672 \t Accuracy = 0.7851998209953308\n",
      "\n",
      "Epoch 162, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03954123705625534 \t Accuracy = 0.7859997749328613\n",
      "\n",
      "Epoch 162, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04609329625964165 \t Accuracy = 0.7731998562812805\n",
      "\n",
      "Epoch 162, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04438002407550812 \t Accuracy = 0.7791997790336609\n",
      "\n",
      "Epoch 162, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04360048100352287 \t Accuracy = 0.7771998047828674\n",
      "\n",
      "Epoch 162, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03576507791876793 \t Accuracy = 0.785399854183197\n",
      "\n",
      "Epoch 163, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04167453944683075 \t Accuracy = 0.77979975938797\n",
      "\n",
      "Epoch 163, CIFAR-10 Batch 2:  \n",
      "Loss = 0.040844596922397614 \t Accuracy = 0.778799831867218\n",
      "\n",
      "Epoch 163, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04544423893094063 \t Accuracy = 0.7803998589515686\n",
      "\n",
      "Epoch 163, CIFAR-10 Batch 4:  \n",
      "Loss = 0.050296563655138016 \t Accuracy = 0.7745998501777649\n",
      "\n",
      "Epoch 163, CIFAR-10 Batch 5:  \n",
      "Loss = 0.04243681952357292 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 164, CIFAR-10 Batch 1:  \n",
      "Loss = 0.040683455765247345 \t Accuracy = 0.7825998663902283\n",
      "\n",
      "Epoch 164, CIFAR-10 Batch 2:  \n",
      "Loss = 0.042438071221113205 \t Accuracy = 0.7765998244285583\n",
      "\n",
      "Epoch 164, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04559044912457466 \t Accuracy = 0.777999758720398\n",
      "\n",
      "Epoch 164, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04475736618041992 \t Accuracy = 0.7765998244285583\n",
      "\n",
      "Epoch 164, CIFAR-10 Batch 5:  \n",
      "Loss = 0.030874023213982582 \t Accuracy = 0.7873998284339905\n",
      "\n",
      "Epoch 165, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03942418098449707 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 165, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04113044589757919 \t Accuracy = 0.7801998257637024\n",
      "\n",
      "Epoch 165, CIFAR-10 Batch 3:  \n",
      "Loss = 0.036776889115571976 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 165, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04133159667253494 \t Accuracy = 0.7791998386383057\n",
      "\n",
      "Epoch 165, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03473080322146416 \t Accuracy = 0.7871997952461243\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03921112045645714 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 166, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0427754782140255 \t Accuracy = 0.7837998270988464\n",
      "\n",
      "Epoch 166, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04242599010467529 \t Accuracy = 0.7847998142242432\n",
      "\n",
      "Epoch 166, CIFAR-10 Batch 4:  \n",
      "Loss = 0.0425117090344429 \t Accuracy = 0.7741998434066772\n",
      "\n",
      "Epoch 166, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03322858735918999 \t Accuracy = 0.7899998426437378\n",
      "\n",
      "Epoch 167, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04545358940958977 \t Accuracy = 0.7823998332023621\n",
      "\n",
      "Epoch 167, CIFAR-10 Batch 2:  \n",
      "Loss = 0.05314253643155098 \t Accuracy = 0.7767998576164246\n",
      "\n",
      "Epoch 167, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04116063192486763 \t Accuracy = 0.7821998000144958\n",
      "\n",
      "Epoch 167, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03850983455777168 \t Accuracy = 0.7811998724937439\n",
      "\n",
      "Epoch 167, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03080001287162304 \t Accuracy = 0.7839999198913574\n",
      "\n",
      "Epoch 168, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03913339972496033 \t Accuracy = 0.7757998108863831\n",
      "\n",
      "Epoch 168, CIFAR-10 Batch 2:  \n",
      "Loss = 0.042508576065301895 \t Accuracy = 0.7781998515129089\n",
      "\n",
      "Epoch 168, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04306640848517418 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 168, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04322492331266403 \t Accuracy = 0.7727998495101929\n",
      "\n",
      "Epoch 168, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03570195287466049 \t Accuracy = 0.7853997945785522\n",
      "\n",
      "Epoch 169, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04004484787583351 \t Accuracy = 0.7793998122215271\n",
      "\n",
      "Epoch 169, CIFAR-10 Batch 2:  \n",
      "Loss = 0.043776869773864746 \t Accuracy = 0.7833998203277588\n",
      "\n",
      "Epoch 169, CIFAR-10 Batch 3:  \n",
      "Loss = 0.053451817482709885 \t Accuracy = 0.776999831199646\n",
      "\n",
      "Epoch 169, CIFAR-10 Batch 4:  \n",
      "Loss = 0.047759734094142914 \t Accuracy = 0.7717998027801514\n",
      "\n",
      "Epoch 169, CIFAR-10 Batch 5:  \n",
      "Loss = 0.035128552466630936 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 170, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03760305047035217 \t Accuracy = 0.7811998128890991\n",
      "\n",
      "Epoch 170, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04415985941886902 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 170, CIFAR-10 Batch 3:  \n",
      "Loss = 0.044201336801052094 \t Accuracy = 0.7767998576164246\n",
      "\n",
      "Epoch 170, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03670809417963028 \t Accuracy = 0.7827998995780945\n",
      "\n",
      "Epoch 170, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03388253599405289 \t Accuracy = 0.7765998244285583\n",
      "\n",
      "Epoch 171, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03988153114914894 \t Accuracy = 0.7811998128890991\n",
      "\n",
      "Epoch 171, CIFAR-10 Batch 2:  \n",
      "Loss = 0.05153303220868111 \t Accuracy = 0.7783998250961304\n",
      "\n",
      "Epoch 171, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04421299695968628 \t Accuracy = 0.7837998867034912\n",
      "\n",
      "Epoch 171, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03581078723073006 \t Accuracy = 0.7765998840332031\n",
      "\n",
      "Epoch 171, CIFAR-10 Batch 5:  \n",
      "Loss = 0.031519368290901184 \t Accuracy = 0.7851998209953308\n",
      "\n",
      "Epoch 172, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04001092165708542 \t Accuracy = 0.7825998067855835\n",
      "\n",
      "Epoch 172, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04090418294072151 \t Accuracy = 0.7861998081207275\n",
      "\n",
      "Epoch 172, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04118046164512634 \t Accuracy = 0.7849997878074646\n",
      "\n",
      "Epoch 172, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03163260966539383 \t Accuracy = 0.781599760055542\n",
      "\n",
      "Epoch 172, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03305535763502121 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 173, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03484746441245079 \t Accuracy = 0.7851998805999756\n",
      "\n",
      "Epoch 173, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04248695820569992 \t Accuracy = 0.7775997519493103\n",
      "\n",
      "Epoch 173, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04398271441459656 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 173, CIFAR-10 Batch 4:  \n",
      "Loss = 0.033491674810647964 \t Accuracy = 0.7775998711585999\n",
      "\n",
      "Epoch 173, CIFAR-10 Batch 5:  \n",
      "Loss = 0.034224726259708405 \t Accuracy = 0.7803998589515686\n",
      "\n",
      "Epoch 174, CIFAR-10 Batch 1:  \n",
      "Loss = 0.04466404765844345 \t Accuracy = 0.7773997783660889\n",
      "\n",
      "Epoch 174, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04136177524924278 \t Accuracy = 0.7831998467445374\n",
      "\n",
      "Epoch 174, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04384772107005119 \t Accuracy = 0.7807998657226562\n",
      "\n",
      "Epoch 174, CIFAR-10 Batch 4:  \n",
      "Loss = 0.04024318978190422 \t Accuracy = 0.773999810218811\n",
      "\n",
      "Epoch 174, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03404467552900314 \t Accuracy = 0.7749997973442078\n",
      "\n",
      "Epoch 175, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03778428956866264 \t Accuracy = 0.7853997945785522\n",
      "\n",
      "Epoch 175, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04115311801433563 \t Accuracy = 0.7793998122215271\n",
      "\n",
      "Epoch 175, CIFAR-10 Batch 3:  \n",
      "Loss = 0.042363062500953674 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 175, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03999309986829758 \t Accuracy = 0.7821998000144958\n",
      "\n",
      "Epoch 175, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03905390202999115 \t Accuracy = 0.772199809551239\n",
      "\n",
      "Epoch 176, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03686448559165001 \t Accuracy = 0.7811997532844543\n",
      "\n",
      "Epoch 176, CIFAR-10 Batch 2:  \n",
      "Loss = 0.05221160501241684 \t Accuracy = 0.7773997783660889\n",
      "\n",
      "Epoch 176, CIFAR-10 Batch 3:  \n",
      "Loss = 0.046091899275779724 \t Accuracy = 0.7793997526168823\n",
      "\n",
      "Epoch 176, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03974492847919464 \t Accuracy = 0.7757998704910278\n",
      "\n",
      "Epoch 176, CIFAR-10 Batch 5:  \n",
      "Loss = 0.039185378700494766 \t Accuracy = 0.7735998630523682\n",
      "\n",
      "Epoch 177, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03467899560928345 \t Accuracy = 0.7719997763633728\n",
      "\n",
      "Epoch 177, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03825927898287773 \t Accuracy = 0.7731998562812805\n",
      "\n",
      "Epoch 177, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04886350780725479 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 177, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03440481424331665 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 177, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03713750094175339 \t Accuracy = 0.7735998034477234\n",
      "\n",
      "Epoch 178, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03699154406785965 \t Accuracy = 0.7839998006820679\n",
      "\n",
      "Epoch 178, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03956722840666771 \t Accuracy = 0.7751997709274292\n",
      "\n",
      "Epoch 178, CIFAR-10 Batch 3:  \n",
      "Loss = 0.0432438850402832 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 178, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03347843885421753 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 178, CIFAR-10 Batch 5:  \n",
      "Loss = 0.036214496940374374 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 179, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03663978353142738 \t Accuracy = 0.7879998683929443\n",
      "\n",
      "Epoch 179, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03964289277791977 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 179, CIFAR-10 Batch 3:  \n",
      "Loss = 0.043322619050741196 \t Accuracy = 0.7831997871398926\n",
      "\n",
      "Epoch 179, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03537347912788391 \t Accuracy = 0.7803997993469238\n",
      "\n",
      "Epoch 179, CIFAR-10 Batch 5:  \n",
      "Loss = 0.036265790462493896 \t Accuracy = 0.7755998373031616\n",
      "\n",
      "Epoch 180, CIFAR-10 Batch 1:  \n",
      "Loss = 0.037363093346357346 \t Accuracy = 0.784599781036377\n",
      "\n",
      "Epoch 180, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04624662548303604 \t Accuracy = 0.7745999097824097\n",
      "\n",
      "Epoch 180, CIFAR-10 Batch 3:  \n",
      "Loss = 0.045766543596982956 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 180, CIFAR-10 Batch 4:  \n",
      "Loss = 0.0382038950920105 \t Accuracy = 0.773399829864502\n",
      "\n",
      "Epoch 180, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03642952814698219 \t Accuracy = 0.776999831199646\n",
      "\n",
      "Epoch 181, CIFAR-10 Batch 1:  \n",
      "Loss = 0.035136111080646515 \t Accuracy = 0.7827998399734497\n",
      "\n",
      "Epoch 181, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0397828184068203 \t Accuracy = 0.7765998840332031\n",
      "\n",
      "Epoch 181, CIFAR-10 Batch 3:  \n",
      "Loss = 0.0388360358774662 \t Accuracy = 0.7827997803688049\n",
      "\n",
      "Epoch 181, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03421812504529953 \t Accuracy = 0.7775998115539551\n",
      "\n",
      "Epoch 181, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03306914120912552 \t Accuracy = 0.7743998765945435\n",
      "\n",
      "Epoch 182, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03417811170220375 \t Accuracy = 0.7829997539520264\n",
      "\n",
      "Epoch 182, CIFAR-10 Batch 2:  \n",
      "Loss = 0.038655128329992294 \t Accuracy = 0.7773997783660889\n",
      "\n",
      "Epoch 182, CIFAR-10 Batch 3:  \n",
      "Loss = 0.044063668698072433 \t Accuracy = 0.7833998799324036\n",
      "\n",
      "Epoch 182, CIFAR-10 Batch 4:  \n",
      "Loss = 0.041402168571949005 \t Accuracy = 0.7755998373031616\n",
      "\n",
      "Epoch 182, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03499729558825493 \t Accuracy = 0.7819998264312744\n",
      "\n",
      "Epoch 183, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03485662490129471 \t Accuracy = 0.7885997891426086\n",
      "\n",
      "Epoch 183, CIFAR-10 Batch 2:  \n",
      "Loss = 0.037367500364780426 \t Accuracy = 0.7823998332023621\n",
      "\n",
      "Epoch 183, CIFAR-10 Batch 3:  \n",
      "Loss = 0.038041938096284866 \t Accuracy = 0.7873998880386353\n",
      "\n",
      "Epoch 183, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03733371943235397 \t Accuracy = 0.7809997797012329\n",
      "\n",
      "Epoch 183, CIFAR-10 Batch 5:  \n",
      "Loss = 0.030114509165287018 \t Accuracy = 0.7877998352050781\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184, CIFAR-10 Batch 1:  \n",
      "Loss = 0.030677011236548424 \t Accuracy = 0.7849998474121094\n",
      "\n",
      "Epoch 184, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03924179449677467 \t Accuracy = 0.7737998366355896\n",
      "\n",
      "Epoch 184, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03879543021321297 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 184, CIFAR-10 Batch 4:  \n",
      "Loss = 0.031150370836257935 \t Accuracy = 0.785199761390686\n",
      "\n",
      "Epoch 184, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03261153772473335 \t Accuracy = 0.7823998332023621\n",
      "\n",
      "Epoch 185, CIFAR-10 Batch 1:  \n",
      "Loss = 0.034763697534799576 \t Accuracy = 0.7799997925758362\n",
      "\n",
      "Epoch 185, CIFAR-10 Batch 2:  \n",
      "Loss = 0.041448067873716354 \t Accuracy = 0.7743998169898987\n",
      "\n",
      "Epoch 185, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03811383992433548 \t Accuracy = 0.7837998270988464\n",
      "\n",
      "Epoch 185, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03391940891742706 \t Accuracy = 0.7777997851371765\n",
      "\n",
      "Epoch 185, CIFAR-10 Batch 5:  \n",
      "Loss = 0.03509068861603737 \t Accuracy = 0.7753998637199402\n",
      "\n",
      "Epoch 186, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03482497110962868 \t Accuracy = 0.7803998589515686\n",
      "\n",
      "Epoch 186, CIFAR-10 Batch 2:  \n",
      "Loss = 0.043594468384981155 \t Accuracy = 0.7701998353004456\n",
      "\n",
      "Epoch 186, CIFAR-10 Batch 3:  \n",
      "Loss = 0.04076507315039635 \t Accuracy = 0.7753998041152954\n",
      "\n",
      "Epoch 186, CIFAR-10 Batch 4:  \n",
      "Loss = 0.034376610070466995 \t Accuracy = 0.779999852180481\n",
      "\n",
      "Epoch 186, CIFAR-10 Batch 5:  \n",
      "Loss = 0.030056899413466454 \t Accuracy = 0.7839998006820679\n",
      "\n",
      "Epoch 187, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03129331022500992 \t Accuracy = 0.7857998013496399\n",
      "\n",
      "Epoch 187, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04978133365511894 \t Accuracy = 0.7709997892379761\n",
      "\n",
      "Epoch 187, CIFAR-10 Batch 3:  \n",
      "Loss = 0.039788395166397095 \t Accuracy = 0.7825998067855835\n",
      "\n",
      "Epoch 187, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03190756216645241 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 187, CIFAR-10 Batch 5:  \n",
      "Loss = 0.030965212732553482 \t Accuracy = 0.7863998413085938\n",
      "\n",
      "Epoch 188, CIFAR-10 Batch 1:  \n",
      "Loss = 0.039691101759672165 \t Accuracy = 0.7765998244285583\n",
      "\n",
      "Epoch 188, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03935570642352104 \t Accuracy = 0.770399808883667\n",
      "\n",
      "Epoch 188, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03767188638448715 \t Accuracy = 0.779999852180481\n",
      "\n",
      "Epoch 188, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03202642500400543 \t Accuracy = 0.7787997722625732\n",
      "\n",
      "Epoch 188, CIFAR-10 Batch 5:  \n",
      "Loss = 0.032136544585227966 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 189, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03370842710137367 \t Accuracy = 0.776999831199646\n",
      "\n",
      "Epoch 189, CIFAR-10 Batch 2:  \n",
      "Loss = 0.048172079026699066 \t Accuracy = 0.7747998237609863\n",
      "\n",
      "Epoch 189, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03337198868393898 \t Accuracy = 0.7843998074531555\n",
      "\n",
      "Epoch 189, CIFAR-10 Batch 4:  \n",
      "Loss = 0.027648232877254486 \t Accuracy = 0.7855998277664185\n",
      "\n",
      "Epoch 189, CIFAR-10 Batch 5:  \n",
      "Loss = 0.024740146473050117 \t Accuracy = 0.7869998216629028\n",
      "\n",
      "Epoch 190, CIFAR-10 Batch 1:  \n",
      "Loss = 0.031386032700538635 \t Accuracy = 0.7767998576164246\n",
      "\n",
      "Epoch 190, CIFAR-10 Batch 2:  \n",
      "Loss = 0.037647124379873276 \t Accuracy = 0.7707998752593994\n",
      "\n",
      "Epoch 190, CIFAR-10 Batch 3:  \n",
      "Loss = 0.036539748311042786 \t Accuracy = 0.7803997993469238\n",
      "\n",
      "Epoch 190, CIFAR-10 Batch 4:  \n",
      "Loss = 0.036035362631082535 \t Accuracy = 0.7761998176574707\n",
      "\n",
      "Epoch 190, CIFAR-10 Batch 5:  \n",
      "Loss = 0.031806785613298416 \t Accuracy = 0.7849999070167542\n",
      "\n",
      "Epoch 191, CIFAR-10 Batch 1:  \n",
      "Loss = 0.037869781255722046 \t Accuracy = 0.7793998122215271\n",
      "\n",
      "Epoch 191, CIFAR-10 Batch 2:  \n",
      "Loss = 0.036716677248477936 \t Accuracy = 0.7765998840332031\n",
      "\n",
      "Epoch 191, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03718613460659981 \t Accuracy = 0.7807998657226562\n",
      "\n",
      "Epoch 191, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03458038717508316 \t Accuracy = 0.7745997905731201\n",
      "\n",
      "Epoch 191, CIFAR-10 Batch 5:  \n",
      "Loss = 0.02771950699388981 \t Accuracy = 0.7849997282028198\n",
      "\n",
      "Epoch 192, CIFAR-10 Batch 1:  \n",
      "Loss = 0.031008999794721603 \t Accuracy = 0.776999831199646\n",
      "\n",
      "Epoch 192, CIFAR-10 Batch 2:  \n",
      "Loss = 0.04222780093550682 \t Accuracy = 0.7757998704910278\n",
      "\n",
      "Epoch 192, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03523200750350952 \t Accuracy = 0.7815998792648315\n",
      "\n",
      "Epoch 192, CIFAR-10 Batch 4:  \n",
      "Loss = 0.026162270456552505 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 192, CIFAR-10 Batch 5:  \n",
      "Loss = 0.022569825872778893 \t Accuracy = 0.7907998561859131\n",
      "\n",
      "Epoch 193, CIFAR-10 Batch 1:  \n",
      "Loss = 0.042031098157167435 \t Accuracy = 0.7705997824668884\n",
      "\n",
      "Epoch 193, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0386030375957489 \t Accuracy = 0.7701997756958008\n",
      "\n",
      "Epoch 193, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03711620345711708 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 193, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03193988651037216 \t Accuracy = 0.7799997925758362\n",
      "\n",
      "Epoch 193, CIFAR-10 Batch 5:  \n",
      "Loss = 0.028780430555343628 \t Accuracy = 0.7855998277664185\n",
      "\n",
      "Epoch 194, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03759580850601196 \t Accuracy = 0.7715998888015747\n",
      "\n",
      "Epoch 194, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03501075133681297 \t Accuracy = 0.7759997844696045\n",
      "\n",
      "Epoch 194, CIFAR-10 Batch 3:  \n",
      "Loss = 0.036315854638814926 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 194, CIFAR-10 Batch 4:  \n",
      "Loss = 0.02868305891752243 \t Accuracy = 0.7825998067855835\n",
      "\n",
      "Epoch 194, CIFAR-10 Batch 5:  \n",
      "Loss = 0.02636095881462097 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 195, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03291761502623558 \t Accuracy = 0.7737997770309448\n",
      "\n",
      "Epoch 195, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03304899483919144 \t Accuracy = 0.7715997695922852\n",
      "\n",
      "Epoch 195, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03388742357492447 \t Accuracy = 0.7821998000144958\n",
      "\n",
      "Epoch 195, CIFAR-10 Batch 4:  \n",
      "Loss = 0.027865808457136154 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 195, CIFAR-10 Batch 5:  \n",
      "Loss = 0.02406473085284233 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 196, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03105294704437256 \t Accuracy = 0.7779998779296875\n",
      "\n",
      "Epoch 196, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03420455381274223 \t Accuracy = 0.7727998495101929\n",
      "\n",
      "Epoch 196, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03337445855140686 \t Accuracy = 0.7831998467445374\n",
      "\n",
      "Epoch 196, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03252480924129486 \t Accuracy = 0.7749997973442078\n",
      "\n",
      "Epoch 196, CIFAR-10 Batch 5:  \n",
      "Loss = 0.025948910042643547 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 197, CIFAR-10 Batch 1:  \n",
      "Loss = 0.035916779190301895 \t Accuracy = 0.7657997608184814\n",
      "\n",
      "Epoch 197, CIFAR-10 Batch 2:  \n",
      "Loss = 0.031142015010118484 \t Accuracy = 0.7747998237609863\n",
      "\n",
      "Epoch 197, CIFAR-10 Batch 3:  \n",
      "Loss = 0.036564793437719345 \t Accuracy = 0.7705997824668884\n",
      "\n",
      "Epoch 197, CIFAR-10 Batch 4:  \n",
      "Loss = 0.026197489351034164 \t Accuracy = 0.7783998847007751\n",
      "\n",
      "Epoch 197, CIFAR-10 Batch 5:  \n",
      "Loss = 0.024528060108423233 \t Accuracy = 0.7859998941421509\n",
      "\n",
      "Epoch 198, CIFAR-10 Batch 1:  \n",
      "Loss = 0.027241315692663193 \t Accuracy = 0.7755997776985168\n",
      "\n",
      "Epoch 198, CIFAR-10 Batch 2:  \n",
      "Loss = 0.036097243428230286 \t Accuracy = 0.7681998610496521\n",
      "\n",
      "Epoch 198, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03240925446152687 \t Accuracy = 0.7887998223304749\n",
      "\n",
      "Epoch 198, CIFAR-10 Batch 4:  \n",
      "Loss = 0.026524707674980164 \t Accuracy = 0.7869998216629028\n",
      "\n",
      "Epoch 198, CIFAR-10 Batch 5:  \n",
      "Loss = 0.030440807342529297 \t Accuracy = 0.7847998142242432\n",
      "\n",
      "Epoch 199, CIFAR-10 Batch 1:  \n",
      "Loss = 0.02971191704273224 \t Accuracy = 0.7773998379707336\n",
      "\n",
      "Epoch 199, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0347849540412426 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 199, CIFAR-10 Batch 3:  \n",
      "Loss = 0.030565891414880753 \t Accuracy = 0.7871997952461243\n",
      "\n",
      "Epoch 199, CIFAR-10 Batch 4:  \n",
      "Loss = 0.025263775140047073 \t Accuracy = 0.7805997729301453\n",
      "\n",
      "Epoch 199, CIFAR-10 Batch 5:  \n",
      "Loss = 0.028064005076885223 \t Accuracy = 0.7891998291015625\n",
      "\n",
      "Epoch 200, CIFAR-10 Batch 1:  \n",
      "Loss = 0.026929352432489395 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 200, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03272512927651405 \t Accuracy = 0.7731997966766357\n",
      "\n",
      "Epoch 200, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03008861094713211 \t Accuracy = 0.7823998332023621\n",
      "\n",
      "Epoch 200, CIFAR-10 Batch 4:  \n",
      "Loss = 0.03121369332075119 \t Accuracy = 0.7757998704910278\n",
      "\n",
      "Epoch 200, CIFAR-10 Batch 5:  \n",
      "Loss = 0.023772837594151497 \t Accuracy = 0.7901998162269592\n",
      "\n",
      "Epoch 201, CIFAR-10 Batch 1:  \n",
      "Loss = 0.031438201665878296 \t Accuracy = 0.7737998962402344\n",
      "\n",
      "Epoch 201, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03251377120614052 \t Accuracy = 0.7825998067855835\n",
      "\n",
      "Epoch 201, CIFAR-10 Batch 3:  \n",
      "Loss = 0.033699169754981995 \t Accuracy = 0.7783998847007751\n",
      "\n",
      "Epoch 201, CIFAR-10 Batch 4:  \n",
      "Loss = 0.028329264372587204 \t Accuracy = 0.7825998067855835\n",
      "\n",
      "Epoch 201, CIFAR-10 Batch 5:  \n",
      "Loss = 0.022369571030139923 \t Accuracy = 0.7855997681617737\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03297411650419235 \t Accuracy = 0.7801997661590576\n",
      "\n",
      "Epoch 202, CIFAR-10 Batch 2:  \n",
      "Loss = 0.032827310264110565 \t Accuracy = 0.7791998386383057\n",
      "\n",
      "Epoch 202, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03366406261920929 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 202, CIFAR-10 Batch 4:  \n",
      "Loss = 0.025827590376138687 \t Accuracy = 0.783599853515625\n",
      "\n",
      "Epoch 202, CIFAR-10 Batch 5:  \n",
      "Loss = 0.020013093948364258 \t Accuracy = 0.7847998142242432\n",
      "\n",
      "Epoch 203, CIFAR-10 Batch 1:  \n",
      "Loss = 0.02597956359386444 \t Accuracy = 0.7765997648239136\n",
      "\n",
      "Epoch 203, CIFAR-10 Batch 2:  \n",
      "Loss = 0.031151939183473587 \t Accuracy = 0.783399760723114\n",
      "\n",
      "Epoch 203, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03268146887421608 \t Accuracy = 0.778799831867218\n",
      "\n",
      "Epoch 203, CIFAR-10 Batch 4:  \n",
      "Loss = 0.026961755007505417 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 203, CIFAR-10 Batch 5:  \n",
      "Loss = 0.024027524515986443 \t Accuracy = 0.7829998135566711\n",
      "\n",
      "Epoch 204, CIFAR-10 Batch 1:  \n",
      "Loss = 0.02886107750236988 \t Accuracy = 0.7757998108863831\n",
      "\n",
      "Epoch 204, CIFAR-10 Batch 2:  \n",
      "Loss = 0.027437636628746986 \t Accuracy = 0.7789998650550842\n",
      "\n",
      "Epoch 204, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02789420448243618 \t Accuracy = 0.7833998203277588\n",
      "\n",
      "Epoch 204, CIFAR-10 Batch 4:  \n",
      "Loss = 0.025894612073898315 \t Accuracy = 0.7791998982429504\n",
      "\n",
      "Epoch 204, CIFAR-10 Batch 5:  \n",
      "Loss = 0.02244327776134014 \t Accuracy = 0.7867997884750366\n",
      "\n",
      "Epoch 205, CIFAR-10 Batch 1:  \n",
      "Loss = 0.028212253004312515 \t Accuracy = 0.7823997735977173\n",
      "\n",
      "Epoch 205, CIFAR-10 Batch 2:  \n",
      "Loss = 0.026535801589488983 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 205, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03224785998463631 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 205, CIFAR-10 Batch 4:  \n",
      "Loss = 0.02535436302423477 \t Accuracy = 0.786399781703949\n",
      "\n",
      "Epoch 205, CIFAR-10 Batch 5:  \n",
      "Loss = 0.02586985006928444 \t Accuracy = 0.7829997539520264\n",
      "\n",
      "Epoch 206, CIFAR-10 Batch 1:  \n",
      "Loss = 0.029276400804519653 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 206, CIFAR-10 Batch 2:  \n",
      "Loss = 0.029068727046251297 \t Accuracy = 0.7787998914718628\n",
      "\n",
      "Epoch 206, CIFAR-10 Batch 3:  \n",
      "Loss = 0.028633099049329758 \t Accuracy = 0.7781999111175537\n",
      "\n",
      "Epoch 206, CIFAR-10 Batch 4:  \n",
      "Loss = 0.023869168013334274 \t Accuracy = 0.7777998447418213\n",
      "\n",
      "Epoch 206, CIFAR-10 Batch 5:  \n",
      "Loss = 0.02338389866054058 \t Accuracy = 0.7803997993469238\n",
      "\n",
      "Epoch 207, CIFAR-10 Batch 1:  \n",
      "Loss = 0.034276966005563736 \t Accuracy = 0.7765998840332031\n",
      "\n",
      "Epoch 207, CIFAR-10 Batch 2:  \n",
      "Loss = 0.028552034869790077 \t Accuracy = 0.7723997831344604\n",
      "\n",
      "Epoch 207, CIFAR-10 Batch 3:  \n",
      "Loss = 0.029608890414237976 \t Accuracy = 0.7741998434066772\n",
      "\n",
      "Epoch 207, CIFAR-10 Batch 4:  \n",
      "Loss = 0.02471042424440384 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 207, CIFAR-10 Batch 5:  \n",
      "Loss = 0.026186533272266388 \t Accuracy = 0.7795998454093933\n",
      "\n",
      "Epoch 208, CIFAR-10 Batch 1:  \n",
      "Loss = 0.027800729498267174 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 208, CIFAR-10 Batch 2:  \n",
      "Loss = 0.031086735427379608 \t Accuracy = 0.7799997925758362\n",
      "\n",
      "Epoch 208, CIFAR-10 Batch 3:  \n",
      "Loss = 0.024585844948887825 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 208, CIFAR-10 Batch 4:  \n",
      "Loss = 0.020652011036872864 \t Accuracy = 0.7845998406410217\n",
      "\n",
      "Epoch 208, CIFAR-10 Batch 5:  \n",
      "Loss = 0.020060747861862183 \t Accuracy = 0.7873997688293457\n",
      "\n",
      "Epoch 209, CIFAR-10 Batch 1:  \n",
      "Loss = 0.02796132117509842 \t Accuracy = 0.7705997824668884\n",
      "\n",
      "Epoch 209, CIFAR-10 Batch 2:  \n",
      "Loss = 0.030788982287049294 \t Accuracy = 0.7747998237609863\n",
      "\n",
      "Epoch 209, CIFAR-10 Batch 3:  \n",
      "Loss = 0.027545835822820663 \t Accuracy = 0.7807998061180115\n",
      "\n",
      "Epoch 209, CIFAR-10 Batch 4:  \n",
      "Loss = 0.024694381281733513 \t Accuracy = 0.7791998386383057\n",
      "\n",
      "Epoch 209, CIFAR-10 Batch 5:  \n",
      "Loss = 0.024342700839042664 \t Accuracy = 0.7799997925758362\n",
      "\n",
      "Epoch 210, CIFAR-10 Batch 1:  \n",
      "Loss = 0.031246813014149666 \t Accuracy = 0.7755998373031616\n",
      "\n",
      "Epoch 210, CIFAR-10 Batch 2:  \n",
      "Loss = 0.031231559813022614 \t Accuracy = 0.7821998000144958\n",
      "\n",
      "Epoch 210, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02833818458020687 \t Accuracy = 0.7799997925758362\n",
      "\n",
      "Epoch 210, CIFAR-10 Batch 4:  \n",
      "Loss = 0.023515628650784492 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 210, CIFAR-10 Batch 5:  \n",
      "Loss = 0.022437341511249542 \t Accuracy = 0.7801998853683472\n",
      "\n",
      "Epoch 211, CIFAR-10 Batch 1:  \n",
      "Loss = 0.025213971734046936 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 211, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03010769560933113 \t Accuracy = 0.7721998691558838\n",
      "\n",
      "Epoch 211, CIFAR-10 Batch 3:  \n",
      "Loss = 0.028705362230539322 \t Accuracy = 0.7877998352050781\n",
      "\n",
      "Epoch 211, CIFAR-10 Batch 4:  \n",
      "Loss = 0.023759866133332253 \t Accuracy = 0.7803998589515686\n",
      "\n",
      "Epoch 211, CIFAR-10 Batch 5:  \n",
      "Loss = 0.025309816002845764 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 212, CIFAR-10 Batch 1:  \n",
      "Loss = 0.028452066704630852 \t Accuracy = 0.7715998291969299\n",
      "\n",
      "Epoch 212, CIFAR-10 Batch 2:  \n",
      "Loss = 0.030964381992816925 \t Accuracy = 0.7781997919082642\n",
      "\n",
      "Epoch 212, CIFAR-10 Batch 3:  \n",
      "Loss = 0.028034385293722153 \t Accuracy = 0.7869998216629028\n",
      "\n",
      "Epoch 212, CIFAR-10 Batch 4:  \n",
      "Loss = 0.026123059913516045 \t Accuracy = 0.7773997783660889\n",
      "\n",
      "Epoch 212, CIFAR-10 Batch 5:  \n",
      "Loss = 0.027209432795643806 \t Accuracy = 0.7801998853683472\n",
      "\n",
      "Epoch 213, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03269076719880104 \t Accuracy = 0.7739998698234558\n",
      "\n",
      "Epoch 213, CIFAR-10 Batch 2:  \n",
      "Loss = 0.032087892293930054 \t Accuracy = 0.7747998237609863\n",
      "\n",
      "Epoch 213, CIFAR-10 Batch 3:  \n",
      "Loss = 0.03324975073337555 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 213, CIFAR-10 Batch 4:  \n",
      "Loss = 0.022673606872558594 \t Accuracy = 0.7813997864723206\n",
      "\n",
      "Epoch 213, CIFAR-10 Batch 5:  \n",
      "Loss = 0.022531719878315926 \t Accuracy = 0.786399781703949\n",
      "\n",
      "Epoch 214, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03301834315061569 \t Accuracy = 0.779999852180481\n",
      "\n",
      "Epoch 214, CIFAR-10 Batch 2:  \n",
      "Loss = 0.024204619228839874 \t Accuracy = 0.7803997993469238\n",
      "\n",
      "Epoch 214, CIFAR-10 Batch 3:  \n",
      "Loss = 0.026934972032904625 \t Accuracy = 0.7867997884750366\n",
      "\n",
      "Epoch 214, CIFAR-10 Batch 4:  \n",
      "Loss = 0.021429313346743584 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 214, CIFAR-10 Batch 5:  \n",
      "Loss = 0.025677897036075592 \t Accuracy = 0.7767998576164246\n",
      "\n",
      "Epoch 215, CIFAR-10 Batch 1:  \n",
      "Loss = 0.03127940744161606 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 215, CIFAR-10 Batch 2:  \n",
      "Loss = 0.028972625732421875 \t Accuracy = 0.77979975938797\n",
      "\n",
      "Epoch 215, CIFAR-10 Batch 3:  \n",
      "Loss = 0.027322720736265182 \t Accuracy = 0.7839998006820679\n",
      "\n",
      "Epoch 215, CIFAR-10 Batch 4:  \n",
      "Loss = 0.021974872797727585 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 215, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01928984560072422 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 216, CIFAR-10 Batch 1:  \n",
      "Loss = 0.026440754532814026 \t Accuracy = 0.7811998724937439\n",
      "\n",
      "Epoch 216, CIFAR-10 Batch 2:  \n",
      "Loss = 0.03534632548689842 \t Accuracy = 0.778799831867218\n",
      "\n",
      "Epoch 216, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02975345402956009 \t Accuracy = 0.7815998196601868\n",
      "\n",
      "Epoch 216, CIFAR-10 Batch 4:  \n",
      "Loss = 0.023484187200665474 \t Accuracy = 0.7771998047828674\n",
      "\n",
      "Epoch 216, CIFAR-10 Batch 5:  \n",
      "Loss = 0.022083865478634834 \t Accuracy = 0.7845999002456665\n",
      "\n",
      "Epoch 217, CIFAR-10 Batch 1:  \n",
      "Loss = 0.032028213143348694 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 217, CIFAR-10 Batch 2:  \n",
      "Loss = 0.030877264216542244 \t Accuracy = 0.7723998427391052\n",
      "\n",
      "Epoch 217, CIFAR-10 Batch 3:  \n",
      "Loss = 0.029957644641399384 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 217, CIFAR-10 Batch 4:  \n",
      "Loss = 0.02608870528638363 \t Accuracy = 0.7743998169898987\n",
      "\n",
      "Epoch 217, CIFAR-10 Batch 5:  \n",
      "Loss = 0.02273554913699627 \t Accuracy = 0.7783997654914856\n",
      "\n",
      "Epoch 218, CIFAR-10 Batch 1:  \n",
      "Loss = 0.02891536056995392 \t Accuracy = 0.7723998427391052\n",
      "\n",
      "Epoch 218, CIFAR-10 Batch 2:  \n",
      "Loss = 0.029043354094028473 \t Accuracy = 0.7699998021125793\n",
      "\n",
      "Epoch 218, CIFAR-10 Batch 3:  \n",
      "Loss = 0.026636093854904175 \t Accuracy = 0.7807998061180115\n",
      "\n",
      "Epoch 218, CIFAR-10 Batch 4:  \n",
      "Loss = 0.021885845810174942 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 218, CIFAR-10 Batch 5:  \n",
      "Loss = 0.019881458953022957 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 219, CIFAR-10 Batch 1:  \n",
      "Loss = 0.025896325707435608 \t Accuracy = 0.7773998379707336\n",
      "\n",
      "Epoch 219, CIFAR-10 Batch 2:  \n",
      "Loss = 0.02673480100929737 \t Accuracy = 0.7753998637199402\n",
      "\n",
      "Epoch 219, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02689017914235592 \t Accuracy = 0.7843998074531555\n",
      "\n",
      "Epoch 219, CIFAR-10 Batch 4:  \n",
      "Loss = 0.022620443254709244 \t Accuracy = 0.7765998840332031\n",
      "\n",
      "Epoch 219, CIFAR-10 Batch 5:  \n",
      "Loss = 0.0217818021774292 \t Accuracy = 0.7841998338699341\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220, CIFAR-10 Batch 1:  \n",
      "Loss = 0.025883683934807777 \t Accuracy = 0.7787997722625732\n",
      "\n",
      "Epoch 220, CIFAR-10 Batch 2:  \n",
      "Loss = 0.025954922661185265 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 220, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02350769378244877 \t Accuracy = 0.7891998291015625\n",
      "\n",
      "Epoch 220, CIFAR-10 Batch 4:  \n",
      "Loss = 0.02150600776076317 \t Accuracy = 0.7775998115539551\n",
      "\n",
      "Epoch 220, CIFAR-10 Batch 5:  \n",
      "Loss = 0.017128756269812584 \t Accuracy = 0.7827997803688049\n",
      "\n",
      "Epoch 221, CIFAR-10 Batch 1:  \n",
      "Loss = 0.02433968521654606 \t Accuracy = 0.7781997919082642\n",
      "\n",
      "Epoch 221, CIFAR-10 Batch 2:  \n",
      "Loss = 0.030485862866044044 \t Accuracy = 0.7729998230934143\n",
      "\n",
      "Epoch 221, CIFAR-10 Batch 3:  \n",
      "Loss = 0.025188060477375984 \t Accuracy = 0.7803998589515686\n",
      "\n",
      "Epoch 221, CIFAR-10 Batch 4:  \n",
      "Loss = 0.021428218111395836 \t Accuracy = 0.7823998332023621\n",
      "\n",
      "Epoch 221, CIFAR-10 Batch 5:  \n",
      "Loss = 0.018771085888147354 \t Accuracy = 0.7867998480796814\n",
      "\n",
      "Epoch 222, CIFAR-10 Batch 1:  \n",
      "Loss = 0.021696850657463074 \t Accuracy = 0.778799831867218\n",
      "\n",
      "Epoch 222, CIFAR-10 Batch 2:  \n",
      "Loss = 0.024935971945524216 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 222, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02069954015314579 \t Accuracy = 0.7891998291015625\n",
      "\n",
      "Epoch 222, CIFAR-10 Batch 4:  \n",
      "Loss = 0.01917250081896782 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 222, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01723160222172737 \t Accuracy = 0.7825998663902283\n",
      "\n",
      "Epoch 223, CIFAR-10 Batch 1:  \n",
      "Loss = 0.020547643303871155 \t Accuracy = 0.7761998176574707\n",
      "\n",
      "Epoch 223, CIFAR-10 Batch 2:  \n",
      "Loss = 0.024791590869426727 \t Accuracy = 0.7865998148918152\n",
      "\n",
      "Epoch 223, CIFAR-10 Batch 3:  \n",
      "Loss = 0.022944539785385132 \t Accuracy = 0.7843998074531555\n",
      "\n",
      "Epoch 223, CIFAR-10 Batch 4:  \n",
      "Loss = 0.02173713967204094 \t Accuracy = 0.7751998901367188\n",
      "\n",
      "Epoch 223, CIFAR-10 Batch 5:  \n",
      "Loss = 0.018979039043188095 \t Accuracy = 0.7905998229980469\n",
      "\n",
      "Epoch 224, CIFAR-10 Batch 1:  \n",
      "Loss = 0.021944938227534294 \t Accuracy = 0.7809998393058777\n",
      "\n",
      "Epoch 224, CIFAR-10 Batch 2:  \n",
      "Loss = 0.023602116852998734 \t Accuracy = 0.776999831199646\n",
      "\n",
      "Epoch 224, CIFAR-10 Batch 3:  \n",
      "Loss = 0.026632972061634064 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 224, CIFAR-10 Batch 4:  \n",
      "Loss = 0.02041037008166313 \t Accuracy = 0.7787997722625732\n",
      "\n",
      "Epoch 224, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01887301169335842 \t Accuracy = 0.7831998467445374\n",
      "\n",
      "Epoch 225, CIFAR-10 Batch 1:  \n",
      "Loss = 0.020116742700338364 \t Accuracy = 0.7827998399734497\n",
      "\n",
      "Epoch 225, CIFAR-10 Batch 2:  \n",
      "Loss = 0.022404856979846954 \t Accuracy = 0.778799831867218\n",
      "\n",
      "Epoch 225, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02282024174928665 \t Accuracy = 0.7829998135566711\n",
      "\n",
      "Epoch 225, CIFAR-10 Batch 4:  \n",
      "Loss = 0.02020202949643135 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 225, CIFAR-10 Batch 5:  \n",
      "Loss = 0.016148118302226067 \t Accuracy = 0.786399781703949\n",
      "\n",
      "Epoch 226, CIFAR-10 Batch 1:  \n",
      "Loss = 0.019506750628352165 \t Accuracy = 0.7831998467445374\n",
      "\n",
      "Epoch 226, CIFAR-10 Batch 2:  \n",
      "Loss = 0.021116536110639572 \t Accuracy = 0.7793998122215271\n",
      "\n",
      "Epoch 226, CIFAR-10 Batch 3:  \n",
      "Loss = 0.022550571709871292 \t Accuracy = 0.77979975938797\n",
      "\n",
      "Epoch 226, CIFAR-10 Batch 4:  \n",
      "Loss = 0.0221048966050148 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 226, CIFAR-10 Batch 5:  \n",
      "Loss = 0.016688352450728416 \t Accuracy = 0.7895998358726501\n",
      "\n",
      "Epoch 227, CIFAR-10 Batch 1:  \n",
      "Loss = 0.019748348742723465 \t Accuracy = 0.7919998168945312\n",
      "\n",
      "Epoch 227, CIFAR-10 Batch 2:  \n",
      "Loss = 0.02444838546216488 \t Accuracy = 0.7815998196601868\n",
      "\n",
      "Epoch 227, CIFAR-10 Batch 3:  \n",
      "Loss = 0.025210333988070488 \t Accuracy = 0.7767997980117798\n",
      "\n",
      "Epoch 227, CIFAR-10 Batch 4:  \n",
      "Loss = 0.020507827401161194 \t Accuracy = 0.7761998176574707\n",
      "\n",
      "Epoch 227, CIFAR-10 Batch 5:  \n",
      "Loss = 0.017654869705438614 \t Accuracy = 0.7861998081207275\n",
      "\n",
      "Epoch 228, CIFAR-10 Batch 1:  \n",
      "Loss = 0.019519353285431862 \t Accuracy = 0.7889997959136963\n",
      "\n",
      "Epoch 228, CIFAR-10 Batch 2:  \n",
      "Loss = 0.02710544317960739 \t Accuracy = 0.7749997973442078\n",
      "\n",
      "Epoch 228, CIFAR-10 Batch 3:  \n",
      "Loss = 0.023055769503116608 \t Accuracy = 0.7839998006820679\n",
      "\n",
      "Epoch 228, CIFAR-10 Batch 4:  \n",
      "Loss = 0.01902686059474945 \t Accuracy = 0.7763997912406921\n",
      "\n",
      "Epoch 228, CIFAR-10 Batch 5:  \n",
      "Loss = 0.018938032910227776 \t Accuracy = 0.7833998203277588\n",
      "\n",
      "Epoch 229, CIFAR-10 Batch 1:  \n",
      "Loss = 0.018854008987545967 \t Accuracy = 0.7861998677253723\n",
      "\n",
      "Epoch 229, CIFAR-10 Batch 2:  \n",
      "Loss = 0.02135622128844261 \t Accuracy = 0.7825998067855835\n",
      "\n",
      "Epoch 229, CIFAR-10 Batch 3:  \n",
      "Loss = 0.0204470232129097 \t Accuracy = 0.7825998067855835\n",
      "\n",
      "Epoch 229, CIFAR-10 Batch 4:  \n",
      "Loss = 0.016659867018461227 \t Accuracy = 0.7843998074531555\n",
      "\n",
      "Epoch 229, CIFAR-10 Batch 5:  \n",
      "Loss = 0.019361678510904312 \t Accuracy = 0.7843998670578003\n",
      "\n",
      "Epoch 230, CIFAR-10 Batch 1:  \n",
      "Loss = 0.018650144338607788 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 230, CIFAR-10 Batch 2:  \n",
      "Loss = 0.022140594199299812 \t Accuracy = 0.779999852180481\n",
      "\n",
      "Epoch 230, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02664181962609291 \t Accuracy = 0.7809998393058777\n",
      "\n",
      "Epoch 230, CIFAR-10 Batch 4:  \n",
      "Loss = 0.020338747650384903 \t Accuracy = 0.7771998643875122\n",
      "\n",
      "Epoch 230, CIFAR-10 Batch 5:  \n",
      "Loss = 0.0158734992146492 \t Accuracy = 0.7869998216629028\n",
      "\n",
      "Epoch 231, CIFAR-10 Batch 1:  \n",
      "Loss = 0.019103990867733955 \t Accuracy = 0.787199854850769\n",
      "\n",
      "Epoch 231, CIFAR-10 Batch 2:  \n",
      "Loss = 0.030796118080615997 \t Accuracy = 0.776999831199646\n",
      "\n",
      "Epoch 231, CIFAR-10 Batch 3:  \n",
      "Loss = 0.020792199298739433 \t Accuracy = 0.7905998229980469\n",
      "\n",
      "Epoch 231, CIFAR-10 Batch 4:  \n",
      "Loss = 0.019192781299352646 \t Accuracy = 0.7839997410774231\n",
      "\n",
      "Epoch 231, CIFAR-10 Batch 5:  \n",
      "Loss = 0.019839057698845863 \t Accuracy = 0.7859998345375061\n",
      "\n",
      "Epoch 232, CIFAR-10 Batch 1:  \n",
      "Loss = 0.02294081076979637 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 232, CIFAR-10 Batch 2:  \n",
      "Loss = 0.022028207778930664 \t Accuracy = 0.7801998257637024\n",
      "\n",
      "Epoch 232, CIFAR-10 Batch 3:  \n",
      "Loss = 0.020435132086277008 \t Accuracy = 0.7851998209953308\n",
      "\n",
      "Epoch 232, CIFAR-10 Batch 4:  \n",
      "Loss = 0.01841839775443077 \t Accuracy = 0.7843998670578003\n",
      "\n",
      "Epoch 232, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01658647507429123 \t Accuracy = 0.7857998013496399\n",
      "\n",
      "Epoch 233, CIFAR-10 Batch 1:  \n",
      "Loss = 0.020472684875130653 \t Accuracy = 0.7843998074531555\n",
      "\n",
      "Epoch 233, CIFAR-10 Batch 2:  \n",
      "Loss = 0.018347466364502907 \t Accuracy = 0.7887998223304749\n",
      "\n",
      "Epoch 233, CIFAR-10 Batch 3:  \n",
      "Loss = 0.018698932603001595 \t Accuracy = 0.788199782371521\n",
      "\n",
      "Epoch 233, CIFAR-10 Batch 4:  \n",
      "Loss = 0.016546418890357018 \t Accuracy = 0.7837998867034912\n",
      "\n",
      "Epoch 233, CIFAR-10 Batch 5:  \n",
      "Loss = 0.015804491937160492 \t Accuracy = 0.7857998013496399\n",
      "\n",
      "Epoch 234, CIFAR-10 Batch 1:  \n",
      "Loss = 0.020328236743807793 \t Accuracy = 0.7731998562812805\n",
      "\n",
      "Epoch 234, CIFAR-10 Batch 2:  \n",
      "Loss = 0.020256316289305687 \t Accuracy = 0.7827997803688049\n",
      "\n",
      "Epoch 234, CIFAR-10 Batch 3:  \n",
      "Loss = 0.020483089610934258 \t Accuracy = 0.7855997681617737\n",
      "\n",
      "Epoch 234, CIFAR-10 Batch 4:  \n",
      "Loss = 0.01722390577197075 \t Accuracy = 0.7801998257637024\n",
      "\n",
      "Epoch 234, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01601262018084526 \t Accuracy = 0.7869998812675476\n",
      "\n",
      "Epoch 235, CIFAR-10 Batch 1:  \n",
      "Loss = 0.023100944235920906 \t Accuracy = 0.778799831867218\n",
      "\n",
      "Epoch 235, CIFAR-10 Batch 2:  \n",
      "Loss = 0.02146133966743946 \t Accuracy = 0.7753998637199402\n",
      "\n",
      "Epoch 235, CIFAR-10 Batch 3:  \n",
      "Loss = 0.020923208445310593 \t Accuracy = 0.7889997959136963\n",
      "\n",
      "Epoch 235, CIFAR-10 Batch 4:  \n",
      "Loss = 0.0185258612036705 \t Accuracy = 0.7827998399734497\n",
      "\n",
      "Epoch 235, CIFAR-10 Batch 5:  \n",
      "Loss = 0.016852010041475296 \t Accuracy = 0.784599781036377\n",
      "\n",
      "Epoch 236, CIFAR-10 Batch 1:  \n",
      "Loss = 0.020213132724165916 \t Accuracy = 0.7809998393058777\n",
      "\n",
      "Epoch 236, CIFAR-10 Batch 2:  \n",
      "Loss = 0.022709250450134277 \t Accuracy = 0.7805997729301453\n",
      "\n",
      "Epoch 236, CIFAR-10 Batch 3:  \n",
      "Loss = 0.020544879138469696 \t Accuracy = 0.7873997688293457\n",
      "\n",
      "Epoch 236, CIFAR-10 Batch 4:  \n",
      "Loss = 0.015079542994499207 \t Accuracy = 0.7783998250961304\n",
      "\n",
      "Epoch 236, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01768006570637226 \t Accuracy = 0.7895997762680054\n",
      "\n",
      "Epoch 237, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016317401081323624 \t Accuracy = 0.7791997790336609\n",
      "\n",
      "Epoch 237, CIFAR-10 Batch 2:  \n",
      "Loss = 0.023663505911827087 \t Accuracy = 0.776999831199646\n",
      "\n",
      "Epoch 237, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02090439572930336 \t Accuracy = 0.7857998609542847\n",
      "\n",
      "Epoch 237, CIFAR-10 Batch 4:  \n",
      "Loss = 0.016966473311185837 \t Accuracy = 0.7835997939109802\n",
      "\n",
      "Epoch 237, CIFAR-10 Batch 5:  \n",
      "Loss = 0.015743032097816467 \t Accuracy = 0.7869998216629028\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01822880655527115 \t Accuracy = 0.7773998379707336\n",
      "\n",
      "Epoch 238, CIFAR-10 Batch 2:  \n",
      "Loss = 0.019038420170545578 \t Accuracy = 0.7875998020172119\n",
      "\n",
      "Epoch 238, CIFAR-10 Batch 3:  \n",
      "Loss = 0.021045224741101265 \t Accuracy = 0.786399781703949\n",
      "\n",
      "Epoch 238, CIFAR-10 Batch 4:  \n",
      "Loss = 0.017349405214190483 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 238, CIFAR-10 Batch 5:  \n",
      "Loss = 0.016491269692778587 \t Accuracy = 0.7895998358726501\n",
      "\n",
      "Epoch 239, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01583336479961872 \t Accuracy = 0.7847998142242432\n",
      "\n",
      "Epoch 239, CIFAR-10 Batch 2:  \n",
      "Loss = 0.019408078864216805 \t Accuracy = 0.7783998250961304\n",
      "\n",
      "Epoch 239, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01900649443268776 \t Accuracy = 0.7879997491836548\n",
      "\n",
      "Epoch 239, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014810638502240181 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 239, CIFAR-10 Batch 5:  \n",
      "Loss = 0.014978036284446716 \t Accuracy = 0.7903997898101807\n",
      "\n",
      "Epoch 240, CIFAR-10 Batch 1:  \n",
      "Loss = 0.015474607236683369 \t Accuracy = 0.7843998074531555\n",
      "\n",
      "Epoch 240, CIFAR-10 Batch 2:  \n",
      "Loss = 0.022596515715122223 \t Accuracy = 0.770399808883667\n",
      "\n",
      "Epoch 240, CIFAR-10 Batch 3:  \n",
      "Loss = 0.023059619590640068 \t Accuracy = 0.7759997844696045\n",
      "\n",
      "Epoch 240, CIFAR-10 Batch 4:  \n",
      "Loss = 0.018277524039149284 \t Accuracy = 0.7795998454093933\n",
      "\n",
      "Epoch 240, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01600736379623413 \t Accuracy = 0.7837997674942017\n",
      "\n",
      "Epoch 241, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016536394134163857 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 241, CIFAR-10 Batch 2:  \n",
      "Loss = 0.021516617387533188 \t Accuracy = 0.776999831199646\n",
      "\n",
      "Epoch 241, CIFAR-10 Batch 3:  \n",
      "Loss = 0.018441524356603622 \t Accuracy = 0.7857998013496399\n",
      "\n",
      "Epoch 241, CIFAR-10 Batch 4:  \n",
      "Loss = 0.021140342578291893 \t Accuracy = 0.7747998237609863\n",
      "\n",
      "Epoch 241, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01799304597079754 \t Accuracy = 0.7849997878074646\n",
      "\n",
      "Epoch 242, CIFAR-10 Batch 1:  \n",
      "Loss = 0.021219292655587196 \t Accuracy = 0.7723997831344604\n",
      "\n",
      "Epoch 242, CIFAR-10 Batch 2:  \n",
      "Loss = 0.022916041314601898 \t Accuracy = 0.7701997756958008\n",
      "\n",
      "Epoch 242, CIFAR-10 Batch 3:  \n",
      "Loss = 0.023039916530251503 \t Accuracy = 0.7769997119903564\n",
      "\n",
      "Epoch 242, CIFAR-10 Batch 4:  \n",
      "Loss = 0.019636500626802444 \t Accuracy = 0.7695997953414917\n",
      "\n",
      "Epoch 242, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01619907282292843 \t Accuracy = 0.7857998013496399\n",
      "\n",
      "Epoch 243, CIFAR-10 Batch 1:  \n",
      "Loss = 0.0163409486413002 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 243, CIFAR-10 Batch 2:  \n",
      "Loss = 0.021853527054190636 \t Accuracy = 0.7759998440742493\n",
      "\n",
      "Epoch 243, CIFAR-10 Batch 3:  \n",
      "Loss = 0.020197853446006775 \t Accuracy = 0.7847998142242432\n",
      "\n",
      "Epoch 243, CIFAR-10 Batch 4:  \n",
      "Loss = 0.018747270107269287 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 243, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01948978379368782 \t Accuracy = 0.7825998663902283\n",
      "\n",
      "Epoch 244, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01857241988182068 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 244, CIFAR-10 Batch 2:  \n",
      "Loss = 0.019864633679389954 \t Accuracy = 0.7749998569488525\n",
      "\n",
      "Epoch 244, CIFAR-10 Batch 3:  \n",
      "Loss = 0.022887732833623886 \t Accuracy = 0.777999758720398\n",
      "\n",
      "Epoch 244, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014688466675579548 \t Accuracy = 0.7791997790336609\n",
      "\n",
      "Epoch 244, CIFAR-10 Batch 5:  \n",
      "Loss = 0.015612117946147919 \t Accuracy = 0.7865997552871704\n",
      "\n",
      "Epoch 245, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01727522350847721 \t Accuracy = 0.7777999043464661\n",
      "\n",
      "Epoch 245, CIFAR-10 Batch 2:  \n",
      "Loss = 0.01591363362967968 \t Accuracy = 0.7771998643875122\n",
      "\n",
      "Epoch 245, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017699744552373886 \t Accuracy = 0.7889998555183411\n",
      "\n",
      "Epoch 245, CIFAR-10 Batch 4:  \n",
      "Loss = 0.019372450187802315 \t Accuracy = 0.7839998602867126\n",
      "\n",
      "Epoch 245, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01716328039765358 \t Accuracy = 0.7891998291015625\n",
      "\n",
      "Epoch 246, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016694461926817894 \t Accuracy = 0.7751997709274292\n",
      "\n",
      "Epoch 246, CIFAR-10 Batch 2:  \n",
      "Loss = 0.016787832602858543 \t Accuracy = 0.7777998447418213\n",
      "\n",
      "Epoch 246, CIFAR-10 Batch 3:  \n",
      "Loss = 0.018588406965136528 \t Accuracy = 0.7829998731613159\n",
      "\n",
      "Epoch 246, CIFAR-10 Batch 4:  \n",
      "Loss = 0.015620600432157516 \t Accuracy = 0.7831998467445374\n",
      "\n",
      "Epoch 246, CIFAR-10 Batch 5:  \n",
      "Loss = 0.013880313374102116 \t Accuracy = 0.7865998148918152\n",
      "\n",
      "Epoch 247, CIFAR-10 Batch 1:  \n",
      "Loss = 0.015165125951170921 \t Accuracy = 0.7861998677253723\n",
      "\n",
      "Epoch 247, CIFAR-10 Batch 2:  \n",
      "Loss = 0.01709156483411789 \t Accuracy = 0.7839997410774231\n",
      "\n",
      "Epoch 247, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01900171861052513 \t Accuracy = 0.781799852848053\n",
      "\n",
      "Epoch 247, CIFAR-10 Batch 4:  \n",
      "Loss = 0.020177163183689117 \t Accuracy = 0.7747997641563416\n",
      "\n",
      "Epoch 247, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01631435565650463 \t Accuracy = 0.7871997952461243\n",
      "\n",
      "Epoch 248, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016116732731461525 \t Accuracy = 0.7813997864723206\n",
      "\n",
      "Epoch 248, CIFAR-10 Batch 2:  \n",
      "Loss = 0.019707461819052696 \t Accuracy = 0.7745997309684753\n",
      "\n",
      "Epoch 248, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02252132259309292 \t Accuracy = 0.7837998867034912\n",
      "\n",
      "Epoch 248, CIFAR-10 Batch 4:  \n",
      "Loss = 0.017409684136509895 \t Accuracy = 0.7807998657226562\n",
      "\n",
      "Epoch 248, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01524544321000576 \t Accuracy = 0.7867998480796814\n",
      "\n",
      "Epoch 249, CIFAR-10 Batch 1:  \n",
      "Loss = 0.019334441050887108 \t Accuracy = 0.769399881362915\n",
      "\n",
      "Epoch 249, CIFAR-10 Batch 2:  \n",
      "Loss = 0.0217124130576849 \t Accuracy = 0.7723997831344604\n",
      "\n",
      "Epoch 249, CIFAR-10 Batch 3:  \n",
      "Loss = 0.021391578018665314 \t Accuracy = 0.7909997701644897\n",
      "\n",
      "Epoch 249, CIFAR-10 Batch 4:  \n",
      "Loss = 0.01572062075138092 \t Accuracy = 0.7831997871398926\n",
      "\n",
      "Epoch 249, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01672901026904583 \t Accuracy = 0.7807998061180115\n",
      "\n",
      "Epoch 250, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014273221604526043 \t Accuracy = 0.7821998000144958\n",
      "\n",
      "Epoch 250, CIFAR-10 Batch 2:  \n",
      "Loss = 0.016359154134988785 \t Accuracy = 0.7793998122215271\n",
      "\n",
      "Epoch 250, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02056432142853737 \t Accuracy = 0.785399854183197\n",
      "\n",
      "Epoch 250, CIFAR-10 Batch 4:  \n",
      "Loss = 0.018745113164186478 \t Accuracy = 0.7817997932434082\n",
      "\n",
      "Epoch 250, CIFAR-10 Batch 5:  \n",
      "Loss = 0.016305411234498024 \t Accuracy = 0.7855998277664185\n",
      "\n",
      "Epoch 251, CIFAR-10 Batch 1:  \n",
      "Loss = 0.019605200737714767 \t Accuracy = 0.7719998359680176\n",
      "\n",
      "Epoch 251, CIFAR-10 Batch 2:  \n",
      "Loss = 0.016123980283737183 \t Accuracy = 0.785199761390686\n",
      "\n",
      "Epoch 251, CIFAR-10 Batch 3:  \n",
      "Loss = 0.020829621702432632 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 251, CIFAR-10 Batch 4:  \n",
      "Loss = 0.016629930585622787 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 251, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01654994860291481 \t Accuracy = 0.7889997959136963\n",
      "\n",
      "Epoch 252, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014573745429515839 \t Accuracy = 0.7781997919082642\n",
      "\n",
      "Epoch 252, CIFAR-10 Batch 2:  \n",
      "Loss = 0.018748866394162178 \t Accuracy = 0.7813997864723206\n",
      "\n",
      "Epoch 252, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02035689726471901 \t Accuracy = 0.7791998386383057\n",
      "\n",
      "Epoch 252, CIFAR-10 Batch 4:  \n",
      "Loss = 0.01812337525188923 \t Accuracy = 0.7745998501777649\n",
      "\n",
      "Epoch 252, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01618083380162716 \t Accuracy = 0.7873997688293457\n",
      "\n",
      "Epoch 253, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01986708864569664 \t Accuracy = 0.7773997783660889\n",
      "\n",
      "Epoch 253, CIFAR-10 Batch 2:  \n",
      "Loss = 0.02316884510219097 \t Accuracy = 0.7705998420715332\n",
      "\n",
      "Epoch 253, CIFAR-10 Batch 3:  \n",
      "Loss = 0.020400743931531906 \t Accuracy = 0.7767998576164246\n",
      "\n",
      "Epoch 253, CIFAR-10 Batch 4:  \n",
      "Loss = 0.01742406189441681 \t Accuracy = 0.7791998386383057\n",
      "\n",
      "Epoch 253, CIFAR-10 Batch 5:  \n",
      "Loss = 0.017513981088995934 \t Accuracy = 0.7879997491836548\n",
      "\n",
      "Epoch 254, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016957424581050873 \t Accuracy = 0.7813998460769653\n",
      "\n",
      "Epoch 254, CIFAR-10 Batch 2:  \n",
      "Loss = 0.01563875749707222 \t Accuracy = 0.7853997349739075\n",
      "\n",
      "Epoch 254, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01812286674976349 \t Accuracy = 0.7737998366355896\n",
      "\n",
      "Epoch 254, CIFAR-10 Batch 4:  \n",
      "Loss = 0.018444275483489037 \t Accuracy = 0.772199809551239\n",
      "\n",
      "Epoch 254, CIFAR-10 Batch 5:  \n",
      "Loss = 0.017019329592585564 \t Accuracy = 0.7901998162269592\n",
      "\n",
      "Epoch 255, CIFAR-10 Batch 1:  \n",
      "Loss = 0.018104754388332367 \t Accuracy = 0.7803997993469238\n",
      "\n",
      "Epoch 255, CIFAR-10 Batch 2:  \n",
      "Loss = 0.019722620025277138 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 255, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017483899369835854 \t Accuracy = 0.7827997803688049\n",
      "\n",
      "Epoch 255, CIFAR-10 Batch 4:  \n",
      "Loss = 0.016118472442030907 \t Accuracy = 0.786399781703949\n",
      "\n",
      "Epoch 255, CIFAR-10 Batch 5:  \n",
      "Loss = 0.014249324798583984 \t Accuracy = 0.78659987449646\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01658342033624649 \t Accuracy = 0.7783998250961304\n",
      "\n",
      "Epoch 256, CIFAR-10 Batch 2:  \n",
      "Loss = 0.016827603802084923 \t Accuracy = 0.7843997478485107\n",
      "\n",
      "Epoch 256, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017984766513109207 \t Accuracy = 0.7821998596191406\n",
      "\n",
      "Epoch 256, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014038696885108948 \t Accuracy = 0.77979975938797\n",
      "\n",
      "Epoch 256, CIFAR-10 Batch 5:  \n",
      "Loss = 0.017121370881795883 \t Accuracy = 0.7803997993469238\n",
      "\n",
      "Epoch 257, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01623079553246498 \t Accuracy = 0.7791997790336609\n",
      "\n",
      "Epoch 257, CIFAR-10 Batch 2:  \n",
      "Loss = 0.019367357715964317 \t Accuracy = 0.784599781036377\n",
      "\n",
      "Epoch 257, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01774032972753048 \t Accuracy = 0.7833998799324036\n",
      "\n",
      "Epoch 257, CIFAR-10 Batch 4:  \n",
      "Loss = 0.017768414691090584 \t Accuracy = 0.778799831867218\n",
      "\n",
      "Epoch 257, CIFAR-10 Batch 5:  \n",
      "Loss = 0.014032994396984577 \t Accuracy = 0.7907997965812683\n",
      "\n",
      "Epoch 258, CIFAR-10 Batch 1:  \n",
      "Loss = 0.018528619781136513 \t Accuracy = 0.7811998128890991\n",
      "\n",
      "Epoch 258, CIFAR-10 Batch 2:  \n",
      "Loss = 0.019442379474639893 \t Accuracy = 0.788199782371521\n",
      "\n",
      "Epoch 258, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02188899554312229 \t Accuracy = 0.7807998061180115\n",
      "\n",
      "Epoch 258, CIFAR-10 Batch 4:  \n",
      "Loss = 0.017088383436203003 \t Accuracy = 0.7795997858047485\n",
      "\n",
      "Epoch 258, CIFAR-10 Batch 5:  \n",
      "Loss = 0.012369416654109955 \t Accuracy = 0.791799783706665\n",
      "\n",
      "Epoch 259, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01514396257698536 \t Accuracy = 0.7845998406410217\n",
      "\n",
      "Epoch 259, CIFAR-10 Batch 2:  \n",
      "Loss = 0.022993285208940506 \t Accuracy = 0.7779998183250427\n",
      "\n",
      "Epoch 259, CIFAR-10 Batch 3:  \n",
      "Loss = 0.019111910834908485 \t Accuracy = 0.7799997925758362\n",
      "\n",
      "Epoch 259, CIFAR-10 Batch 4:  \n",
      "Loss = 0.015338988974690437 \t Accuracy = 0.7771998047828674\n",
      "\n",
      "Epoch 259, CIFAR-10 Batch 5:  \n",
      "Loss = 0.014525581151247025 \t Accuracy = 0.7887997627258301\n",
      "\n",
      "Epoch 260, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01992025226354599 \t Accuracy = 0.7741998434066772\n",
      "\n",
      "Epoch 260, CIFAR-10 Batch 2:  \n",
      "Loss = 0.018387336283922195 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 260, CIFAR-10 Batch 3:  \n",
      "Loss = 0.019138799980282784 \t Accuracy = 0.7797998785972595\n",
      "\n",
      "Epoch 260, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014828793704509735 \t Accuracy = 0.7759997844696045\n",
      "\n",
      "Epoch 260, CIFAR-10 Batch 5:  \n",
      "Loss = 0.013630926609039307 \t Accuracy = 0.7881997227668762\n",
      "\n",
      "Epoch 261, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016584042459726334 \t Accuracy = 0.7845998406410217\n",
      "\n",
      "Epoch 261, CIFAR-10 Batch 2:  \n",
      "Loss = 0.016942571848630905 \t Accuracy = 0.7835997939109802\n",
      "\n",
      "Epoch 261, CIFAR-10 Batch 3:  \n",
      "Loss = 0.021177398040890694 \t Accuracy = 0.7771998047828674\n",
      "\n",
      "Epoch 261, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014570320025086403 \t Accuracy = 0.773399829864502\n",
      "\n",
      "Epoch 261, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01686934381723404 \t Accuracy = 0.7915998101234436\n",
      "\n",
      "Epoch 262, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014893200248479843 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 262, CIFAR-10 Batch 2:  \n",
      "Loss = 0.019471898674964905 \t Accuracy = 0.7749998569488525\n",
      "\n",
      "Epoch 262, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017816368490457535 \t Accuracy = 0.7783998250961304\n",
      "\n",
      "Epoch 262, CIFAR-10 Batch 4:  \n",
      "Loss = 0.021187130361795425 \t Accuracy = 0.7705997824668884\n",
      "\n",
      "Epoch 262, CIFAR-10 Batch 5:  \n",
      "Loss = 0.0167471244931221 \t Accuracy = 0.7849998474121094\n",
      "\n",
      "Epoch 263, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01888463832437992 \t Accuracy = 0.7781997919082642\n",
      "\n",
      "Epoch 263, CIFAR-10 Batch 2:  \n",
      "Loss = 0.01854831539094448 \t Accuracy = 0.785199761390686\n",
      "\n",
      "Epoch 263, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01946941390633583 \t Accuracy = 0.7803997993469238\n",
      "\n",
      "Epoch 263, CIFAR-10 Batch 4:  \n",
      "Loss = 0.016330823302268982 \t Accuracy = 0.7747998237609863\n",
      "\n",
      "Epoch 263, CIFAR-10 Batch 5:  \n",
      "Loss = 0.017545906826853752 \t Accuracy = 0.7843998074531555\n",
      "\n",
      "Epoch 264, CIFAR-10 Batch 1:  \n",
      "Loss = 0.02016497030854225 \t Accuracy = 0.7757998108863831\n",
      "\n",
      "Epoch 264, CIFAR-10 Batch 2:  \n",
      "Loss = 0.018351683393120766 \t Accuracy = 0.7831998467445374\n",
      "\n",
      "Epoch 264, CIFAR-10 Batch 3:  \n",
      "Loss = 0.019592296332120895 \t Accuracy = 0.7775998711585999\n",
      "\n",
      "Epoch 264, CIFAR-10 Batch 4:  \n",
      "Loss = 0.016912059858441353 \t Accuracy = 0.776999831199646\n",
      "\n",
      "Epoch 264, CIFAR-10 Batch 5:  \n",
      "Loss = 0.017809513956308365 \t Accuracy = 0.7849998474121094\n",
      "\n",
      "Epoch 265, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016107771545648575 \t Accuracy = 0.778999924659729\n",
      "\n",
      "Epoch 265, CIFAR-10 Batch 2:  \n",
      "Loss = 0.016331078484654427 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 265, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017179446294903755 \t Accuracy = 0.7809998393058777\n",
      "\n",
      "Epoch 265, CIFAR-10 Batch 4:  \n",
      "Loss = 0.017489880323410034 \t Accuracy = 0.7713998556137085\n",
      "\n",
      "Epoch 265, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01332108024507761 \t Accuracy = 0.785399854183197\n",
      "\n",
      "Epoch 266, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01799837499856949 \t Accuracy = 0.7781998515129089\n",
      "\n",
      "Epoch 266, CIFAR-10 Batch 2:  \n",
      "Loss = 0.016767673194408417 \t Accuracy = 0.7745997905731201\n",
      "\n",
      "Epoch 266, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017295625060796738 \t Accuracy = 0.7727998495101929\n",
      "\n",
      "Epoch 266, CIFAR-10 Batch 4:  \n",
      "Loss = 0.022178074344992638 \t Accuracy = 0.763799786567688\n",
      "\n",
      "Epoch 266, CIFAR-10 Batch 5:  \n",
      "Loss = 0.016321679577231407 \t Accuracy = 0.7833998203277588\n",
      "\n",
      "Epoch 267, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01663076877593994 \t Accuracy = 0.7803997993469238\n",
      "\n",
      "Epoch 267, CIFAR-10 Batch 2:  \n",
      "Loss = 0.015561982989311218 \t Accuracy = 0.7811997532844543\n",
      "\n",
      "Epoch 267, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01863567717373371 \t Accuracy = 0.773999810218811\n",
      "\n",
      "Epoch 267, CIFAR-10 Batch 4:  \n",
      "Loss = 0.019823875278234482 \t Accuracy = 0.7647998332977295\n",
      "\n",
      "Epoch 267, CIFAR-10 Batch 5:  \n",
      "Loss = 0.014330183155834675 \t Accuracy = 0.787199854850769\n",
      "\n",
      "Epoch 268, CIFAR-10 Batch 1:  \n",
      "Loss = 0.013750565238296986 \t Accuracy = 0.7771997451782227\n",
      "\n",
      "Epoch 268, CIFAR-10 Batch 2:  \n",
      "Loss = 0.018499158322811127 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 268, CIFAR-10 Batch 3:  \n",
      "Loss = 0.019997410476207733 \t Accuracy = 0.7749998569488525\n",
      "\n",
      "Epoch 268, CIFAR-10 Batch 4:  \n",
      "Loss = 0.01573134958744049 \t Accuracy = 0.7727997899055481\n",
      "\n",
      "Epoch 268, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01518061850219965 \t Accuracy = 0.7827998399734497\n",
      "\n",
      "Epoch 269, CIFAR-10 Batch 1:  \n",
      "Loss = 0.015673302114009857 \t Accuracy = 0.7761998176574707\n",
      "\n",
      "Epoch 269, CIFAR-10 Batch 2:  \n",
      "Loss = 0.015252934768795967 \t Accuracy = 0.7829997539520264\n",
      "\n",
      "Epoch 269, CIFAR-10 Batch 3:  \n",
      "Loss = 0.016286732628941536 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 269, CIFAR-10 Batch 4:  \n",
      "Loss = 0.013838290236890316 \t Accuracy = 0.7753998041152954\n",
      "\n",
      "Epoch 269, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01805603690445423 \t Accuracy = 0.7827997803688049\n",
      "\n",
      "Epoch 270, CIFAR-10 Batch 1:  \n",
      "Loss = 0.019453849643468857 \t Accuracy = 0.7741998434066772\n",
      "\n",
      "Epoch 270, CIFAR-10 Batch 2:  \n",
      "Loss = 0.016909021884202957 \t Accuracy = 0.7793998718261719\n",
      "\n",
      "Epoch 270, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02003878727555275 \t Accuracy = 0.7765998244285583\n",
      "\n",
      "Epoch 270, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014142490923404694 \t Accuracy = 0.7775998115539551\n",
      "\n",
      "Epoch 270, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01724463887512684 \t Accuracy = 0.7783998250961304\n",
      "\n",
      "Epoch 271, CIFAR-10 Batch 1:  \n",
      "Loss = 0.012468297965824604 \t Accuracy = 0.7789998054504395\n",
      "\n",
      "Epoch 271, CIFAR-10 Batch 2:  \n",
      "Loss = 0.016823790967464447 \t Accuracy = 0.7807998061180115\n",
      "\n",
      "Epoch 271, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017966661602258682 \t Accuracy = 0.7809997797012329\n",
      "\n",
      "Epoch 271, CIFAR-10 Batch 4:  \n",
      "Loss = 0.015473246574401855 \t Accuracy = 0.7749998569488525\n",
      "\n",
      "Epoch 271, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01810402050614357 \t Accuracy = 0.7809997797012329\n",
      "\n",
      "Epoch 272, CIFAR-10 Batch 1:  \n",
      "Loss = 0.021633602678775787 \t Accuracy = 0.7767997980117798\n",
      "\n",
      "Epoch 272, CIFAR-10 Batch 2:  \n",
      "Loss = 0.02048221044242382 \t Accuracy = 0.7861998677253723\n",
      "\n",
      "Epoch 272, CIFAR-10 Batch 3:  \n",
      "Loss = 0.02008621022105217 \t Accuracy = 0.7775998115539551\n",
      "\n",
      "Epoch 272, CIFAR-10 Batch 4:  \n",
      "Loss = 0.016032610088586807 \t Accuracy = 0.7689998149871826\n",
      "\n",
      "Epoch 272, CIFAR-10 Batch 5:  \n",
      "Loss = 0.013535003177821636 \t Accuracy = 0.7869998812675476\n",
      "\n",
      "Epoch 273, CIFAR-10 Batch 1:  \n",
      "Loss = 0.018442438915371895 \t Accuracy = 0.7741997838020325\n",
      "\n",
      "Epoch 273, CIFAR-10 Batch 2:  \n",
      "Loss = 0.014461841434240341 \t Accuracy = 0.7807998657226562\n",
      "\n",
      "Epoch 273, CIFAR-10 Batch 3:  \n",
      "Loss = 0.016246680170297623 \t Accuracy = 0.7849997878074646\n",
      "\n",
      "Epoch 273, CIFAR-10 Batch 4:  \n",
      "Loss = 0.013136221095919609 \t Accuracy = 0.7773997783660889\n",
      "\n",
      "Epoch 273, CIFAR-10 Batch 5:  \n",
      "Loss = 0.021478669717907906 \t Accuracy = 0.7831998467445374\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01821138523519039 \t Accuracy = 0.7791997790336609\n",
      "\n",
      "Epoch 274, CIFAR-10 Batch 2:  \n",
      "Loss = 0.014706862159073353 \t Accuracy = 0.7793998122215271\n",
      "\n",
      "Epoch 274, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017544858157634735 \t Accuracy = 0.7839998006820679\n",
      "\n",
      "Epoch 274, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014138377271592617 \t Accuracy = 0.7781997919082642\n",
      "\n",
      "Epoch 274, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01807018183171749 \t Accuracy = 0.7783997654914856\n",
      "\n",
      "Epoch 275, CIFAR-10 Batch 1:  \n",
      "Loss = 0.018524443730711937 \t Accuracy = 0.7707998752593994\n",
      "\n",
      "Epoch 275, CIFAR-10 Batch 2:  \n",
      "Loss = 0.015235508792102337 \t Accuracy = 0.7749998569488525\n",
      "\n",
      "Epoch 275, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01780288852751255 \t Accuracy = 0.7735998630523682\n",
      "\n",
      "Epoch 275, CIFAR-10 Batch 4:  \n",
      "Loss = 0.018812526017427444 \t Accuracy = 0.7641998529434204\n",
      "\n",
      "Epoch 275, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01728261262178421 \t Accuracy = 0.7811998128890991\n",
      "\n",
      "Epoch 276, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016374047845602036 \t Accuracy = 0.7753998041152954\n",
      "\n",
      "Epoch 276, CIFAR-10 Batch 2:  \n",
      "Loss = 0.015616684220731258 \t Accuracy = 0.7867999076843262\n",
      "\n",
      "Epoch 276, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01837041787803173 \t Accuracy = 0.7823998332023621\n",
      "\n",
      "Epoch 276, CIFAR-10 Batch 4:  \n",
      "Loss = 0.019328530877828598 \t Accuracy = 0.7705998420715332\n",
      "\n",
      "Epoch 276, CIFAR-10 Batch 5:  \n",
      "Loss = 0.018677137792110443 \t Accuracy = 0.7767997980117798\n",
      "\n",
      "Epoch 277, CIFAR-10 Batch 1:  \n",
      "Loss = 0.020720528438687325 \t Accuracy = 0.7701998949050903\n",
      "\n",
      "Epoch 277, CIFAR-10 Batch 2:  \n",
      "Loss = 0.018322281539440155 \t Accuracy = 0.7817997932434082\n",
      "\n",
      "Epoch 277, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017203183844685555 \t Accuracy = 0.784599781036377\n",
      "\n",
      "Epoch 277, CIFAR-10 Batch 4:  \n",
      "Loss = 0.01568414643406868 \t Accuracy = 0.769199788570404\n",
      "\n",
      "Epoch 277, CIFAR-10 Batch 5:  \n",
      "Loss = 0.018083561211824417 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 278, CIFAR-10 Batch 1:  \n",
      "Loss = 0.026121143251657486 \t Accuracy = 0.762799859046936\n",
      "\n",
      "Epoch 278, CIFAR-10 Batch 2:  \n",
      "Loss = 0.019281864166259766 \t Accuracy = 0.7859998345375061\n",
      "\n",
      "Epoch 278, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017748286947607994 \t Accuracy = 0.7837998867034912\n",
      "\n",
      "Epoch 278, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014945145696401596 \t Accuracy = 0.7787998914718628\n",
      "\n",
      "Epoch 278, CIFAR-10 Batch 5:  \n",
      "Loss = 0.016669858247041702 \t Accuracy = 0.7797998785972595\n",
      "\n",
      "Epoch 279, CIFAR-10 Batch 1:  \n",
      "Loss = 0.023508252575993538 \t Accuracy = 0.7683998346328735\n",
      "\n",
      "Epoch 279, CIFAR-10 Batch 2:  \n",
      "Loss = 0.02393462508916855 \t Accuracy = 0.7931997776031494\n",
      "\n",
      "Epoch 279, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017959721386432648 \t Accuracy = 0.7831997871398926\n",
      "\n",
      "Epoch 279, CIFAR-10 Batch 4:  \n",
      "Loss = 0.015547056682407856 \t Accuracy = 0.7777998447418213\n",
      "\n",
      "Epoch 279, CIFAR-10 Batch 5:  \n",
      "Loss = 0.028139512985944748 \t Accuracy = 0.7619998455047607\n",
      "\n",
      "Epoch 280, CIFAR-10 Batch 1:  \n",
      "Loss = 0.02594960480928421 \t Accuracy = 0.7599998116493225\n",
      "\n",
      "Epoch 280, CIFAR-10 Batch 2:  \n",
      "Loss = 0.022120922803878784 \t Accuracy = 0.784599781036377\n",
      "\n",
      "Epoch 280, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017434626817703247 \t Accuracy = 0.7853997945785522\n",
      "\n",
      "Epoch 280, CIFAR-10 Batch 4:  \n",
      "Loss = 0.015406938269734383 \t Accuracy = 0.7809998989105225\n",
      "\n",
      "Epoch 280, CIFAR-10 Batch 5:  \n",
      "Loss = 0.014877920038998127 \t Accuracy = 0.779999852180481\n",
      "\n",
      "Epoch 281, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014827581122517586 \t Accuracy = 0.7783998250961304\n",
      "\n",
      "Epoch 281, CIFAR-10 Batch 2:  \n",
      "Loss = 0.016198212280869484 \t Accuracy = 0.7857997417449951\n",
      "\n",
      "Epoch 281, CIFAR-10 Batch 3:  \n",
      "Loss = 0.016909882426261902 \t Accuracy = 0.7809997797012329\n",
      "\n",
      "Epoch 281, CIFAR-10 Batch 4:  \n",
      "Loss = 0.012784247286617756 \t Accuracy = 0.7823998332023621\n",
      "\n",
      "Epoch 281, CIFAR-10 Batch 5:  \n",
      "Loss = 0.017447253689169884 \t Accuracy = 0.7777998447418213\n",
      "\n",
      "Epoch 282, CIFAR-10 Batch 1:  \n",
      "Loss = 0.02364138327538967 \t Accuracy = 0.7653998732566833\n",
      "\n",
      "Epoch 282, CIFAR-10 Batch 2:  \n",
      "Loss = 0.02019098773598671 \t Accuracy = 0.7807998061180115\n",
      "\n",
      "Epoch 282, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01714569330215454 \t Accuracy = 0.7803998589515686\n",
      "\n",
      "Epoch 282, CIFAR-10 Batch 4:  \n",
      "Loss = 0.012880505062639713 \t Accuracy = 0.7821998596191406\n",
      "\n",
      "Epoch 282, CIFAR-10 Batch 5:  \n",
      "Loss = 0.013504937291145325 \t Accuracy = 0.775199830532074\n",
      "\n",
      "Epoch 283, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016637973487377167 \t Accuracy = 0.7615997791290283\n",
      "\n",
      "Epoch 283, CIFAR-10 Batch 2:  \n",
      "Loss = 0.01931321620941162 \t Accuracy = 0.7809998393058777\n",
      "\n",
      "Epoch 283, CIFAR-10 Batch 3:  \n",
      "Loss = 0.019599253311753273 \t Accuracy = 0.7697998285293579\n",
      "\n",
      "Epoch 283, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014947650022804737 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 283, CIFAR-10 Batch 5:  \n",
      "Loss = 0.014165626838803291 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 284, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01629486493766308 \t Accuracy = 0.7727997899055481\n",
      "\n",
      "Epoch 284, CIFAR-10 Batch 2:  \n",
      "Loss = 0.014575830660760403 \t Accuracy = 0.7867997288703918\n",
      "\n",
      "Epoch 284, CIFAR-10 Batch 3:  \n",
      "Loss = 0.016731444746255875 \t Accuracy = 0.7815998792648315\n",
      "\n",
      "Epoch 284, CIFAR-10 Batch 4:  \n",
      "Loss = 0.013287637382745743 \t Accuracy = 0.7819998860359192\n",
      "\n",
      "Epoch 284, CIFAR-10 Batch 5:  \n",
      "Loss = 0.014176169410347939 \t Accuracy = 0.7753998041152954\n",
      "\n",
      "Epoch 285, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01504926010966301 \t Accuracy = 0.7697997689247131\n",
      "\n",
      "Epoch 285, CIFAR-10 Batch 2:  \n",
      "Loss = 0.020072918385267258 \t Accuracy = 0.781799852848053\n",
      "\n",
      "Epoch 285, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01918213441967964 \t Accuracy = 0.7809998393058777\n",
      "\n",
      "Epoch 285, CIFAR-10 Batch 4:  \n",
      "Loss = 0.015485730022192001 \t Accuracy = 0.7791998386383057\n",
      "\n",
      "Epoch 285, CIFAR-10 Batch 5:  \n",
      "Loss = 0.021090107038617134 \t Accuracy = 0.773399829864502\n",
      "\n",
      "Epoch 286, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01756010577082634 \t Accuracy = 0.7731997966766357\n",
      "\n",
      "Epoch 286, CIFAR-10 Batch 2:  \n",
      "Loss = 0.014175092801451683 \t Accuracy = 0.7775998115539551\n",
      "\n",
      "Epoch 286, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01682007499039173 \t Accuracy = 0.7823998332023621\n",
      "\n",
      "Epoch 286, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014240936376154423 \t Accuracy = 0.7815998792648315\n",
      "\n",
      "Epoch 286, CIFAR-10 Batch 5:  \n",
      "Loss = 0.013207018375396729 \t Accuracy = 0.7805997729301453\n",
      "\n",
      "Epoch 287, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014114545658230782 \t Accuracy = 0.7783998847007751\n",
      "\n",
      "Epoch 287, CIFAR-10 Batch 2:  \n",
      "Loss = 0.014161285944283009 \t Accuracy = 0.783399760723114\n",
      "\n",
      "Epoch 287, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01641392894089222 \t Accuracy = 0.7775998115539551\n",
      "\n",
      "Epoch 287, CIFAR-10 Batch 4:  \n",
      "Loss = 0.013406951911747456 \t Accuracy = 0.7775997519493103\n",
      "\n",
      "Epoch 287, CIFAR-10 Batch 5:  \n",
      "Loss = 0.014431554824113846 \t Accuracy = 0.773999810218811\n",
      "\n",
      "Epoch 288, CIFAR-10 Batch 1:  \n",
      "Loss = 0.015604008920490742 \t Accuracy = 0.7719998955726624\n",
      "\n",
      "Epoch 288, CIFAR-10 Batch 2:  \n",
      "Loss = 0.015558872371912003 \t Accuracy = 0.7887997627258301\n",
      "\n",
      "Epoch 288, CIFAR-10 Batch 3:  \n",
      "Loss = 0.016187278553843498 \t Accuracy = 0.7749998569488525\n",
      "\n",
      "Epoch 288, CIFAR-10 Batch 4:  \n",
      "Loss = 0.016188036650419235 \t Accuracy = 0.7809998393058777\n",
      "\n",
      "Epoch 288, CIFAR-10 Batch 5:  \n",
      "Loss = 0.018251676112413406 \t Accuracy = 0.7709998488426208\n",
      "\n",
      "Epoch 289, CIFAR-10 Batch 1:  \n",
      "Loss = 0.014664890244603157 \t Accuracy = 0.7767997980117798\n",
      "\n",
      "Epoch 289, CIFAR-10 Batch 2:  \n",
      "Loss = 0.017167525365948677 \t Accuracy = 0.7847998142242432\n",
      "\n",
      "Epoch 289, CIFAR-10 Batch 3:  \n",
      "Loss = 0.017738649621605873 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 289, CIFAR-10 Batch 4:  \n",
      "Loss = 0.012723907828330994 \t Accuracy = 0.7797998189926147\n",
      "\n",
      "Epoch 289, CIFAR-10 Batch 5:  \n",
      "Loss = 0.013565176166594028 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 290, CIFAR-10 Batch 1:  \n",
      "Loss = 0.015752751380205154 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 290, CIFAR-10 Batch 2:  \n",
      "Loss = 0.021072540432214737 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 290, CIFAR-10 Batch 3:  \n",
      "Loss = 0.018483441323041916 \t Accuracy = 0.7837997674942017\n",
      "\n",
      "Epoch 290, CIFAR-10 Batch 4:  \n",
      "Loss = 0.012901532463729382 \t Accuracy = 0.7845999002456665\n",
      "\n",
      "Epoch 290, CIFAR-10 Batch 5:  \n",
      "Loss = 0.012119418941438198 \t Accuracy = 0.7777997851371765\n",
      "\n",
      "Epoch 291, CIFAR-10 Batch 1:  \n",
      "Loss = 0.015472515486180782 \t Accuracy = 0.7701998353004456\n",
      "\n",
      "Epoch 291, CIFAR-10 Batch 2:  \n",
      "Loss = 0.015559252351522446 \t Accuracy = 0.7845998406410217\n",
      "\n",
      "Epoch 291, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01802801340818405 \t Accuracy = 0.7755998373031616\n",
      "\n",
      "Epoch 291, CIFAR-10 Batch 4:  \n",
      "Loss = 0.012088865041732788 \t Accuracy = 0.7777998447418213\n",
      "\n",
      "Epoch 291, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01853848434984684 \t Accuracy = 0.7763999104499817\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 292, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016173120588064194 \t Accuracy = 0.7751997709274292\n",
      "\n",
      "Epoch 292, CIFAR-10 Batch 2:  \n",
      "Loss = 0.017510006204247475 \t Accuracy = 0.7775998115539551\n",
      "\n",
      "Epoch 292, CIFAR-10 Batch 3:  \n",
      "Loss = 0.018548626452684402 \t Accuracy = 0.7809997797012329\n",
      "\n",
      "Epoch 292, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014173106290400028 \t Accuracy = 0.7781998515129089\n",
      "\n",
      "Epoch 292, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01458871178328991 \t Accuracy = 0.778799831867218\n",
      "\n",
      "Epoch 293, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01311319600790739 \t Accuracy = 0.7755998373031616\n",
      "\n",
      "Epoch 293, CIFAR-10 Batch 2:  \n",
      "Loss = 0.013588971458375454 \t Accuracy = 0.7861998677253723\n",
      "\n",
      "Epoch 293, CIFAR-10 Batch 3:  \n",
      "Loss = 0.016834130510687828 \t Accuracy = 0.7775998115539551\n",
      "\n",
      "Epoch 293, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014986615628004074 \t Accuracy = 0.7811998128890991\n",
      "\n",
      "Epoch 293, CIFAR-10 Batch 5:  \n",
      "Loss = 0.012409827671945095 \t Accuracy = 0.7847998738288879\n",
      "\n",
      "Epoch 294, CIFAR-10 Batch 1:  \n",
      "Loss = 0.012219720520079136 \t Accuracy = 0.7697998285293579\n",
      "\n",
      "Epoch 294, CIFAR-10 Batch 2:  \n",
      "Loss = 0.017006058245897293 \t Accuracy = 0.7841998338699341\n",
      "\n",
      "Epoch 294, CIFAR-10 Batch 3:  \n",
      "Loss = 0.022464396432042122 \t Accuracy = 0.7755998373031616\n",
      "\n",
      "Epoch 294, CIFAR-10 Batch 4:  \n",
      "Loss = 0.013912850059568882 \t Accuracy = 0.775199830532074\n",
      "\n",
      "Epoch 294, CIFAR-10 Batch 5:  \n",
      "Loss = 0.016108589246869087 \t Accuracy = 0.7781997919082642\n",
      "\n",
      "Epoch 295, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01960834302008152 \t Accuracy = 0.7723998427391052\n",
      "\n",
      "Epoch 295, CIFAR-10 Batch 2:  \n",
      "Loss = 0.017410987988114357 \t Accuracy = 0.7861998081207275\n",
      "\n",
      "Epoch 295, CIFAR-10 Batch 3:  \n",
      "Loss = 0.01718820631504059 \t Accuracy = 0.7857998609542847\n",
      "\n",
      "Epoch 295, CIFAR-10 Batch 4:  \n",
      "Loss = 0.012974271550774574 \t Accuracy = 0.7803997993469238\n",
      "\n",
      "Epoch 295, CIFAR-10 Batch 5:  \n",
      "Loss = 0.013203187845647335 \t Accuracy = 0.7829998731613159\n",
      "\n",
      "Epoch 296, CIFAR-10 Batch 1:  \n",
      "Loss = 0.01799873448908329 \t Accuracy = 0.7695999145507812\n",
      "\n",
      "Epoch 296, CIFAR-10 Batch 2:  \n",
      "Loss = 0.01869075372815132 \t Accuracy = 0.7771997451782227\n",
      "\n",
      "Epoch 296, CIFAR-10 Batch 3:  \n",
      "Loss = 0.016777776181697845 \t Accuracy = 0.7785997986793518\n",
      "\n",
      "Epoch 296, CIFAR-10 Batch 4:  \n",
      "Loss = 0.01368518266826868 \t Accuracy = 0.779999852180481\n",
      "\n",
      "Epoch 296, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01350951474159956 \t Accuracy = 0.7783998847007751\n",
      "\n",
      "Epoch 297, CIFAR-10 Batch 1:  \n",
      "Loss = 0.010144908912479877 \t Accuracy = 0.7783998250961304\n",
      "\n",
      "Epoch 297, CIFAR-10 Batch 2:  \n",
      "Loss = 0.01456358376890421 \t Accuracy = 0.7851998209953308\n",
      "\n",
      "Epoch 297, CIFAR-10 Batch 3:  \n",
      "Loss = 0.0161436740309 \t Accuracy = 0.783599853515625\n",
      "\n",
      "Epoch 297, CIFAR-10 Batch 4:  \n",
      "Loss = 0.01238749735057354 \t Accuracy = 0.7763997912406921\n",
      "\n",
      "Epoch 297, CIFAR-10 Batch 5:  \n",
      "Loss = 0.013544758781790733 \t Accuracy = 0.7763999104499817\n",
      "\n",
      "Epoch 298, CIFAR-10 Batch 1:  \n",
      "Loss = 0.013249166309833527 \t Accuracy = 0.7783998250961304\n",
      "\n",
      "Epoch 298, CIFAR-10 Batch 2:  \n",
      "Loss = 0.01548855658620596 \t Accuracy = 0.7821998596191406\n",
      "\n",
      "Epoch 298, CIFAR-10 Batch 3:  \n",
      "Loss = 0.015524807386100292 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 298, CIFAR-10 Batch 4:  \n",
      "Loss = 0.016348376870155334 \t Accuracy = 0.770599901676178\n",
      "\n",
      "Epoch 298, CIFAR-10 Batch 5:  \n",
      "Loss = 0.012844428420066833 \t Accuracy = 0.7843998074531555\n",
      "\n",
      "Epoch 299, CIFAR-10 Batch 1:  \n",
      "Loss = 0.016370877623558044 \t Accuracy = 0.7755998373031616\n",
      "\n",
      "Epoch 299, CIFAR-10 Batch 2:  \n",
      "Loss = 0.018910368904471397 \t Accuracy = 0.7781998515129089\n",
      "\n",
      "Epoch 299, CIFAR-10 Batch 3:  \n",
      "Loss = 0.015391223132610321 \t Accuracy = 0.7823997735977173\n",
      "\n",
      "Epoch 299, CIFAR-10 Batch 4:  \n",
      "Loss = 0.013890905305743217 \t Accuracy = 0.7761998176574707\n",
      "\n",
      "Epoch 299, CIFAR-10 Batch 5:  \n",
      "Loss = 0.01352677121758461 \t Accuracy = 0.78059983253479\n",
      "\n",
      "Epoch 300, CIFAR-10 Batch 1:  \n",
      "Loss = 0.012925229035317898 \t Accuracy = 0.7763998508453369\n",
      "\n",
      "Epoch 300, CIFAR-10 Batch 2:  \n",
      "Loss = 0.016354579478502274 \t Accuracy = 0.7869998216629028\n",
      "\n",
      "Epoch 300, CIFAR-10 Batch 3:  \n",
      "Loss = 0.016881447285413742 \t Accuracy = 0.7785998582839966\n",
      "\n",
      "Epoch 300, CIFAR-10 Batch 4:  \n",
      "Loss = 0.014753276482224464 \t Accuracy = 0.7809998393058777\n",
      "\n",
      "Epoch 300, CIFAR-10 Batch 5:  \n",
      "Loss = 0.013262402266263962 \t Accuracy = 0.7777998447418213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7757194638252258\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nif4/RroPjYFlKhgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22c56193748>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
